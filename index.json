[{"uri":"https://nhatanhaxtanh.github.io/nhatanh/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Le Nhat Anh\nPhone Number: 0934560352\nEmail: lenhatanh2411@gmail.com\nUniversity: FPT University HCM Campus\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"1. High-Level Architecture The IELTS BandUp platform is built on a robust, highly available architecture on AWS. The system is designed to handle user traffic securely while providing low-latency access to study materials and AI-powered features.\n2. Core AWS Services To achieve the goals of scalability, security, and high availability, we utilize the following key AWS services:\nNetworking \u0026amp; Content Delivery Amazon VPC (Virtual Private Cloud): The foundational network layer. We utilize a custom VPC with isolated Public and Private subnets to strictly control traffic flow. NAT Gateway: Allows instances in private subnets (like our Backend containers) to access the internet for updates or external API calls without being exposed to incoming public traffic. Application Load Balancer (ALB): Distributes incoming application traffic across multiple targets (containers) in different Availability Zones, ensuring fault tolerance. Amazon Route 53: A scalable Domain Name System (DNS) web service used for domain registration and traffic routing. Compute \u0026amp; Containers Amazon ECS (Elastic Container Service) on Fargate: A serverless compute engine for containers. We use Fargate to run both our Next.js Frontend and Spring Boot Backend, removing the need to provision or manage servers. Amazon ECR (Elastic Container Registry): A fully managed container registry where we store, manage, and deploy our Docker container images. Database \u0026amp; Storage Amazon RDS (Relational Database Service): We use PostgreSQL in a Multi-AZ deployment (Primary and Standby) to ensure data durability and disaster recovery for user profiles and test data. Amazon ElastiCache (Redis): Acts as an in-memory data store to cache frequent queries and manage user sessions, significantly improving application performance. Amazon S3 (Simple Storage Service): Stores static assets, media files (audio for listening tests), and user-generated content securely. AI \u0026amp; Serverless Integration To power the intelligent features of BandUp (Writing/Speaking Feedback, Flashcard Generation), we use a Serverless approach:\nAmazon Bedrock \u0026amp; Google Gemini API: The core Generative AI models used to analyze user inputs and generate personalized study feedback. AWS Lambda: Serverless compute functions that orchestrate the AI workflow, connecting the application to AI models. Amazon SQS (Simple Queue Service): Decouples the backend from the AI processing layer, allowing requests to be queued and processed asynchronously to prevent system overload. Amazon API Gateway: Acts as the \u0026ldquo;front door\u0026rdquo; for the AI services, managing RESTful API calls securely. DevOps \u0026amp; CI/CD AWS CodePipeline: Automates the release pipelines for fast and reliable application and infrastructure updates. AWS CodeBuild: Compiles source code, runs tests, and produces software packages (Docker images) ready to deploy. Security AWS WAF (Web Application Firewall): Protects the web application from common web exploits. AWS Secrets Manager: Securely stores and manages sensitive credentials (database passwords, API keys) throughout their lifecycle. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.4-setup-fe/5.4.1-docker/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize our Next.js Frontend application and push the Docker image to Amazon Elastic Container Registry (ECR). This image will later be used by ECS Fargate to launch the application.\n1. Prepare Dockerfile We use a multi-stage Dockerfile optimized for Bun (a fast JavaScript runtime) and Next.js. This configuration reduces the final image size and improves security.\nBase Image: oven/bun:1.1.26 Builder: Compiles the Next.js application. Runner: A lightweight production environment exposing port 3000. 2. Build Docker Image Run the following command in your project root to build the image. We tag it as band-up-frontend.\ndocker build -t band-up-frontend . The build process will install dependencies using bun install and compile the project.\n3. Verify Local Image Once the build is complete, verify that the image exists locally.\ndocker image ls You should see band-up-frontend with the latest tag.\nTest Locally: You can try running the container locally to ensure it starts up correctly before pushing to AWS.\ndocker run -p 3000:3000 band-up-frontend:latest 4. Push to Amazon ECR Now we need to upload this image to AWS.\nStep 1: Create Repository\nGo to Amazon ECR \u0026gt; Repositories. Click Create repository. Visibility settings: Private. Repository name: band-up-frontend. Click Create repository. Step 2: Push Image Use the AWS CLI to authenticate and push the image. Replace [AWS_ACCOUNT_ID] and [REGION] with your details.\nLogin to ECR:\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com Tag the Image:\ndocker tag band-up-frontend:latest [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Push the Image:\ndocker push [AWS_ACCOUNT_ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:latest Once the push is complete, your image is hosted on AWS ECR and ready for deployment.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.5-setup-be/5.5.1-ecr/","title":"Setup ECR &amp; Push Image","tags":[],"description":"","content":"In this step, we will containerize the Spring Boot Backend and push the optimized Docker image to Amazon ECR.\n1. Dockerfile Strategy For the Backend, we utilize a Multi-Stage Build strategy with eclipse-temurin:21 (Java 21). This ensures a small and secure final image.\nStage 1 (Deps): Resolves and downloads Maven dependencies. Stage 2 (Package): Builds the application and extracts the Spring Boot Layered Jar. This splits the application into layers (dependencies, spring-boot-loader, application code), allowing Docker to cache unchanged layers (like dependencies) effectively. Stage 3 (Final): Copies the extracted layers into a lightweight JRE image. It also creates a non-privileged user appuser for security. 2. Build Docker Image Run the build command in the backend project root. We tag the image as band-up-backend.\ndocker build -t band-up-backend . Docker will execute the stages defined above.\n3. Create ECR Repository We need a repository to store this image.\nNavigate to Amazon ECR \u0026gt; Create repository. Repository name: band-up-backend (Ensure this matches your push command). Visibility: Private. Image tag mutability: Mutable. Click Create repository. 4. Push Image to ECR Once the image is built and the repository is ready, proceed to push.\nStep 1: Tag the Image Tag the local image with a version number (e.g., v1.0.0).\ndocker tag band-up-backend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 Step 2: Push to ECR Upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-backend:v1.0.0 5. Verify Navigate to the Amazon ECR Console and select the bandup-backend repository. You should see the image tagged v1.0.0.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.3-network/5.3.1-vpc/","title":"VPC, Subnets &amp; Routing","tags":[],"description":"","content":"In this step, we establish the isolated network environment for IELTS BandUp. We will create a Virtual Private Cloud (VPC), partition it into subnets across multiple Availability Zones, and configure routing for internet access.\n1. Create the VPC First, we need a private network space.\nNavigate to the VPC Dashboard. Click Create VPC. Choose VPC only. Name tag: band-up-vpc IPv4 CIDR block: 10.0.0.0/16 (This provides 65,536 IP addresses, sufficient for future scaling). Leave other settings as default and click Create VPC. 2. Create Subnets Next, we divide the VPC into smaller networks (Subnets) distributed across two Availability Zones (AZs) for High Availability. We will follow this IP schema:\nSubnet Name Type CIDR Block Availability Zone public-subnet-1 Public 10.0.0.0/24 ap-southeast-1a public-subnet-2 Public 10.0.1.0/24 ap-southeast-1b private-app-subnet-1 Private 10.0.2.0/24 ap-southeast-1a private-app-subnet-2 Private 10.0.3.0/24 ap-southeast-1b private-database-subnet-1 Database 10.0.4.0/24 ap-southeast-1a private-database-subnet-2 Database 10.0.5.0/24 ap-southeast-1b Steps:\nGo to Subnets \u0026gt; Create subnet. Select the VPC ID: band-up-vpc. Enter the Subnet name, Availability Zone, and IPv4 CIDR block for each subnet according to the table above. Repeat the process until all 6 subnets are created. 3. Create Internet Gateway (IGW) By default, a VPC is closed to the internet. To allow resources in our Public Subnets to communicate with the outside world, we need an Internet Gateway.\nGo to Internet gateways \u0026gt; Create internet gateway. Name tag: band-up-igw. Click Create internet gateway. After creation, click Actions \u0026gt; Attach to VPC. Select band-up-vpc and click Attach internet gateway. 4. Configure Route Tables Finally, we need to direct traffic from our Public Subnets to the Internet Gateway.\nGo to Route tables \u0026gt; Create route table. Name: public-route-table. VPC: band-up-vpc. Click Create route table. Add Route to Internet:\nSelect the newly created public-route-table. Go to the Routes tab \u0026gt; Edit routes. Add a new route: Destination: 0.0.0.0/0 (All traffic). Target: Select Internet Gateway -\u0026gt; band-up-igw. Click Save changes. Associate Subnets:\nGo to the Subnet associations tab \u0026gt; Edit subnet associations. Select only the Public Subnets (public-subnet-1 and public-subnet-2). Click Save associations. 5. Verify Configuration To verify that the network architecture is correctly established, navigate back to your VPC Dashboard, select band-up-vpc, and view the Resource map tab. You should see a clear structure linking your Public Subnets to the Route Table and the Internet Gateway.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Post-Event Report: “The First Cloud Journey (FCJ) Program Kick-off” Event Objectives Officially launch the 12-week The First Cloud Journey (FCJ) program. Connect and socialize among members, Mentors, and the Organizing Committee (OC). Provide an overview of the organization, learning goals, and project implementation roadmap. Guide the team working process and proceed with forming project teams. Speaker List Representative of the Organizing Committee (OC) - Introduced the program\u0026rsquo;s vision and mission. Featured Mentors - Shared experiences and successful Cloud career paths. Former Members (Alumni) - Shared practical lessons and insights. Key Highlights Program Framework and 12-Week Roadmap Detailed presentation of the 12-week roadmap, covering everything from basic AWS knowledge (VPC, EC2) to implementing a complete Serverless project. Established specific goals regarding knowledge acquisition and the minimum viable product (MVP) deliverable. Working Rules and Organizational Culture Rules on discipline and participation: Emphasized individual commitment and responsibility throughout the journey. Culture of sharing and support: Built a strong sense of community among members. Team Formation Process and Project Guidelines Guidance on how to create effective teams (role assignment, communication tools). Provided initial direction for Cloud projects for teams to begin research. AWS Account Security Instructions on setting up Two-Factor Authentication (2FA) and basic steps for cost management (Budget) from the start. Key Takeaways Mindset and Commitment Importance of discipline: Clearly recognized the seriousness and level of commitment required to complete the program. Role of community: Understood the value of learning and support from Mentors and other members. Organizational Knowledge Program objectives: Clearly understood the Cloud knowledge and skills the program aims to equip participants with. Clear learning path: Grasped the main modules to be conquered over the 12 weeks. Initial Practical Skills Team formation process: Successfully completed the creation of the project team with new members. Basic security: Learned the initial steps to set up a secure and cost-optimized AWS account. Application to Work Immediately formed the team and assigned initial tasks. Began researching the organization and fundamental Cloud concepts. Executed basic operations on the AWS account such as setting up 2FA and Budget (as per Worklog Week 1). Event Experience Participating in the Kick-off was an experience full of energy and clear direction. This event laid the foundation for the entire 12-week journey:\nConnection and Community Energy Feeling welcomed: Met and socialized with all members of FCJ, creating a positive learning and teamwork environment. Seriousness level: The Kick-off clearly conveyed the OC\u0026rsquo;s commitment, making me aware of the necessary seriousness throughout the internship. Roadmap and Objectives Clear direction: Saw the entire 12-week roadmap visually for the first time, helping me proactively plan my studies. Project value: Understood that the ultimate goal is not just to learn, but also to create a real Cloud product. Initial Practical Lessons Importance of 2FA and Budget: Received direct guidance on AWS security and cost management operations—essential lessons from day one of using the Cloud. Teamwork: Quickly formed the team and began discussing coordination methods. Conclusion The Kick-off was more than just an introductory event; it was a training session that provided sufficient information and motivation for me to confidently start the 12-week journey of conquering the Cloud.\nEvent Photos Add your photos here Overall, the event successfully achieved its goals: providing information, establishing discipline, and igniting enthusiasm for a journey of specialized learning and development.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives Connect and get acquainted with members of First Cloud Journey (FCJ). Understand the organization and basic AWS services. Tasks to be carried out this week Day Task Start Date Completion Date Reference Material 2 - Attend the FCJ kick-off session.\n- Learn about the FCJ organization.\n- Form a team for collaborative project work. 06/09/2025 06/09/2025 3 - Create an AWS account.\n- Study cloud computing concepts.\n- Draw a sample architecture using draw.io. 09/09/2025 09/09/2025 REFER HERE 4 - Explore the objectives of the First Cloud Journey program and the AWS website.\n- Perform initial setup on the AWS account:\n+ Create a budget.\n+ Create user groups.\n+ Enable two-factor authentication (2FA).\n- Learn about the AWS Support Center and how to submit support requests. 10/09/2025 10/09/2025 REFER HERE 5 - Create a VPC and configure its settings.\n- Create and configure Subnets (including a public Subnet with auto-assigned public IPs).\n- Set up an Internet Gateway and attach it to the VPC.\n- Create and configure a Route Table, connecting it to the Internet Gateway.\n- Configure Subnet Associations.\n- Create Security Groups (public and private). 11/09/2025 14/09/2025 REFER HERE Week 1 Achievements Successfully participated in the FCJ kick-off session and connected with team members. Gained an understanding of the FCJ organization and its objectives. Formed a team to collaborate on projects. Created an AWS account and completed initial setup tasks: Set up a budget to monitor costs. Created user groups for access management. Enabled two-factor authentication (2FA) for enhanced security. Studied basic cloud computing concepts and successfully drew a sample architecture using draw.io. Learned how to navigate the AWS Support Center and submit support requests. Successfully completed VPC-related tasks: Created and configured a VPC. Set up Subnets, including a public Subnet with auto-assigned public IPs. Created and attached an Internet Gateway to the VPC. Configured a Route Table and connected it to the Internet Gateway. Established Subnet Associations. Created public and private Security Groups for secure resource access. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This page documents the entire Worklog carried out throughout the First Cloud Journey (FCJ) internship program at AWS. This document details the process of learning, implementing the Bandup IELTS project, troubleshooting system errors, and participating in specialized events over 12 weeks (approximately 3 months).\nDuring these 12 weeks, I transitioned from familiarizing myself with core Cloud concepts to building and optimizing a complete AI-powered Serverless application on AWS, completing 50+ AWS Skill Builder courses along the way.\nSummary of Work by Week: Week Focus Area Week 1 Completed mid-term exam, began implementing foundational CRUD functionalities, researched serverless architecture (Lambda, API Gateway, DynamoDB), and set up development environment. Week 2 Resolved AWS account issues, configured Hybrid DNS with Route 53 Resolver and VPC Peering, learned CloudFormation and Cloud9 for IaC development. Week 3 Finalized AI-powered Lambda functions, integrated Gemini API for IELTS evaluation, completed RAG pipeline for flashcard generation, and attended the final AWS Cloud Mastery Series. Week 4 Familiarization with FCJ, AWS account creation, basic Cloud concepts, and foundational network setup (VPC, Subnets, Internet Gateway). Week 5 Transitioned to AWS SAM, refactored CRUD functionalities, integrated Docker for build environment, and successfully deployed project to AWS overcoming local debugging challenges. Week 6 Mastered AWS Transit Gateway for centralized network management, deep dive into EC2 Auto Scaling, Lightsail, and Migration services (DMS, VM Import/Export). Week 7 Comprehensive review and knowledge consolidation of core AWS services (Compute, Storage, Networking, Database, Security) in preparation for the mid-term exam. Week 8 Mastered AWS Storage services (S3, Glacier, Storage Gateway), enhanced Python skills, finalized project architecture, and attended DevSecOps \u0026amp; Amazon Q Developer Webinar. Week 9 Mastered Amazon EC2 and VPC fundamentals, completed AWS Skill Builder courses on IAM, Budgets, EC2, and attended Cloud Day event for AI/Data insights. Week 10 Debugged CORS and template validation errors, integrated Frontend/Backend, completed Read/Delete functions, resolved Cognito authentication issues, and attended AWS Cloud Mastery Series #1. Week 11 Analyzed and optimized AWS costs, designed Serverless infrastructure architecture, learned RDS, DynamoDB, ElastiCache, and set up AWS Toolkit for VS Code. Week 12 Implemented Multi-Stack architecture for optimization, fixed the persistent CORS error, and began integrating AI Services (Lambda, Bedrock). AWS Skill Builder Learning Path (Weeks 2-5) Category Courses Completed Networking VPC, Route 53, VPC Peering, Transit Gateway, Networking Workshop Compute EC2, EC2 Auto Scaling, Lightsail, Lightsail Containers Security IAM, IAM Roles for EC2 Database RDS, DynamoDB, ElastiCache Migration VM Import/Export, DMS, SCT, Elastic Disaster Recovery DevOps CloudFormation, Cloud9, AWS CLI, AWS Toolkit for VS Code Cost Management AWS Budgets, Cost Explorer, Service Quotas, Right-Sizing Architecture Building Highly Available Web Applications AWS Skill Builder Learning Path (Weeks 6-10) Category Courses Completed Storage Static Website Hosting with S3, AWS Backup, CloudFront Reliability Data Protection with AWS Backup Development AWS Toolkit for VS Code, Serverless patterns Learning Progression Weeks 1-5: Foundation \u0026amp; Exploration\nCore AWS services (EC2, S3, VPC, IAM) Networking fundamentals (VPC, Route 53, Transit Gateway) Cost optimization and architecture design Infrastructure as Code (CloudFormation, Cloud9) Weeks 6-7: Consolidation \u0026amp; Assessment\nStorage services mastery (S3, Glacier, Storage Gateway) Disaster recovery and backup strategies Comprehensive exam preparation Mid-term exam completion Weeks 8-10: Implementation \u0026amp; Deployment\nServerless architecture implementation (Lambda, API Gateway, DynamoDB) AWS SAM framework adoption Docker integration for consistent builds Frontend-Backend integration Production deployment and debugging Weeks 11-12: Advanced Features \u0026amp; AI Integration\nMulti-stack architecture optimization AI services integration (Bedrock, Gemini API) RAG pipeline implementation Final project completion "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/3-blogstranslated/3.1-blog1/","title":"Analyze media content with AWS AI services","tags":[],"description":"","content":"Published: 2025-06-02 - Authors: Jack Bradham and Meera Balasubramaniam.\nOrganizations that manage large audio and video archives often struggle to extract value from their media libraries. Imagine a radio network with thousands of broadcast hours across multiple channels\u0026ndash;they must verify ad placement, identify interview segments, and analyze programming patterns. In this post, we show how you can automatically convert unstructured media files into searchable, analyzable content. By combining Amazon Transcribe, Amazon Bedrock, Amazon QuickSight, and Amazon Q, organizations can achieve the following goals:\nProcess and convert media into text as soon as files are uploaded Identify ad slots, interview segments, and topical programs Extract deeper insights with foundation models (FMs) Build a searchable knowledge base Deliver rich visualizations that drive decisions Enable natural-language queries across the entire media archive Visualize complex information with easy-to-understand graphics In the sections that follow, you will see how these AWS services work together so your organization can tap into the full potential of its media, whether you need to ensure advertising compliance, analyze content, or locate specific moments within thousands of hours of recordings.\nSolution overview This solution provides an event-driven media analytics pipeline that transforms how you manage and capture value from your content:\nOptimize content management - Automatically process media files on upload, saving time and reducing manual work. Surface richer insights - Produce accurate transcripts that capture context such as speaker, timing, and key moments. Harness AI at scale - Extract meaningful information and uncover hidden patterns without manually reviewing each file. Build a searchable knowledge base - Turn scattered media files into an explorable data hub for your entire team. Create tailored interfaces - Develop a customizable UI to search the data store. Deliver powerful visualizations - Convert findings into visuals that make even complex datasets easy to grasp. The following diagram illustrates the architecture.\nThis event-driven architecture automatically processes and analyzes media using AWS services. The workflow includes the following steps:\nUsers upload media files into an Amazon Simple Storage Service (Amazon S3) bucket. The New Media event triggers the first AWS Step Functions workflow, which performs initial classification based on file-name values and launches transcription. Amazon Transcribe converts audio to accurate, readable text. The transcripts are stored securely in an S3 bucket for further analysis, and the Transcription Complete event triggers the next phase. A second Step Functions workflow processes the transcripts. Using predefined prompts, Amazon Bedrock analyzes the transcripts to extract actionable insights. The extracted metadata is saved into an S3 data lake. The processed results are organized systematically, partitioned by date (year/month/day), and tagged with relevant attributes. This structured data enables natural-language querying through Amazon Q (when configured as a knowledge base), interactive visualizations through QuickSight, and seamless exploration. Amazon Athena powers data exploration against the data lake. Athena acts as the data source for QuickSight, turning complex datasets into clear, compelling visuals. This architecture transforms raw media into searchable, analyzable data while maintaining an organized structure for efficient access. The event-driven design ensures that new uploads are handled automatically, and the combination of AWS AI services unlocks a deeper understanding of the content. Each AWS service contributes to this transformation:\nAmazon Bedrock - Reviews transcripts to extract entities and insights: uses advanced foundation models, detects ads/interviews/programming segments, and derives actionable insights. Amazon EventBridge - Drives event-based orchestration: monitors new media and completed transcripts, and automatically triggers Step Functions workflows. AWS Lambda - Handles custom logic: bridges to Amazon Bedrock, executes tailored prompts, and adds flexible processing stages. Amazon Q - Acts as the user interface and Retrieval Augmented Generation (RAG) layer: provides enterprise-ready generative AI with built-in security controls such as SSO and responsible AI guardrails, enables natural-language queries across the archive, links responses to original media, and surfaces conversational access. Amazon QuickSight - Turns insights into compelling dashboards, visualizes media analytics end to end, and helps track ads, shows, and other patterns. Amazon S3 - Stores original media, transcripts, and processed insights; automatically emits events when new content arrives. AWS Step Functions - Orchestrates intake and AI analysis phases, complete with robust error handling and retries. Amazon Transcribe - Converts speech to accurate text, identifies speakers, and adds timestamps for precise navigation. Security considerations While this post focuses on the technical implementation of the media analytics pipeline, every production deployment must include comprehensive security controls.\nSecure storage in Amazon S3\nEnable server-side encryption with AWS Key Management Service (AWS KMS) keys. Apply restrictive bucket policies so that only authorized principals can access data. Turn on Amazon S3 Block Public Access at the account and bucket levels. Enable versioning for recovery. Create lifecycle policies to manage retention windows. Enable S3 access logging. Use presigned URLs for temporary access. Identity and access management\nCreate least-privilege service roles for Step Functions, Amazon Transcribe jobs, Amazon Bedrock API calls, and Athena queries. Enforce role-based access control (RBAC). Rotate credentials regularly and enable multi-factor authentication (MFA). Use AWS Organizations to manage multi-account environments. Network security\nDeploy VPC endpoints for Amazon S3, Athena, and QuickSight. Configure network ACLs and security groups aligned with your segmentation strategy. Enable VPC Flow Logs and use AWS PrivateLink where available. Configure route tables to control data flow. Data encryption\nApply AWS KMS encryption to S3 objects. Require TLS 1.2+ for every API interaction. Enable automatic key rotation and consider envelope encryption for sensitive data. Monitoring and detection\nEnable AWS CloudTrail for API auditing. Turn on Amazon GuardDuty for threat detection. Configure Amazon CloudWatch metrics, alarms, and log groups for application telemetry. Enable S3 server access logging and VPC Flow Logs. Access control\nEnforce fine-grained access for Amazon Bedrock models, Athena queries, and QuickSight dashboards. Review permissions regularly. Compliance requirements and governance policies may affect how you implement this solution. Consult AWS security best practices and partner with your security team to tailor the controls to your use case. For more guidance, see Best Practices for Security, Identity, \u0026amp; Compliance.\nPrerequisites To follow along, you need:\nAn AWS account with an IAM user (or role) that can access Lambda, Amazon Bedrock, Amazon S3, Amazon Transcribe, Step Functions, and IAM. Access to the foundation models you plan to use in Amazon Bedrock. Refer to Access Amazon Bedrock foundation models. An Amazon QuickSight Enterprise subscription. See Setting up for Amazon QuickSight. Create the S3 buckets This solution uses three distinct buckets to support the media analytics workflow:\nRaw media bucket - Stores the input files. Transcription outputs bucket - Stores Amazon Transcribe results. Processed insights bucket - Stores structured, AI-enriched data. Follow the Creating a general purpose bucket guide to provision each bucket.\nConfigure EventBridge Enable event notifications on the raw input bucket so EventBridge can trigger automation. The system watches for activity in the S3 buckets\u0026ndash;when new content is uploaded or transcripts are available, EventBridge invokes the right Step Functions workflow. For details, see Creating rules that react to events in Amazon EventBridge.\nExample filter for newly uploaded media:\n{ \u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: [\u0026#34;rawinputbucket\u0026#34;] } } } Example filter for newly added transcripts in the data lake:\n{ \u0026#34;source\u0026#34;: [\u0026#34;aws.s3\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;Object Created\u0026#34;], \u0026#34;detail\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: [\u0026#34;business-data-lake\u0026#34;] }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: [{ \u0026#34;suffix\u0026#34;: \u0026#34;.transcription\u0026#34; }] } } } Build the Step Functions workflows The orchestration layer includes two primary workflows. The first handles media ingestion and transcription, while the second manages AI analysis. Each workflow includes safeguards for potential errors and retry logic. For a walkthrough, see Learn how to get started with Step Functions. One diagram illustrates how new media is ingested, indexed, and transcribed, and another diagram shows how transcripts are analyzed downstream.\nSet up Amazon Transcribe Launch Amazon Transcribe jobs with the permissions you configured. Speech-to-text features such as language detection, speaker identification, and custom vocabularies improve accuracy for your media. Refer to How Amazon Transcribe works for setup details.\nConfigure Amazon Bedrock Drive the AI analyzer by crafting precise prompts that extract meaningful information. Amazon Bedrock examines transcripts to pinpoint segments, speakers, and topics, turning raw text into structured data. For guidance, see Design a prompt.\nExample prompt:\nYou will be reviewing a radio transcription to identify advertisements and extract relevant details. Your task is to analyze the provided transcript and output the results in a specific JSON format based on a given schema. Please follow these steps to complete the task: 1. Carefully read through the entire transcript. 2. Identify all advertisements within the transcript. Look for clear indicators such as product mentions, promotional language, or transitions from regular content to commercial content. 3. For each advertisement you identify, determine the following information: - Company: The name of the company being advertised - Start time: The timestamp in the transcript where the ad begins - End time: The timestamp in the transcript where the ad ends - Product: The specific product or service being advertised 4. Format your findings into a JSON object that follows the provided schema. Each advertisement should be a separate object within an array. 5. Ensure these fields in your response are provided for each advertisement.. All are required fields: company, starttime, endtime, product. 6. Use precise timestamps for start and end times. If exact times are not available, make a best estimate based on the transcript\u0026#39;s context. 7. If a particular field is unclear or not explicitly mentioned in the transcript, you may use \u0026#34;Unknown\u0026#34; as the value. 8. Only respond with json and nothing else. Do not provide comments or explain your answer. 9. Surround the JSON response with standard ```json markers Here\u0026#39;s an example of how your output should be formatted: { \u0026#34;advertisements\u0026#34;: [ { \u0026#34;company\u0026#34;: \u0026#34;TechGadgets Inc.\u0026#34;, \u0026#34;starttime\u0026#34;: \u0026#34;00:05:30\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;00:06:15\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;SmartHome Hub\u0026#34; }, { \u0026#34;company\u0026#34;: \u0026#34;FreshFoods Market\u0026#34;, \u0026#34;starttime\u0026#34;: \u0026#34;00:15:45\u0026#34;, \u0026#34;endtime\u0026#34;: \u0026#34;00:16:30\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;Organic Produce Delivery Service\u0026#34; } ] } Do not add any fields that are not specified in the schema, and ensure all required fields are present for each advertisement. Create a structured data lake Adopt a hierarchical data-organization strategy to enable efficient storage and analytics. Use AWS Glue crawlers to discover and catalog media metadata automatically. For more details, see Using crawlers to populate the Data Catalog.\nConfigure Athena tables so you can run SQL-based queries on the media insights. Example view:\nCREATE OR REPLACE VIEW \u0026#34;commercials_view\u0026#34; AS SELECT metadata.market market, metadata.station_call station_call, metadata.format_type format_type, CAST(metadata.timestamp AS timestamp) timestamp, ads.company adCompany, ads.product adProduct, ads.starttime, ads.endtime FROM (commercials CROSS JOIN UNNEST(advertisements) t (ads)); Deploy Amazon Q Enable natural-language interaction with the media archive using Amazon Q Business. Configure knowledge bases and metadata so users can search and access content through conversational queries. Use the processed insights bucket to populate the knowledge base. For setup, see Getting started with Amazon Q Business. The referenced screenshot shows example conversations with an AI assistant.\nBuild QuickSight dashboards With QuickSight, create visual analyses that make your insights tangible. Connect to Athena views to highlight ad trends, content analysis, and performance metrics through interactive dashboards. See Tutorial: Create an Amazon QuickSight dashboard for a complete walkthrough.\nValidate and optimize the media analytics solution After you deploy the architecture, execute these activities to ensure the solution meets your performance and business goals.\nEstablish comprehensive testing\nCompare transcripts with original media. Verify the accuracy of AI-generated insights. Use representative samples from your library. For example, select a recently processed radio show and compare its transcript with the original audio. Review the AI-generated insights to confirm that major events\u0026ndash;such as ad transitions or interview segments\u0026ndash;were detected correctly. To ensure the system handles all content types, sample a diverse mix, including morning talk shows, evening news, and weekend sports programming.\nBenchmark performance\nMeasure processing time across media types. Track AWS service resource usage. Identify bottlenecks in the workflow. Monitor how long it takes to process different file lengths\u0026ndash;from quick ads to full-length shows. Observe resource-consumption patterns to pinpoint bottlenecks, such as slower transcription for certain formats or opportunities to optimize parallelization.\nValidate real-world experience\nTest natural-language queries with Amazon Q. Confirm the relevance of search results. Gather feedback from target users. Have team members interact with Amazon Q using the questions they would normally ask, like \u0026ldquo;interviews about climate change last week.\u0026rdquo; Collect feedback from distinct personas\u0026ndash;content managers versus compliance reviewers\u0026ndash;to refine the system.\nThis structured testing approach, combined with real-world scenarios, lays the groundwork for a robust, user-friendly media analytics solution.\nAs you transition from initial deployment to production, optimization becomes critical for both cost control and user satisfaction. A network that processes thousands of hours per week can unlock major savings and discovery improvements by refining transcription accuracy or throughput. Marketing teams analyzing ad placement also depend on precise insights. Consider the following optimization strategies:\nTranscription refinement - Adjust language models for domain-specific terminology, fine-tune speaker-diarization settings, and use custom vocabularies. AI insight generation - Iterate on prompts for more focused analysis, experiment with different models, and align extraction parameters with business objectives. Scalability considerations - Load-test with higher media volumes, configure auto scaling, and monitor the cost efficiency of the architecture. Continuous improvement - Establish periodic reviews, track key performance indicators (KPIs), and evolve the solution based on live usage metrics. Start with a pilot deployment and expand gradually as you mature your media-analytics capabilities.\nClean up resources To avoid incurring ongoing charges, remove the resources you created after testing:\nQuickSight - Delete media-analytics dashboards and datasets, and deactivate the Enterprise subscription if it is no longer needed. S3 buckets - Empty and delete the raw media, transcription output, and processed insights buckets. EventBridge rules - Delete rules that monitor S3 bucket activity and remove their targets. Step Functions workflows - Delete the ingestion/transcription workflow and the AI-analysis workflow. Lambda functions - Remove functions that call Amazon Bedrock along with their IAM roles and policies. Data lake components - Delete Athena views and tables, AWS Glue crawlers and databases, and saved query results. Amazon Q - Delete the knowledge bases and any custom configurations. Amazon Bedrock - Remove custom prompts and deactivate FM access if you no longer need it. Amazon Transcribe - Delete custom vocabularies and stored transcription jobs. IAM resources - Delete custom roles and policies tied to this solution. Additional clean-up - Delete related CloudWatch log groups, alarms, or metrics, and remove saved Athena queries. Common use cases Multiple industries can leverage this architecture to mine value from audio and video content. Tailor the solution for your needs, such as broadcast management, corporate communications, educational archives, and more.\nMedia and broadcasting - Track advertising compliance, verify placement accuracy, and analyze programming at scale. Corporate and enterprise - Convert meeting recordings into searchable knowledge bases, identify key decisions and actions, and strengthen knowledge management. Education and training - Build comprehensive catalogs of course material, index training assets for quick lookup, and support continuous learning programs. Legal services - Produce timestamped transcripts, build searchable repositories of hearings, and streamline document review. Healthcare - Extract medical insights from consultations, categorize patient interactions, and assist clinical documentation. Government and public sector - Archive public meetings, auto-classify topics, and improve transparency and accessibility. Customer service - Analyze call-center recordings, identify service trends and pain points, and drive continuous experience improvements. By harnessing AI, organizations can turn raw audio and video into structured insights that enable faster, better-informed decisions.\nConclusion This article demonstrates how to use AWS services to convert unstructured media into actionable intelligence. By combining Amazon Transcribe, Amazon Bedrock, QuickSight, and Amazon Q, you can build an automated, scalable media-analytics solution tailored to your organization\u0026rsquo;s needs. The architecture provides:\nAutomated processing of media files at scale AI-generated insights Natural-language search capabilities Decision-support visualizations Flexible, maintainable infrastructure Organizations can now transform content into searchable knowledge, extract insights automatically, develop data-driven content strategies, and streamline operations through automation. As audio and video libraries continue to grow, the ability to process and extract value efficiently becomes increasingly vital. This architecture delivers a solid foundation for today\u0026rsquo;s requirements while remaining adaptable to future innovations.\nWe invite you to explore how media analytics with AWS AI services can address your organization\u0026rsquo;s unique challenges. Identify your top use cases and unlock the insights hidden across your media archives.\nAbout the authors Jack Bradham is a Senior Solutions Architect at AWS with more than 20 years of leadership experience in technology. Before joining AWS, he held key roles at Microsoft and Google, advising federal agencies and Global 100 enterprises. Jack holds an MBA in International Business from the University of South Carolina and has deep expertise in cloud computing, enterprise architecture, and business transformation. He is passionate about helping customers design scalable cloud solutions that meet business goals through innovation.\nMeera Balasubramaniam is a Senior Solutions Architect and Data Analytics Specialist at AWS. With over 20 years in technology, she helps organizations turn complex business challenges into actionable data solutions. Meera specializes in scalable data architectures, advanced analytics platforms, and cloud solutions. She brings extensive experience in enterprise data strategy, business intelligence, and machine learning, speaks frequently at industry events, and mentors the next generation of cloud experts.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/5.6.1-api-gateway/","title":"API Gateway","tags":[],"description":"","content":"Overview Create Amazon API Gateway as the entry point for AI service requests.\nCreate REST API Navigate to API Gateway → Create API → REST API Setting Value API name ielts-ai-api API type REST API Endpoint type Regional Create Resources and Methods Endpoints:\nMethod Path Description POST /writing/evaluate Submit writing sample POST /speaking/evaluate Submit audio recording POST /flashcards/generate Generate flashcard POST /upload/audio Upload audio POST /upload/document Upload document Configure SQS Integration For each POST endpoint, configure SQS integration:\nSelect method → Integration Request Integration type: AWS Service AWS Service: SQS HTTP method: POST Action: SendMessage Execution role: API Gateway role with SQS permissions Request Mapping Template:\nAction=SendMessage\u0026amp;MessageBody=$util.urlEncode($input.body)\u0026amp;QueueUrl=$util.urlEncode(\u0026#39;https://sqs.ap-southeast-1.amazonaws.com/{account}/ielts-writing-queue\u0026#39;) Enable CORS Select resource → Enable CORS Access-Control-Allow-Origin: * (or specific domain) Access-Control-Allow-Methods: POST, GET, OPTIONS Deploy API Actions → Deploy API Stage name: dev Note the invoke URL: https://{api-id}.execute-api.ap-southeast-1.amazonaws.com/dev AWS CLI Commands # Create REST API API_ID=$(aws apigateway create-rest-api \\ --name ielts-ai-api \\ --endpoint-configuration types=REGIONAL \\ --query \u0026#39;id\u0026#39; --output text) # Get root resource ROOT_ID=$(aws apigateway get-resources \\ --rest-api-id $API_ID \\ --query \u0026#39;items[?path==`/`].id\u0026#39; --output text) # Create /ai resource AI_RESOURCE=$(aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $ROOT_ID \\ --path-part ai \\ --query \u0026#39;id\u0026#39; --output text) # Create /ai/writing-assessment resource aws apigateway create-resource \\ --rest-api-id $API_ID \\ --parent-id $AI_RESOURCE \\ --path-part writing-assessment Next Steps Proceed to SQS Queues.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.7-cicd-pipeline/5.7.1-create-gwe/","title":"Connect GitLab repo &amp; create CodeBuild project","tags":[],"description":"","content":"Objective Configure two CodeBuild projects (frontend and backend) and a trigger from GitLab Release events that starts CodePipeline. CodePipeline invokes CodeBuild using the repository’s existing frontend-buildspec.yml and backend-buildspec.yml, and then deploys to ECS.\nAWS Resources CodeBuild projects: Frontend: Source = CodePipeline; Buildspec = frontend-buildspec.yml Backend: Source = CodePipeline; Buildspec = backend-buildspec.yml CodePipeline (later step) consuming artifacts and deploying to ECS Create CodeBuild Projects \u0026amp; Connect GitLab Repository In the creating new CodeBuild Project configuration section, select Default project. In the Source section, choose GitLab and Band-Up repository. Leave default configurations for Environment. Specify Band-Up own buildspec for frontend/backend like the following image: Submit and create the other frontend/backend CodeBuild project likewise. Summary You now have two CodeBuild projects (frontend and backend) ready to be invoked by CodePipeline. A GitLab Release event can trigger CodePipeline, which then runs each project with its corresponding frontend-buildspec.yml and backend-buildspec.yml. In subsequent steps, CodePipeline will take the build artifacts and deploy to ECS.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Post-Event Report: “DX Talk#7: Reinventing DevSecOps with AWS Generative AI” Event Objectives Explore strategic and practical perspectives on the powerful transformation of AI in DevSecOps. Introduce comprehensive AI integration solutions: From process automation, intelligent risk prediction, to continuous security testing in CI/CD, and rapid response to potential threats. \u0026ldquo;Dissect\u0026rdquo; Case Studies from CMC Global and AWS on how leading businesses apply AI in addressing security and system operations issues. Provide orientation for DevSecOps Engineers in a highly demanding market, emphasizing continuous learning and specialization. Speaker List Mr. Le Thanh Duc – Cloud Delivery Manager, CMC Global Mr. Du Quoc Thanh – Technical Leader, CMC Global Special Guest: Mr. Van Hoang Kha – Cloud Engineer, AWS Community Builder Key Highlights Comprehensive AI Integration Solution in DevSecOps Process automation, intelligent risk prediction. Continuous security testing in the CI/CD pipeline. Rapid response to potential threats. Case Studies and Practical Lessons Directly \u0026ldquo;dissected\u0026rdquo; projects from CMC Global and AWS to learn how businesses apply AI in addressing security and system operations issues. Explored how AI helps eliminate rigid \u0026ldquo;gatekeepers,\u0026rdquo; 24/7 on-call shifts, and costly manual response procedures in DevSecOps. Career Orientation and Professional Development Discussed that continuously updating trends and enhancing specialization are the key to engineers conquering complex projects. Key Takeaways AI Integration and Automation Clearly understood the Comprehensive AI Integration Solution in DevSecOps, including process automation and continuous security testing in CI/CD. Grasped how AI can enable intelligent risk prediction and accelerate threat response. Modern DevSecOps Mindset Recognized the importance of integrating security into the SDLC (Software Development Life Cycle). Understood how popular tools like Jenkins (CI/CD), SonarQube (SAST), OWASP ZAP (DAST), and Terraform (IaC) operate. Generative AI Tools Explored Amazon Q Developer – a powerful generative AI assistant supporting code generation, testing, vulnerability scanning, and software development optimization on AWS. Application to Work (From Worklog Week 6) Apply the DevSecOps mindset to the current project, integrating security from the early development stages. Research Amazon Q Developer for integration into the workflow to support code generation and security testing. Investigate CI/CD tools and SAST/DAST security testing for inclusion in the project\u0026rsquo;s development roadmap. Event Experience The workshop \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; provided a strategic and practical, multi-faceted view of the future of Cloud security and system operations:\nPerspectives from Experts Listened to strategic perspectives from leading experts like Mr. Le Thanh Duc, Mr. Du Quoc Thanh, and special guest Mr. Van Hoang Kha. Through Case Studies, I understood how large enterprises apply Generative AI to solve complex security and operational challenges. Value of AI in DevSecOps Clearly saw AI\u0026rsquo;s potential to eliminate manual bottlenecks (like rigid gatekeepers, 24/7 shifts), transforming DevSecOps into a more automated and intelligent process. The event provided specific solutions for comprehensive AI integration, from process automation to risk prediction. Professional Orientation Emphasized the importance of continuously updating trends and enhancing specialization for DevSecOps engineers to conquer complex future projects. Had the opportunity for direct Q\u0026amp;A with the speakers to address questions about integrating AI into my own project. Conclusion The event was an invaluable source of information, helping me understand not only the technology but also career direction, especially how to use powerful tools like Amazon Q Developer to boost productivity and integrate security into the development workflow.\nEvent Photos Overall, the event successfully explored the strategic and practical perspectives of DevSecOps with Generative AI, providing inspiration and clear direction for young engineers.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives Complete Module 2, mastering Amazon EC2 and VPC fundamentals. Prepare and configure essential resources for launching EC2 instances. Explore Amazon Route 53 and DNS management concepts. Participate in Cloud Day event for AI and Data insights. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study VPC architecture and networking components in depth.\n- Learn AWS architecture design principles from Mentor Gia Hung\u0026rsquo;s lectures.\n- Complete AWS Skill Builder: Networking Essentials with Amazon VPC. 15/09/2025 16/09/2025 AWS VPC Documentation 3 - Create VPC resources to prepare for EC2 instance deployment.\n- Launch EC2 instances using configured resources.\n- Study Security Groups and Network ACLs.\n- Complete: Compute Essentials with Amazon EC2. 16/09/2025 17/09/2025 AWS EC2 Documentation Introduction to Amazon EC2 Deploying FCJ Management Application with Auto Scaling Group 4 - Resolve AWS account authentication issues by submitting verification documents.\n- Complete: Creating Your First AWS Account and Getting Help with AWS Support. 17/09/2025 20/09/2025 AWS Support Request Support with AWS Support 5 - Attend Cloud Day event.\n- Gain insights into AI and Data trends.\n- Network with prominent mentors in the AWS community. 18/09/2025 18/09/2025 Cloud Day Event AWS Skill Builder Courses Completed Course Category Status Creating Your First AWS Account Getting Started ✅ Managing Costs with AWS Budgets Cost Management ✅ Getting Help with AWS Support Support ✅ Access Management with AWS IAM Security ✅ Networking Essentials with Amazon VPC Networking ✅ Compute Essentials with Amazon EC2 Compute ✅ Instance Profiling with IAM Roles for EC2 Security ✅ Week 2 Achievements Technical Skills Acquired:\nGained comprehensive understanding of VPC architecture and EC2 fundamentals Mastered the process of preparing resources for EC2 instance deployment: Created and configured Subnets for network segmentation Set up Internet Gateway for external connectivity Configured Route Tables to manage traffic routing Implemented Security Groups to control inbound/outbound traffic Understood IAM roles and instance profiles for secure EC2 access Learned AWS cost management strategies using AWS Budgets Cloud Day Event Highlights:\nParticipated in networking sessions with AWS mentors and industry professionals Gained valuable insights into AI and Data market trends Understood the future potential and market demand for AI technologies Received commemorative gift from event organizers Key Takeaways:\nVPC is the foundation for all AWS networking - understanding it is critical Security Groups act as virtual firewalls at the instance level IAM roles eliminate the need for hardcoded credentials in EC2 instances AWS Budgets help prevent unexpected costs through proactive monitoring "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.3-network/5.3.2-alb/","title":"Application Load Balancer (ALB)","tags":[],"description":"","content":"The Application Load Balancer (ALB) serves as the entry point for all incoming traffic to our platform. It distributes requests to the appropriate containers (Frontend or Backend) and handles SSL termination.\n1. Create Security Group for ALB Before creating the load balancer, we need a firewall rule that allows public access.\nGo to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Security group name: alb-sg. Description: Allow http and https traffic. VPC: Select band-up-vpc. Inbound rules: Add the following rules to allow traffic from anywhere: Type: HTTP | Port: 80 | Source: Anywhere-IPv4 (0.0.0.0/0). Type: HTTPS | Port: 443 | Source: Anywhere-IPv4 (0.0.0.0/0). Click Create security group. 2. Create Target Group The ALB needs to know where to route the traffic. We will create a Target Group for our Frontend service first.\nGo to EC2 Dashboard \u0026gt; Target groups \u0026gt; Create target group. Choose a target type: Select IP addresses (Required for ECS Fargate). Target group name: target-bandup-fe. Protocol: HTTP. Port: 3000 (Our Next.js frontend runs on port 3000). VPC: Select band-up-vpc. Click Next. Register targets: Since we haven\u0026rsquo;t deployed the ECS tasks yet, skip this step and click Create target group. 3. Create Application Load Balancer Now, we aggregate everything into the Load Balancer.\nStep 1: Basic Configuration\nGo to Load Balancers \u0026gt; Create load balancer. Select Application Load Balancer and click Create. Load balancer name: bandup-public-alb. Scheme: Internet-facing (To allow public access). IP address type: IPv4. Step 2: Network Mapping\nVPC: Select band-up-vpc. Mappings: Select two Availability Zones (ap-southeast-1a and ap-southeast-1b). Subnets: IMPORTANT - Select the Public Subnets (public-subnet-1 and public-subnet-2) created in the previous section. Note: Ensure you do not select Private subnets, otherwise the ALB cannot be reached from the internet. Step 3: Security Groups \u0026amp; Listeners\nSecurity groups: Deselect the default one and choose alb-sg. Listeners and routing: Protocol: HTTP | Port: 80. Default action: Forward to target-bandup-fe. Click Create load balancer. Your ALB is now provisioning. Once active, it will be ready to route traffic to your frontend application.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.5-setup-be/5.5.2-rds/","title":"Create PostgreSQL RDS","tags":[],"description":"","content":"In this step, we provision an Amazon RDS for PostgreSQL instance. This will serve as the primary persistent data store for the IELTS BandUp platform. We will configure it for high availability and security within our VPC.\n1. Configure Security Groups Before creating the database, we need to define the firewall rules.\nStep 1.1: Create Backend Security Group This group is for the ECS Fargate tasks (the application layer) to control outbound traffic.\nName: ecs-backend-sg. Inbound: Allow port 8080 (Spring Boot default) from the ALB. Step 1.2: Create RDS Security Group This group is attached to the database itself.\nName: rds-sg. Inbound: Allow PostgreSQL traffic (Port 5432) only from the ecs-backend-sg created above (or the VPC CIDR for testing). This ensures only our application can talk to the database. 2. Create DB Subnet Group RDS needs to know which subnets it is allowed to use. We will group our private database subnets together.\nNavigate to Amazon RDS \u0026gt; Subnet groups \u0026gt; Create DB subnet group. Name: bandup-db-subnet-group. VPC: Select band-up-vpc. Add subnets: Select the Availability Zones and choose the dedicated private-database-subnet-1 and private-database-subnet-2. 3. Create the Database Now, we provision the PostgreSQL instance.\nNavigate to Databases \u0026gt; Create database. Choose a database creation method: Standard create. Engine options: PostgreSQL (Version 17.6 or latest). Availability and durability: Select Multi-AZ DB instance. This creates a primary DB and a synchronous standby replica in a different Availability Zone for automatic failover. Settings: DB instance identifier: bandup-db. Master username: postgres. Credential management: Self managed. Master password: Set a strong password (save this for later). Instance configuration: DB instance class: Burstable classes -\u0026gt; db.t4g.micro (Cost-effective for workshops/dev). Storage: gp3 (General Purpose SSD) with 20 GiB. Connectivity: Compute resource: Don\u0026rsquo;t connect to an EC2 compute resource. VPC: band-up-vpc. DB subnet group: bandup-db-subnet-group (Created in step 2). Public access: No (Critical for security). VPC security group: Select existing -\u0026gt; rds-sg. Database authentication: Password authentication. Monitoring: Enable Performance Insights (retention 7 days). Additional configuration: Initial database name: band_up (Important: Hibernate will look for this DB name). Backup: Enable automated backups. Encryption: Enable encryption. Click Create database. The provisioning process will take a few minutes. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"This workshop is designed for DevOps Engineers, Cloud Architects, and Full-stack Developers who aim to deploy a modern, AI-integrated application on AWS.\nTo successfully complete this workshop, participants are expected to possess the following knowledge, skills, and tools.\n1. Technical Knowledge Requirements AWS Fundamentals Console Navigation: Familiarity with the AWS Management Console. Core Services: Basic understanding of compute services like Amazon EC2 and AWS Fargate, networking concepts within Amazon VPC, and storage using Amazon S3. IAM \u0026amp; Security: Understanding of AWS Identity and Access Management (IAM), specifically Roles, Policies, and the principle of least privilege. Containerization \u0026amp; Orchestration Docker: Proficiency in creating Dockerfiles, building images, and running containers locally. Understanding of concepts like layers, exposing ports, and environment variables is essential. Refer to the Docker Documentation. ECS Concepts: Familiarity with Amazon ECS terminology including Task Definitions, Services, Clusters, and the operational differences between EC2 and Fargate launch types. DevOps \u0026amp; CI/CD Git: Proficiency in version control (commit, push, branching) to manage source code and trigger automated pipelines. CI/CD Flow: Understanding of Continuous Integration and Continuous Delivery principles using tools like AWS CodePipeline and AWS CodeBuild. Networking Basics Protocols: Understanding of HTTP/HTTPS, DNS resolution with Amazon Route 53, and load balancing concepts using Application Load Balancer. Network Security: Knowledge of IP addressing (CIDR blocks) and traffic control using Security Groups. 2. Environment Setup Before starting the workshop, ensure your local development environment is equipped with the following tools:\nAWS Account: An active AWS account with Administrator access to provision resources. IDE: A code editor such as Visual Studio Code or IntelliJ IDEA. Command Line Tools: AWS CLI (v2): Installed and configured with your account credentials. Installation Guide. Git: Installed for cloning repositories. Downloads. Docker Desktop: Running locally to inspect or build images if necessary. Get Docker. 3. Service Quotas \u0026amp; Costs Cost Alert: This workshop utilizes resources that are not covered by the AWS Free Tier, including:\nNAT Gateways (Hourly charge + Data processing fees) Application Load Balancers ECS Fargate Tasks (vCPU/Memory usage) Amazon RDS \u0026amp; ElastiCache Please ensure you clean up resources immediately after finishing the workshop to avoid unexpected charges. A cleanup guide is provided at the end of this documentation.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.4-setup-fe/5.4.2-ecr/","title":"Setup ECR &amp; IAM Role","tags":[],"description":"","content":"In this step, we prepare the AWS infrastructure required to store our container images. This involves verifying the initial state, creating a necessary IAM Role for ECR replication, and provisioning the repository.\n1. Verify ECR State First, verify the current state of the Private Registry. Initially, there are no repositories created.\n2. Create IAM Role for ECR We need to create a Service-Linked Role that allows Amazon ECR to perform replication actions across regions and accounts.\nNavigate to IAM \u0026gt; Roles \u0026gt; Create role. Select trusted entity: Choose AWS service. Service or use case: Select Elastic Container Registry from the list. Use case: Select Elastic Container Registry - Replication to allow ECR to replicate images. Add permissions: Confirm that the ECRReplicationServiceRolePolicy is attached. This managed policy grants the necessary permissions. Name, review, and create: The role name is automatically set to AWSServiceRoleForECRReplication. Review the configuration and create the role. Result: The role is successfully created and listed in the IAM Roles dashboard. 3. Create ECR Repository Now we create the repository to store the frontend image.\nNavigate to Amazon ECR \u0026gt; Create repository. General settings: Repository name: band-up-frontend. Visibility settings: Private. Image tag settings: Keep Mutable enabled to allow overwriting image tags. Result: The band-up-frontend repository is successfully created with AES-256 encryption enabled by default. 4. Configure CLI Access To push images from your local machine, you need programmatic access via the AWS CLI. We will generate an Access Key for your IAM User.\nNavigate to IAM Dashboard \u0026gt; Users \u0026gt; Select your user (e.g., NamDang). Open the Security credentials tab and click Create access key. Use case: Select Command Line Interface (CLI). Description tag: Enter a meaningful description (e.g., ECR Push Key) and click Create access key. Retrieve Keys: Important! Copy or download the Access Key ID and Secret Access Key immediately, as you cannot retrieve the Secret Key later. 5. Configure AWS CLI Open your terminal and configure the AWS CLI with the credentials you just generated.\naws configure Enter the following details when prompted:\nAWS Access Key ID: [Paste your key] AWS Secret Access Key: [Paste your secret] Default region name: ap-southeast-1 Default output format: json 6. Push Image to ECR Now that the CLI is configured, we can authenticate Docker and push our image.\nStep 1: Login to ECR Run the login command to authenticate your Docker client with the AWS registry.\naws ecr get-login-password --region ap-southeast-1 | docker login --username AWS --password-stdin [Account-ID]https://www.google.com/search?q=.dkr.ecr.ap-southeast-1.amazonaws.com Output: Login Succeeded\nStep 2: Tag the Image We need to tag our local image band-up-frontend:latest with the full ECR repository URI and a version tag (e.g., v1.0.0).\ndocker tag band-up-frontend:latest [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 Step 3: Push the Image Execute the push command to upload the layers to AWS.\ndocker push [Account-ID].dkr.ecr.ap-southeast-1.amazonaws.com/band-up-frontend:v1.0.0 7. Final Verification Return to the Amazon ECR Console and open the band-up-frontend repository. You should see the image with the tag v1.0.0 listed successfully.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/3-blogstranslated/3.2-blog2/","title":"Improve conversational AI response times with Amazon Bedrock streaming API and AWS AppSync","tags":[],"description":"","content":"Published: 2025-07-09 - Authors: Salman Moghal and Philippe Duplessis-Guindon.\nMany enterprises now use large language models (LLMs) in Amazon Bedrock to derive insights from internal data. Amazon Bedrock is a fully managed service that provides access to high-performing foundation models (FMs) from leading AI companies such as AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a rich set of tools to build generative AI applications with security, privacy, and responsible AI guardrails.\nOrganizations deploying conversational AI systems often face a common challenge: although APIs can quickly answer straightforward questions, complex queries that require reasoning and action (ReAct) chains take longer to process, which degrades the user experience. The issue is especially pronounced in regulated industries, where stringent security requirements raise the complexity even further. For example, a global financial-services organization with more than USD 1.5 trillion in assets under management faced this exact challenge. Even after successfully integrating multiple LLMs and data sources, they still needed a solution that honored strict security protocols (including Virtual Private Cloud, or VPC, boundaries and enterprise OAuth) while improving response time for complex prompts.\nAWS AppSync is a fully managed service that lets developers build serverless GraphQL APIs with real-time capabilities. This post shows how to combine AWS AppSync subscriptions with Amazon Bedrock streaming endpoints to deliver incremental LLM responses. We provide an enterprise-ready implementation blueprint so regulated organizations can maintain compliance and boost user experience through instantaneous, real-time streaming responses.\nSolution overview The solution uses AWS AppSync to launch an asynchronous conversational workflow. A core AWS Lambda function interacts with the Amazon Bedrock streaming API. As the LLM generates tokens, they are streamed to the frontend through AWS AppSync mutations and subscriptions.\nA reference implementation of the Lambda function and AWS AppSync API is available in the accompanying sample code. The reference architecture diagram (described below) shows how the AWS services integrate to deliver the target outcome.\nHere is how user requests are processed and how users receive real-time responses from an Amazon Bedrock LLM:\nWhen a user loads the UI, the application subscribes to the GraphQL subscription onSendMessage(), which returns whether the WebSocket connection succeeded. After the user enters a prompt, the application invokes the GraphQL query getLLMResponse, which triggers the data-source Lambda function. The data-source Lambda publishes an event to an Amazon Simple Notification Service (Amazon SNS) topic, and a 201 response is returned to the user, completing the synchronous flow. The sequence diagram illustrates these steps in detail. Next, the Orchestrator Lambda is invoked by the published SNS event and starts the streaming flow by calling the Amazon Bedrock API InvokeModelWithResponseStream. Amazon Bedrock receives the user prompt, initiates a stream, and starts sending token chunks back to the Lambda function. Each time the Orchestrator Lambda receives a token chunk, it calls the GraphQL mutation sendMessage. That mutation triggers the onSendMessage subscription with partial LLM output, and the UI renders those tokens as soon as they arrive. A second sequence diagram breaks down this streaming phase step by step.\nData and API design The AppSync GraphQL schema includes three operation types: query, subscription, and mutation.\nQuery operation input GetLlmResponseInput { sessionId: String! message: String! locale: String! } type Query { getLlmResponse(input: GetLlmResponseInput!): GetLlmResponse @aws_api_key } The synchronous getLlmResponse query accepts a unique sessionId, the user\u0026rsquo;s locale, and the prompt message.\nThe frontend must send a unique sessionId to identify the active chat session. The ID remains the same for the duration of the conversation, and a new ID is generated if the user reloads the UI. The locale indicates the user\u0026rsquo;s language preference (for example en_US). Refer to Languages and locales supported by Amazon Lex V2 for valid values. The message contains the user prompt that is passed to the LLM. Subscription operation type Subscription { onSendMessage(sessionId: String!): SendMessageResponse @aws_subscribe(mutations: [\u0026#34;sendMessage\u0026#34;]) @aws_api_key } The onSendMessage subscription takes the sessionId and allows the frontend to open a WebSocket connection. This setup ensures the UI receives every response emitted by the sendMessage mutation for the same session.\nMutation operation input SendMessageInput { sessionId: String! message: String! locale: String! } type Mutation { sendMessage(input: SendMessageInput!): SendMessageResponse @aws_api_key @aws_iam } The sendMessage mutation accepts a SendMessageInput payload. All required fields (denoted by !) must be supplied so that the message can be delivered to the frontend. The Orchestrator Lambda invokes this mutation to push partial LLM tokens to the UI; the next section covers this function.\nAWS AppSync data-source Lambda function AWS AppSync invokes the data-source Lambda whenever the frontend calls the getLlmResponse query. This synchronous Lambda implementation, provided in the bedrock-appsync-ds-lambda GitHub repository, extracts the user prompt from the query, publishes it to an Amazon SNS topic, and returns a success status code to confirm the message reached the backend for processing.\nAWS AppSync Orchestrator Lambda function The Orchestrator Lambda runs whenever an event is published to the SNS topic. It initializes the Amazon Bedrock streaming API by calling the Boto3 method converse_stream:\nbrt = boto3.client(service_name=\u0026#34;bedrock-runtime\u0026#34;, region_name=\u0026#34;us-west-2\u0026#34;) messages = [] message = { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: parsed_event[\u0026#34;message\u0026#34;]}] } messages.append(message) response = brt.converse_stream( modelId=model_id, messages=messages ) The Lambda creates a Bedrock Runtime client, parses the SNS payload, and builds a prompt that follows the Amazon Bedrock Messages API (with role and content). The modelId is sourced from an environment variable, and the messages parameter must adhere to the Messages API schema. When converse_stream returns successfully, the Lambda iterates through the streaming body to send partial tokens to the frontend:\nstream = response.get(\u0026#34;body\u0026#34;) if stream: self.appsync = AppSync(locale=\u0026#34;en_US\u0026#34;, session_id=session_id) self.appsync.invoke_mutation(DEFAULT_STREAM_START_TOKEN) event_count = 0 buffer = \u0026#34;\u0026#34; for event in stream: if event: if list(event)[0] == \u0026#34;contentBlockDelta\u0026#34;: event_count += 1 buffer += event[\u0026#34;contentBlockDelta\u0026#34;][\u0026#34;delta\u0026#34;][\u0026#34;text\u0026#34;] if event_count \u0026gt; 5: self.appsync.invoke_mutation(buffer) event_count = 0 buffer = \u0026#34;\u0026#34; if len(buffer) != 0: self.appsync.invoke_mutation(buffer) self.appsync.invoke_mutation(DEFAULT_STREAM_END_TOKEN) As soon as the LLM begins responding, the Lambda sends a DEFAULT_STREAM_START_TOKEN through AWS AppSync to signal the UI to start rendering output. The Lambda buffers incoming chunks and only invokes the mutation after five chunks, which reduces network calls and latency while maintaining a fluid experience. After the final tokens are delivered, the Lambda emits DEFAULT_STREAM_END_TOKEN to tell the frontend the stream is complete. See the bedrock-orchestrator-lambda repository for the full implementation.\nPrerequisites To deploy the solution, install the Terraform CLI in your environment and complete the prerequisites listed in the companion GitHub documentation.\nDeploy the solution Open a terminal window. Navigate to the deployment directory. Edit sample.tfvars and update the variables for your AWS environment: region = \u0026#34;us-west-2\u0026#34; lambda_s3_source_bucket_name = \u0026#34;YOUR_DEPLOYMENT_BUCKET\u0026#34; lambda_s3_source_bucket_key = \u0026#34;PREFIX_WITHIN_THE_BUCKET\u0026#34; Run the following commands: terraform init terraform apply -var-file=\u0026#34;sample.tfvars\u0026#34; Detailed deployment instructions are available in the Deploy the solution section of the GitHub repository.\nTest the solution Use the provided sample web UI and run it inside VS Code (see the README for instructions). This UI subscribes to real-time responses and displays incremental tokens as they arrive from Amazon Bedrock.\nClean up Use the same sample.tfvars file from deployment to tear down the resources:\nterraform destroy -var-file=\u0026#34;sample.tfvars\u0026#34; Conclusion This post demonstrated how to integrate the Amazon Bedrock streaming API with AWS AppSync subscriptions to dramatically improve conversational AI responsiveness and user satisfaction. By adopting streaming, the global financial-services organization cut initial response times for complex queries by roughly 75 percent (from 10 seconds to just 2-3 seconds), so users see answers as soon as they are generated instead of waiting for the full completion. The business impact is clear: lower abandonment rates, higher engagement, and a more responsive AI experience. You can deploy the provided Lambda code and Terraform templates to bring the same benefits to your environment quickly.\nFor additional flexibility, AWS AppSync Events offers an alternative deployment pattern that can further enhance real-time capabilities through a fully managed WebSocket API. By resolving the tension between comprehensive AI answers and speed, this streaming approach enables organizations to maintain high-quality interactions while delivering the responsiveness modern users expect.\nAbout the authors Salman Moghal is a Principal Solutions Architect at AWS. He helps customers design secure, scalable data and AI solutions across regulated industries. Salman focuses on aligning technical architectures with business priorities and has deep experience with enterprise identity, VPC security, and generative AI platforms.\nPhilippe Duplessis-Guindon is a Senior Solutions Architect at AWS specializing in analytics and application integration. He partners with customers to launch real-time data applications and conversational AI experiences using services such as Amazon Bedrock, AWS AppSync, and AWS Lambda. Philippe is passionate about helping enterprises deliver responsive, compliant AI solutions.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.7-cicd-pipeline/5.7.2-test-gwe/","title":"Create CodePipeline with GitLab tag trigger","tags":[],"description":"","content":"CodePipeline Design (Source -\u0026gt; Build → Deploy) Build (CodeBuild) Project: the project’s CodeBuild project (Source = CodePipeline) Environment variables: as needed for this project’s build Buildspec: use the repository’s existing buildspec.yml CodePipeline Set up guidelines On choosing pipeline creation option section, choose Build custom pipeline. In pipeline settings tab, specify the pipeline name and use default settings for the pipeline. Turn on webhook events and add filter type of tags. Make sure to set the tag patterns of \u0026ldquo;v*\u0026rdquo;. Add frontend/backend project using AWS CodeBuild in build stage. For the deploy stage, choose Amazon ECS as deploy provider. Specify its cluster and service to deploy. Also make sure to fill in the image definition file form like the image. Submit and then the pipeline is now created. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/2-proposal/","title":"Proposal","tags":[],"description":"","content":"IELTS Self-Learning Web System An Intelligent Platform for Independent IELTS Preparation 1. Executive Summary The IELTS Self-Learning Web System is a comprehensive online platform designed to support students in their independent IELTS preparation journey. The platform provides an all-in-one solution featuring user management, educational blogs, interactive study rooms, practice tests, and AI-powered flashcard systems. Leveraging modern web technologies and AI integration, the system offers personalized learning experiences, real-time collaboration features, and automated assessment tools to help students achieve their IELTS goals efficiently.\n2. Problem Statement What\u0026rsquo;s the Problem? Many IELTS learners face challenges in finding affordable, comprehensive, and interactive self-study platforms. Traditional learning methods lack real-time collaboration, personalized feedback, and integrated practice tools. Students often struggle with:\nLimited access to quality practice materials and mock tests Lack of immediate feedback on Speaking and Writing tasks Difficulty finding study partners and maintaining study discipline Fragmented resources across multiple platforms High costs of traditional IELTS preparation courses The Solution The IELTS Self-Learning Web System provides a unified platform with five core features:\nUser Management System: Multi-tier membership (Guest, Member, Premium, Admin) with social authentication (Google, Facebook), personal profiles, and messaging capabilities.\nEducational Blog Platform: Community-driven content with CRUD operations, genre/tag categorization, advanced filtering, commenting, reporting, and favoriting features.\nInteractive Study Rooms: Virtual study spaces with scheduling, voice/video calls, Pomodoro timers, background music, dictionary integration, and translation support for enhanced learning.\nIELTS Practice Tests: Comprehensive mock tests for Reading and Listening with automatic vocabulary extraction into flashcards, plus AI-powered assessment for Speaking and Writing tasks, and dictation exercises.\nQuizlet Flashcard System: Intelligent flashcard creation with multiple study modes, automatic vocabulary extraction from texts, AI-generated quizzes from uploaded materials, and sharing capabilities.\nBenefits and Return on Investment For Students: Cost-effective alternative to expensive courses, personalized learning paths, 24/7 access to study materials, AI-powered feedback, and collaborative learning environment. Educational Value: Develops self-discipline, provides comprehensive IELTS preparation, enables peer learning, and offers trackable progress. Technical Benefits: Scalable architecture, modern tech stack, AI integration for automated assessment, and potential for future feature expansion. Market Potential: Growing demand for online IELTS preparation, subscription-based revenue model (Guest → Member → Premium), and potential for partnerships with educational institutions. 3. Solution Architecture The platform employs a modern full-stack web architecture designed for scalability, real-time collaboration, and AI integration. The system consists of five major modules working together to provide a comprehensive IELTS learning experience. The infrastructure uses an active-passive Multi-AZ deployment on AWS ECS for high availability, where AZ-1 handles all active traffic and AZ-2 serves as a standby for automatic failover.\nSystem Architecture Overview: Technology Stack Frontend:\nNext.js 14+: Modern React framework for responsive web application TypeScript: Type-safe development TailwindCSS: Utility-first styling WebRTC: Real-time video/voice communication Socket.io Client: Real-time messaging and collaboration Backend:\nSpring Boot 3.x: Monolithic RESTful API architecture Java 17+: Backend programming language Spring Security: Authentication and authorization Spring WebSocket: Real-time communication JWT: Secure token-based authentication OAuth 2.0: Social login integration (Google, Facebook) Database:\nPostgreSQL 14+: Primary relational database for all data (users, tests, blogs, flashcards, study sessions) Amazon ElastiCache (Redis): Caching and session management Cloud Infrastructure (AWS):\nAmazon ECS (Elastic Container Service): Container orchestration with active-passive Multi-AZ deployment Application Load Balancer: Routes traffic to active AZ with automatic failover Amazon RDS for PostgreSQL: Multi-AZ deployment (active primary in AZ-1, passive standby in AZ-2) Amazon S3: Media and file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Monitoring and logging Third-party Services:\nGoogle Gemini Flash API (Free Tier): AI-powered Speaking/Writing assessment and content generation Free Dictionary APIs: Word definitions and examples Open-source translation libraries: Context-aware translation (alternative to paid APIs) Component Design 1. User Management Module:\nMulti-tier authentication system (Guest, Member, Premium, Admin) Social OAuth integration (Google, Facebook) User profile management with learning statistics Real-time messaging system Password recovery and email verification 2. Blog Platform Module:\nCRUD operations for blog posts Genre and tag management system Advanced search and filtering (by tags, genres, date, popularity) Comment system with nested replies Content reporting and moderation Favorite/bookmark functionality SEO-optimized content delivery 3. Study Room Module:\nVirtual study room creation and management Study session scheduling with calendar integration WebRTC-based voice and video calls Pomodoro timer with customizable intervals Background music library with focus playlists Integrated dictionary with free dictionary APIs Translation support using open-source libraries Screen sharing for collaborative study Real-time participant management 4. IELTS Practice Test Module:\nTest bank management (Reading, Listening, Speaking, Writing) Timed test simulations with auto-submission Automatic vocabulary extraction from Reading passages to flashcards AI-powered Speaking assessment using Gemini Flash (pronunciation, fluency, coherence) AI-powered Writing assessment using Gemini Flash (grammar, vocabulary, task achievement) Dictation exercises for Listening practice Detailed score reports and analytics Progress tracking across test types 5. Quizlet Flashcard Module:\nCRUD operations for flashcard sets Multiple study modes (flashcards, learn, test, match, write) Spaced repetition algorithm (SRS) Automatic vocabulary extraction from text passages AI-generated quizzes from uploaded documents/texts using Gemini Flash Collaborative flashcard sets with sharing Study statistics and mastery tracking Import/export functionality 4. Technical Implementation Implementation Phases\nPhase 1: Planning and Design (Weeks 1-2)\nRequirements gathering and user story definition System architecture design and AWS infrastructure planning Database schema design for PostgreSQL UI/UX wireframes and mockups API endpoint design for Spring Boot Third-party service evaluation and integration planning Multi-AZ architecture setup on AWS ECS Phase 2: Core Development (Weeks 3-6)\nSpring Boot application setup with monolithic architecture User authentication and authorization with Spring Security PostgreSQL database setup on Amazon RDS (Multi-AZ: active-passive) JWT and OAuth 2.0 integration (Google, Facebook) Next.js frontend setup with TypeScript Frontend component library creation Basic CRUD operations for all modules AWS ECS cluster configuration with active-passive failover Phase 3: Feature Development (Weeks 7-10)\nBlog platform with advanced filtering and search Study room creation with WebRTC integration Practice test module with automatic grading logic Flashcard system with spaced repetition algorithm Real-time messaging with Spring WebSocket Dictionary and Google Translate API integration Amazon S3 integration for file uploads ElastiCache Redis for session management and caching Phase 4: AI Integration \u0026amp; Deploying (Weeks 11-12)\nGoogle Gemini Flash API integration for Speaking assessment Google Gemini Flash API integration for Writing assessment Automatic vocabulary extraction algorithms AI quiz generation from uploaded content Comprehensive testing (unit, integration, end-to-end) Performance optimization and load testing Security audit and bug fixes Production deployment to AWS ECS with Multi-AZ Monitoring setup with CloudWatch Technical Requirements\nDevelopment Environment:\nJava 17+ for Spring Boot backend Node.js 18+ for Next.js frontend PostgreSQL 14+ for database Docker for local containerization Git for version control Maven for Java dependency management Frontend Requirements:\nNext.js 14+ with App Router and TypeScript WebRTC for real-time video/voice communication Socket.io client for real-time features Form validation (React Hook Form + Zod) Backend Requirements:\nSpring Boot 3.x with Java 17+ Spring Data JPA for database operations Spring Security for authentication/authorization Spring WebSocket for real-time features Spring Web for RESTful APIs JWT for token-based authentication OAuth 2.0 for social login Multipart file upload handling AWS Infrastructure:\nAmazon ECS: Fargate launch type for containerized applications Application Load Balancer: Routes traffic to active AZ with health checks Amazon RDS PostgreSQL: Multi-AZ active-passive deployment for high availability Amazon ElastiCache (Redis): Session and cache management Amazon S3: Media file storage Amazon CloudFront: CDN for static assets Amazon CloudWatch: Logging and monitoring Amazon VPC: Network isolation with public/private subnets across 2 AZs AWS Certificate Manager: SSL/TLS certificates AI/ML Integration:\nGoogle Gemini Flash API (Free Tier) for Speaking/Writing assessment 5. Timeline \u0026amp; Milestones Project Timeline: 3 Months (12 Weeks)\nWeeks 1-2: Planning \u0026amp; Design\n✓ Requirements analysis and documentation ✓ AWS Multi-AZ architecture design ✓ PostgreSQL database schema design ✓ UI/UX mockups completion ✓ Spring Boot project structure setup Deliverable: Complete technical specification document and AWS infrastructure plan Weeks 3-6: Core Development\n✓ Spring Boot monolithic backend setup ✓ Spring Security implementation (JWT, OAuth 2.0) ✓ User management and role-based access control ✓ Amazon RDS PostgreSQL Multi-AZ deployment ✓ Next.js frontend framework and component library ✓ AWS ECS cluster setup with Application Load Balancer ✓ ElastiCache Redis configuration Deliverable: Working authentication system and AWS infrastructure Weeks 7-10: Feature Development\n✓ Blog platform with full CRUD functionality ✓ Study room creation and management ✓ WebRTC integration for video/voice calls Practice test module (Reading \u0026amp; Listening) Flashcard system with study modes Amazon S3 integration for media storage Free dictionary API and open-source translation integration Spring WebSocket for real-time features Deliverable: All five core features operational (without AI) Weeks 11-12: AI Integration \u0026amp; Deployment\n✓ Google Gemini Flash API (Free Tier) integration for Writing assessment ✓ Google Gemini Flash API (Free Tier) integration for Speaking assessment ✓ Vocabulary extraction algorithms ✓ AI quiz generation functionality ✓ Comprehensive testing (unit, integration, E2E) ✓ Performance optimization and security audit ✓ Production deployment to AWS ECS Multi-AZ ✓ CloudWatch monitoring and alerting setup ✓ Final bug fixes and documentation Deliverable: Production-ready application deployed on AWS Key Milestones:\nWeek 2: Technical specification and AWS architecture approved Week 6: Core backend and infrastructure deployed Week 10: All features complete (beta version) Week 12: Production launch with AI integration Weekly Sprint Goals:\nSprint 1-2: Architecture and setup Sprint 3-4: Authentication and database Sprint 5-6: Core API and AWS deployment Sprint 7-8: Blog and study room features Sprint 9-10: Practice tests and flashcards Sprint 11: AI integration and testing Sprint 12: Production deployment and launch 6. Budget Estimation Development Costs (One-time)\nSoftware \u0026amp; Tools:\nDevelopment tools and licenses: $0 (using free/open-source tools) Design tools (Figma Free): $0 Project management tools: $0 (using free tier) Total Software: $0 Initial Setup:\nDomain registration: $1/year SSL certificate: $0 (AWS Certificate Manager - Free) Development servers: $0 (local development) Total Initial Setup: $1 Operating Costs (Monthly)\nInfrastructure \u0026amp; Hosting (AWS):\nAmazon ECS (Fargate): 2 tasks (Next.js + Spring Boot) in active AZ-1: $30/month 2 standby tasks in passive AZ-2 (minimal usage): $10/month Application Load Balancer: $25/month Amazon RDS PostgreSQL (Multi-AZ active-passive): db.t3.medium instance (primary + standby): $85/month Storage (100 GB): $12/month Amazon ElastiCache (Redis): cache.t3.micro: $15/month Amazon S3: Storage (50 GB): $1.15/month Data transfer: $5/month Amazon CloudFront (CDN): $10/month Amazon CloudWatch: $10/month Data Transfer (outbound): $15/month VPC \u0026amp; Networking: $5/month Subtotal AWS Infrastructure: $222/month Third-party Services:\nGoogle Gemini Flash API: Free tier Subtotal Services: $0/month Other Operating Costs:\nMonitoring \u0026amp; analytics: $10/month (integrated with CloudWatch) Backup storage (RDS automated backups): $5/month Domain \u0026amp; SSL renewal: $0.08/month (amortized from $1/year, SSL via AWS Certificate Manager - Free) Subtotal Other: $15.08/month Total Monthly Operating Costs: $237.08/month\nAnnual Budget Summary\nDevelopment Phase (3 Months):\nDevelopment costs: $1 (one-time, domain only) Operating costs (3 months): $711.24 ($237.08 × 3 months) Total Development Phase: $712.24 Year 1 (After Launch):\nDevelopment costs: $1 (one-time, domain only) Operating costs: $2,844.96 ($237.08 × 12 months) Total Year 1: $2,845.96 Year 2+ (Annual Recurring):\nOperating costs: $2,844.96/year Domain renewal: $1/year Total Annual: $2,845.96 Revenue Projections (Subscription Model)\nMembership Tiers:\nGuest: Free (limited features) Member: $5/month (basic features) Premium: $15/month (all features + AI assessments) Conservative Revenue Estimate (Year 1):\nMonth 6-12: Average 100 Premium + 200 Members Revenue: (100 × $15 + 200 × $5) × 7 months = $17,500 Operating costs (Year 1): $2,845.96 Net Profit Year 1: $14,654.04 Break-even: Month 1 after launch Optimistic Revenue Estimate (Year 2):\n500 Premium + 1,000 Members Monthly revenue: $12,500 Annual revenue: $150,000 Operating costs: $2,845.96 Net Profit Year 2: $147,154.04 Profit margin: ~98% after operating costs Cost Optimization Strategies:\nZero personnel costs (self-developed project) Active-passive Multi-AZ deployment reduces costs (standby resources only used during failover) Use AWS ECS Fargate Spot for development environments (70% cost savings) Implement caching with ElastiCache to reduce database queries Use CloudFront CDN to minimize data transfer costs Leverage free tier of Google Gemini Flash API for AI features Utilize free dictionary APIs and open-source translation libraries Free development tools and design software (VS Code, Figma Free, etc.) Leverage AWS Free Tier during initial development RDS automated backups included (7-day retention) Use AWS Certificate Manager for free SSL certificates Implement S3 lifecycle policies to move old data to cheaper storage tiers No email service costs by deferring email features to later phase Standby ECS tasks in AZ-2 kept minimal until failover needed 7. Risk Assessment Risk Matrix High Priority Risks:\nAI API Cost Overruns\nImpact: Low | Probability: Low Description: Google Gemini Flash API free tier has usage limits that may be exceeded Mitigation: Implement usage quotas and rate limiting; monitor API usage through dashboard Contingency: Upgrade to paid tier if free limits are consistently exceeded, or implement queuing system Data Privacy \u0026amp; Security Breaches\nImpact: Critical | Probability: Low Description: User data exposure or unauthorized access Mitigation: Implement encryption, regular security audits, GDPR compliance Contingency: Incident response plan, insurance, legal consultation Scalability Issues\nImpact: High | Probability: Medium Description: System performance degradation with user growth Mitigation: AWS ECS Auto Scaling in active AZ, RDS Multi-AZ active-passive for high availability, ElastiCache for performance Contingency: Scale ECS tasks in active AZ, activate additional tasks in passive AZ if needed, upgrade RDS instance class Medium Priority Risks:\nThird-party Service Downtime\nImpact: Medium | Probability: Low Description: Dependency on external APIs (Gemini Flash free tier, free dictionary APIs) Mitigation: Graceful degradation, inform users when AI features are unavailable Contingency: Cached responses for dictionary lookups, manual grading option for tests during outages User Acquisition Challenges\nImpact: High | Probability: Medium Description: Difficulty attracting and retaining users Mitigation: Marketing strategy, SEO optimization, referral programs Contingency: Pivot features based on feedback, partnerships with schools Content Moderation Issues\nImpact: Medium | Probability: High Description: Inappropriate content in blogs, comments, or study rooms Mitigation: Automated content filtering, reporting system, moderator team Contingency: Community guidelines, user bans, legal disclaimer Low Priority Risks:\nTechnology Stack Obsolescence\nImpact: Low | Probability: Medium Description: Chosen technologies become outdated Mitigation: Regular dependency updates, modular architecture Contingency: Gradual migration plan, refactoring budget Competition from Established Platforms\nImpact: Medium | Probability: High Description: Competing with Duolingo, IELTS.org, etc. Mitigation: Unique features (study rooms, AI assessment), niche targeting Contingency: Differentiation strategy, feature innovation Mitigation Strategies Technical Mitigations:\nImplement comprehensive error handling and logging with CloudWatch Set up CloudWatch alarms for resource utilization and errors Regular automated backups with RDS Multi-AZ and point-in-time recovery Use CloudFront CDN for static assets to reduce ECS load Implement API rate limiting to prevent abuse Code reviews and automated testing in CI/CD pipeline (AWS CodePipeline/GitHub Actions) AWS WAF for application security and DDoS protection Business Mitigations:\nStart with freemium model to build user base Beta testing phase to identify critical issues Gradual feature rollout to manage costs Build community through social media and content marketing Establish partnerships with IELTS teachers and institutions Legal \u0026amp; Compliance Mitigations:\nTerms of Service and Privacy Policy GDPR and data protection compliance Content licensing agreements User consent for data processing Regular compliance audits Contingency Plans Technical Failures:\nDatabase failure: Automatic failover to RDS Multi-AZ standby instance in AZ-2 (passive) ECS task failure in AZ-1: Auto Scaling replaces unhealthy tasks; critical failure triggers AZ-2 activation API downtime: Serve cached content from ElastiCache and queue requests Security breach: AWS Security Hub immediate alerts, lockdown, and investigation Active-passive Multi-AZ ensures high availability with automatic failover to passive AZ-2 Business Failures:\nLow user adoption: Pivot to B2B model (schools, tutors) High churn rate: User interviews, feature improvements Revenue shortfall: Cost optimization, seek investment Legal Issues:\nCopyright claims: Content takedown procedure Privacy complaints: Data deletion and compliance review Terms violations: User suspension and investigation 8. Expected Outcomes Technical Improvements Platform Capabilities:\nFully functional web application with 5 integrated modules Real-time collaboration features (video/voice calls, messaging) AI-powered assessment for Speaking and Writing using Google Gemini Flash Scalable Multi-AZ architecture on AWS ECS supporting 10,000+ concurrent users Mobile-responsive design for learning on-the-go Robust RESTful API built with Spring Boot monolith High availability with 99.9% uptime through active-passive Multi-AZ deployment Technical Achievements:\nModern full-stack web development with Next.js and Spring Boot Real-time communication implementation (WebRTC, Spring WebSocket) Google Gemini Flash AI/ML integration and API management AWS cloud infrastructure and active-passive Multi-AZ architecture implementation PostgreSQL database design and optimization Container orchestration with Amazon ECS Fargate Security best practices with Spring Security and AWS services DevOps practices with CloudWatch monitoring and automated deployments Educational Impact For Students:\nAccessible, affordable IELTS preparation platform Personalized learning paths and progress tracking Immediate feedback on practice tests Community-driven learning environment 24/7 access to study materials and practice tests Estimated 30-40% cost reduction compared to traditional courses Learning Outcomes:\nImproved IELTS scores through consistent practice Better time management with Pomodoro integration Enhanced vocabulary through flashcard system Speaking confidence through AI feedback Writing skills improvement with detailed analysis Business Value Market Position:\nCompetitive alternative to expensive IELTS prep courses Unique combination of features (study rooms + AI + community) Scalable SaaS business model Potential for B2C and B2B markets (schools, tutoring centers) User Growth Targets:\nWeek 12 (Launch): 100 registered users (beta testers) Month 6: 500 registered users Month 12: 2,000 registered users Year 2: 10,000 registered users Premium conversion rate: 10-15% Revenue Potential:\nYear 1: $17,500 (after launch in Month 6) Year 2: $150,000+ (with 500 premium, 1,000 regular members) Year 3: $500,000+ (with market expansion and partnerships) Long-term Value Platform Evolution:\nFoundation for other language learning modules (TOEFL, SAT, etc.) Data collection for improved AI models Community-generated content library Potential for gamification and achievement systems Mobile app development based on web platform success Social Impact:\nDemocratizing IELTS preparation for students worldwide Reducing language learning barriers Building a supportive learning community Enabling peer-to-peer knowledge sharing Creating opportunities for educational content creators Portfolio \u0026amp; Career Benefits:\nComprehensive full-stack project with Spring Boot and Next.js for developer portfolios Real-world experience with AWS cloud services (ECS, RDS, S3, CloudFront, etc.) Active-passive Multi-AZ architecture and high-availability system design experience AI integration experience with Google Gemini Flash Understanding of educational technology (EdTech) Container orchestration and failover strategies Potential startup opportunity or acquisition target Success Metrics:\nUser engagement: Average 3+ sessions per week Test completion rate: 70%+ of started tests User retention: 60%+ monthly active users NPS Score: 50+ (indicating strong user satisfaction) Average IELTS score improvement: 0.5-1.0 band increase "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/5.6.2-sqs-queues/","title":"SQS Queues","tags":[],"description":"","content":"Overview Create Amazon SQS queues for asynchronous AI processing with Dead Letter Queues for failed messages.\nCreate Writing Assessment Queue Setting Value Name ielts-ai-dev-writing-evaluation Type Standard Visibility timeout 5 minutes Message retention 14 days Dead-letter queue ielts-ai-dev-writing-evaluation-dlq Max receives 3 Create Speaking Assessment Queue Setting Value Name ielts-ai-dev-speaking-evaluation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-speaking-evaluation-dlq Create Flashcard Generation Queue Setting Value Name ielts-ai-dev-flashcard-generation Visibility timeout 15 minutes Dead-letter queue ielts-ai-dev-flashcard-generation-dlq AWS CLI Commands # Create Dead Letter Queue aws sqs create-queue --queue-name ielts-writing-dlq # Create main queue with DLQ aws sqs create-queue \\ --queue-name ielts-writing-queue \\ --attributes \u0026#39;{ \u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;300\u0026#34;, \u0026#34;MessageRetentionPeriod\u0026#34;: \u0026#34;1209600\u0026#34;, \u0026#34;RedrivePolicy\u0026#34;: \u0026#34;{\\\u0026#34;deadLetterTargetArn\\\u0026#34;:\\\u0026#34;arn:aws:sqs:ap-southeast-1:{account}:ielts-writing-dlq\\\u0026#34;,\\\u0026#34;maxReceiveCount\\\u0026#34;:\\\u0026#34;3\\\u0026#34;}\u0026#34; }\u0026#39; # Repeat for speaking and flashcard queues aws sqs create-queue --queue-name ielts-speaking-dlq aws sqs create-queue --queue-name ielts-speaking-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; aws sqs create-queue --queue-name ielts-flashcard-dlq aws sqs create-queue --queue-name ielts-flashcard-queue \\ --attributes \u0026#39;{\u0026#34;VisibilityTimeout\u0026#34;: \u0026#34;900\u0026#34;}\u0026#39; Queue Summary Queue Visibility Timeout DLQ Max Receives ielts-writing-queue 5 min ielts-writing-dlq 3 ielts-speaking-queue 15 min ielts-speaking-dlq 3 ielts-flashcard-queue 15 min ielts-flashcard-dlq 3 Next Steps Proceed to Lambda Functions.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS Event Purpose Provide an overview of the AI/ML landscape in Vietnam. Introduce key AWS AI/ML services, especially Amazon SageMaker. Dive into Generative AI with Amazon Bedrock, covering foundation models and modern deployment techniques (RAG, Prompt Engineering). Highlights Morning: AWS AI/ML Services Overview Overview: context of AI/ML in Vietnam and the workshop objectives. Amazon SageMaker: AWS\u0026rsquo;s end-to-end ML platform. ML workflow: data preparation, labeling, training, tuning, and model deployment. Built-in MLOps capabilities. Live demo: walkthrough of SageMaker Studio. Afternoon: Generative AI with Amazon Bedrock Foundation Models: comparison and guidance for choosing models such as Claude, Llama, Titan, etc. Prompt Engineering: Advanced techniques: Chain-of-Thought reasoning, Few-shot learning. Retrieval-Augmented Generation (RAG): RAG architecture and integration with external knowledge bases. Bedrock Agents: building multi-step workflows and tool integrations. Guardrails: safety and content filtering principles. Live demo: building a Generative AI chatbot using Bedrock. Key Takeaways SageMaker: understood as a comprehensive platform that manages the entire ML lifecycle (data prep → training → deployment). Bedrock \u0026amp; GenAI: learned Bedrock\u0026rsquo;s role as a foundation-model management platform, how to compare FMs, and core techniques like Prompt Engineering and RAG. Project application: RAG and Bedrock Agents are useful for enhancing AI/chatbot features in the Travel-Guided project. Live demos provided practical insights into deployment flows and rapid prototyping with Bedrock. Event Experience Impressed by the live demos, especially the fast Bedrock-based chatbot build. Valuable networking and exchange opportunities with AI/ML experts in Vietnam. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives Resolve AWS account issues and create a new account if necessary. Master Hybrid DNS configuration with Route 53 Resolver. Implement and understand VPC Peering for inter-VPC communication. Discuss project plans and finalize programming language with the team. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Access Management with AWS Identity and Access Management (IAM). 21/09/2025 23/09/2025 AWS Identity and Access Management (IAM) Access Control 3 - Complete Lab 10: Route 53 and Hybrid DNS configuration.\n- Launch virtual servers to implement and test DNS setup.\n- Complete: Hybrid DNS Management with Amazon Route 53. 24/09/2025 25/09/2025 FCJ Playlist 4 - Implement VPC Peering for private communication between VPCs.\n- Create necessary resources for VPC Peering configuration.\n- Clean up resources after completion.\n- Complete: Network Integration with VPC Peering. 25/09/2025 26/09/2025 AWS VPC Peering 5 - Attend team meeting to discuss project plans and programming language selection.\n- Set deadlines for team members to study chosen technology stack. 28/09/2025 28/09/2025 Team Meeting AWS Skill Builder Courses Completed Course Category Status Hybrid DNS Management with Amazon Route 53 Networking ✅ Network Integration with VPC Peering Networking ✅ Networking on AWS Workshop Networking ✅ Infrastructure as Code with AWS CloudFormation DevOps ✅ Cloud Development with AWS Cloud9 Development ✅ Static Website Hosting with Amazon S3 Storage ✅ Week 3 Achievements Technical Skills Acquired:\nRoute 53 and Hybrid DNS:\nSuccessfully configured Hybrid DNS infrastructure with Route 53 Resolver Created and configured Outbound Endpoints for DNS query forwarding Set up Route 53 Resolver rules for conditional DNS resolution Implemented Inbound Endpoints for on-premises to AWS DNS queries Successfully connected to RD Gateway Server during practical exercises VPC Peering:\nMastered VPC Peering concepts for private inter-VPC communication without traversing public internet Enabled Cross-Zone and Cross-Region DNS Resolution in VPC Peering: EC2 instances can now resolve DNS of instances in peered VPCs to private IP addresses Understood that without this feature, DNS queries return public IPs, routing traffic through internet Learned resource cleanup procedures to avoid unnecessary costs Infrastructure as Code:\nLearned to provision AWS resources using CloudFormation templates Understood declarative infrastructure management principles Explored AWS Cloud9 as a cloud-based development environment Team Collaboration:\nParticipated in team meeting to finalize project direction Selected programming language for the project Established deadlines for team members to study the chosen technology stack Continued learning journey with FCJ team support Key Takeaways:\nHybrid DNS enables seamless DNS resolution between on-premises and AWS environments VPC Peering is cost-effective for connecting VPCs but has limitations (no transitive peering) CloudFormation templates ensure consistent, repeatable infrastructure deployments AWS Cloud9 eliminates local development environment setup complexity "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.4-setup-fe/5.4.3-sg/","title":"Configure Security Group","tags":[],"description":"","content":"Our Frontend containers run in Private Subnets. To allow them to receive traffic, we must configure a Security Group that acts as a firewall.\nFor security best practices, we will only allow traffic from the Application Load Balancer (ALB) on port 3000. Direct access from the internet or other sources will be blocked.\n1. Create Security Group Navigate to EC2 Dashboard \u0026gt; Security Groups \u0026gt; Create security group. Basic details: Security group name: ecs-private-sg. Description: security group for ecs. VPC: Select band-up-vpc. 2. Configure Inbound Rules This is the most critical step. We need to allow the ALB to talk to our Next.js application.\nInbound rules: Click Add rule. Type: Custom TCP. Port range: 3000 (The port our Next.js app listens on). Source: Select Custom and choose the Security Group ID of your ALB (e.g., alb-sg). Note: By selecting the ALB\u0026rsquo;s Security Group ID instead of an IP range, we ensure that only traffic originating from our Load Balancer is accepted. Outbound rules: Leave the default settings (Allow all traffic) to enable the container to download packages or communicate with external APIs. Click Create security group. The Security Group is now ready to be attached to our ECS Tasks in the next step.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.5-setup-be/5.5.3-redis/","title":"Create ElastiCache (Redis/Valkey)","tags":[],"description":"","content":"In this step, we provision an in-memory data store to handle session management and caching for the backend. We will use Amazon ElastiCache with the Valkey engine (a high-performance, open-source fork of Redis supported by AWS).\n1. Configure Security Group First, create a Security Group to allow the backend to communicate with the cache cluster.\nNavigate to EC2 \u0026gt; Security Groups \u0026gt; Create security group. Name: redis-sg. Inbound rules: Allow Custom TCP traffic on port 6379 from the ecs-backend-sg. (Note: Ensure this is created before proceeding to the ElastiCache console).\n2. Create Subnet Group We need to define which subnets the cache nodes will reside in.\nNavigate to Amazon ElastiCache \u0026gt; Subnet groups \u0026gt; Create subnet group. Name: bandup-cached-subnet-group. VPC: Select band-up-vpc. Subnets: Select private-database-subnet-1 and private-database-subnet-2 (Availability Zones ap-southeast-1a and 1b). 3. Create ElastiCache Cluster Now we provision the cache cluster.\nNavigate to ElastiCache \u0026gt; Caches \u0026gt; Create cache. Engine: Select Valkey - recommended (Compatible with Redis OSS). Deployment option: Select Node-based cluster (Gives more control over instance types). Creation method: Cluster cache. Cluster settings: Cluster mode: Disabled (Simple primary-replica structure is sufficient). Name: bandup-redis. Description: in memory db for bandup. Node configuration: Node type: cache.t3.micro (Cost-effective for testing). Number of replicas: 0 (Standalone node for this workshop). Connectivity: Network type: IPv4. Subnet groups: Select bandup-cached-subnet-group. Security \u0026amp; Encryption: Encryption at rest: Enabled (Default key). Encryption in transit: Enabled. Access control: No access control (We rely on Security Groups). Security groups: Select redis-sg created earlier. Backup: Enable automatic backups (Retention: 1 day). Click Create. The cluster status will change to Creating. Once Available, note down the Primary Endpoint (ending in ...cache.amazonaws.com:6379) to use in the backend configuration.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.3-network/5.3.3-iam/","title":"IAM Roles for ECS","tags":[],"description":"","content":"To allow Amazon ECS to manage your containers, it needs specific permissions. We must create an IAM Role that authorizes the ECS agent to pull container images from Amazon ECR and send logs to Amazon CloudWatch on your behalf.\nCreate ecsTaskExecutionRole Navigate to the IAM Dashboard. In the left navigation pane, choose Roles. Click Create role. Step 1: Trusted Entity\nTrusted entity type: Select AWS service. Service or use case: Choose Elastic Container Service. Select Elastic Container Service Task from the options below. Click Next. Step 2: Add Permissions\nIn the search bar, type AmazonECSTaskExecutionRolePolicy. Check the box next to the policy name AmazonECSTaskExecutionRolePolicy. Note: This managed policy grants permissions to pull images from ECR and upload logs to CloudWatch. Click Next. Step 3: Name and Review\nRole name: Enter ecsTaskExecutionRole. Review the configuration and click Create role. Once created, this role is ready to be assigned to our ECS Task Definitions in the upcoming sections.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.3-network/","title":"Network &amp; Security Infrastructure","tags":[],"description":"","content":"Overview In this section, we will establish the foundational network layer and security boundaries for IELTS BandUp.\nA robust network architecture is critical for protecting sensitive user data and ensuring high availability. Instead of using the default network settings, we will construct a custom Virtual Private Cloud (VPC) designed for a production-grade environment. This setup allows us to strictly control traffic flow between our application components (Frontend, Backend, Database) and the internet.\nFurthermore, we will configure VPC Endpoints to allow our private containers to communicate securely with AWS services (like ECR and S3) without traversing the public internet, enhancing both security and network performance.\nImplementation Steps We will break down the infrastructure setup into the following key tasks:\nVPC \u0026amp; Connectivity: Create the isolated network environment, partition it into Public and Private subnets, and configure Internet Gateways (IGW) for external connectivity. Load Balancing (ALB): Set up the Application Load Balancer and Target Groups to distribute incoming traffic efficiently to our future ECS tasks. IAM Security: Provision the ecsTaskExecutionRole to grant our Fargate containers the necessary permissions to pull images and push logs. VPC Endpoints: Establish private connections to AWS services (ECR, CloudWatch, S3) to secure internal traffic. Content VPC, Subnets \u0026amp; Routing Application Load Balancer (ALB) IAM Roles for ECS VPC Endpoints Setup "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/3-blogstranslated/3.3-blog3/","title":"PixelGuard: Enhance medical data privacy with an AI-based de-identification system for imaging research","tags":[],"description":"","content":"Published: 2025-07-09 - Authors: Shashank Tanksali, Steve Fu, Abhijit Gaonkar, and Javeed Shaik.\nMedical imaging plays a critical role in clinical research, offering insights that improve our understanding of health, disease management, and treatment outcomes. Researchers rely on imaging to study organs, tissues, and cells in healthy and pathological states. These datasets also power education programs, training resources, workshops, and workforce enablement. However, protecting patient privacy within medical imagery is essential to maintain confidentiality, adherence to ethical and legal standards such as HIPAA and HITRUST, and continued patient trust. Images frequently contain sensitive details about conditions, diagnoses, or prognoses. Safeguarding personally identifiable information (PII) and protected health information (PHI) prevents unauthorized disclosure and reinforces trust in healthcare systems.\nRegulations including the Health Insurance Portability and Accountability Act (HIPAA), the Health Information Trust Alliance Common Security Framework (HITRUST CSF) in the United States, and the European Union General Data Protection Regulation (GDPR) dictate how medical data must be processed, stored, anonymized, and shared. Violations can result in substantial penalties and reputational damage, while clinicians shoulder an ethical responsibility to protect patient privacy.\nUnderstanding DICOM Digital Imaging and Communications in Medicine (DICOM) is the industry standard for storing, transmitting, and sharing medical images along with structured metadata. A DICOM file consists of pixel data and metadata fields that describe patient demographics, study parameters, imaging devices, acquisition protocols, clinical notes, and more. Because these fields often contain PII, DICOM files cannot be used for research or training until sensitive information is removed. De-identification must preserve data utility, and optimized file sizes lower storage and processing costs. Organizations may also need to export anonymized assets in DICOM or JPEG formats. AWS HealthImaging, a scalable, high-performance cloud archive for DICOM, delivers sub-second access globally. Storage and transfer TCO can be reduced by using High Throughput JPEG 2000 (HTJ2K), an industry-standard codec.\nSolution guide PixelGuard is a secure de-identification platform built on Amazon Web Services (AWS) by Dr. Adrienne Kline, Assistant Professor at Northwestern University and founder of Xtasis, LLC. PixelGuard de-identifies medical images while preserving clinical value. The software leverages more than 75 AI models capable of detecting and removing multilingual, multi-oriented burned-in text across formats such as DICOM, JPEG, PNG, and NIfTI. It also supports customizable metadata anonymization. With an intuitive UI, enterprise-grade single sign-on (SSO), and on-premises deployment (no data leaves the customer environment), PixelGuard delivers compliant, high-throughput de-identification. The product is available on AWS Marketplace, and AWS Partner ScaleCapacity contributed to the UI, cloud infrastructure, and marketplace launch.\nIngestion layer The ingestion layer provides a UI and API to submit studies for de-identification. Medical device manufacturers can also integrate through the API to automate redaction. Before upload, the web experience and API endpoints can be protected using enterprise identity providers to enforce authorization. Alongside ingesting images, the layer records de-identification profiles that specify which fields to remove. Common configurations, such as HIPAA-compliant field sets, are predefined and stored for reuse.\nPre de-identification layer This staging layer stores images (or archives containing multiple images) prior to processing. It tracks metadata such as job ID, submission timestamp, field-removal profiles, and file formats. This data helps orchestrate downstream processing steps and auditing.\nProcessing layer The processing layer detects the source image format and applies de-identification according to configuration. Operations span metadata scrubbing (for DICOM tags) and pixel-level redaction. Output images can be compressed to optimize storage, and a crosswalk file is generated to map each anonymized object to a unique identifier. The crosswalk must be protected and stored separately from the anonymized images to prevent re-identification.\nDe-identification pipeline output Post-processing storage keeps the anonymized imagery ready for research. Outputs can be prepared in multiple formats to match study requirements. When images are delivered, they are optionally accompanied by the crosswalk file, which maintains the association between anonymized IDs and original IDs. The crosswalk remains encrypted and is only accessible to authorized personnel for legitimate re-identification needs.\nAuditing, monitoring, and observability layer This layer captures access logs to record who accessed data, what they accessed, and when. Detailed error logs support troubleshooting when images cannot be de-identified automatically. These telemetry streams enable governance and simplify compliance reporting.\nFigure 1: PixelGuard logical architecture\nDe-identifying medical images requires cleaning both metadata and pixel content. Some metadata fields must be anonymized rather than deleted to maintain research value. For instance, changing a 20-year-old patient\u0026rsquo;s age to 80 would skew research conclusions. OCR techniques can detect burned-in text, and machine learning (ML) models identify regions to blur or redact. ML models can also produce confidence scores; images that fall below thresholds can be routed to a manual review queue.\nTools and frameworks Ingestion: Implement the UI and API through Amazon API Gateway with request validation, authentication, authorization, and security policy enforcement. Integrate with Amazon Cognito to deliver fine-grained access control. Pre de-identification storage: Store predefined de-identification profiles (for example HIPAA) in Amazon Simple Storage Service (Amazon S3) or a combination of S3 and Amazon DynamoDB. De-identification workflow: Create multi-step pipelines with DICOM parsing libraries such as pydicom and scrubbing utilities such as dicom-anonymizer. Use Amazon Textract and Amazon Comprehend Medical to detect and remove embedded text within images. Output storage: After processing, compress and convert medical images as needed. Store anonymized assets and identifiers in S3 buckets or file systems, with or without compression, ensuring crosswalk data is isolated. Supporting services: Amazon API Gateway enforces organization-specific AuthN/AuthZ policies. AWS Key Management Service (AWS KMS) safeguards encryption keys. AWS Fargate runs containers at scale without managing servers. Amazon Macie adds another layer of protection by discovering sensitive data. AWS CloudFormation enables infrastructure as code (IaC) so you can declaratively define, create, and manage AWS resources. The following screenshots show the PixelGuard experience:\nFigure 2: Configure redaction fields for medical images\nFigure 3: Redaction job completion report\nFigure 4: Side-by-side preview of original and redacted images\nConclusion De-identifying medical imagery is essential for research and training. A robust technical architecture ensures anonymized images cannot be traced back to individuals while preserving scientific utility. PixelGuard, powered by AWS, enables flexible de-identification that upholds privacy laws, enhances security, and lowers risk. At the same time, it accelerates data sharing and collaboration, fueling faster AI-driven discoveries that advance public health while honoring ethical and legal obligations.\nNext steps Access PixelGuard on AWS Marketplace. Explore how the AWS Cloud helps solve medical mysteries: Medical data-sharing innovation through the Undiagnosed Diseases Network. Learn how to build an enterprise medical imaging inference pipeline with MONAI Deploy on AWS. Read about optimizing medical imaging AI deployments with NVIDIA NIMs and AWS services. Contact your AWS representative to design a similar system for healthcare innovation. About the authors Shashank Tanksali is a Solutions Architect at AWS who helps customers realize the full value of AWS technologies through architectural guidance and hands-on support. He collaborates closely with customers to design scalable, secure, cost-optimized cloud solutions that align with business context.\nSteve Fu is a Principal Solutions Architect at AWS. He holds a PhD in Pharmaceutical Science from the University of Mississippi and has more than a decade of experience in technology and biomedical research. Steve is passionate about the positive impact technology can deliver to healthcare and life sciences.\nAbhijit Gaonkar is a Senior DevOps Engineer and AWS Solutions Architect at ScaleCapacity. He specializes in building secure, scalable cloud platforms for public- and private-sector customers, focusing on healthcare, data privacy, and AI workloads. His expertise in infrastructure as code, automation, and AWS-native services has been crucial for compliant, high-performance deployments in regulated environments.\nJaveed Shaik is Head of Cloud Platform Engineering at ScaleCapacity. Over 25 years, he has led data center modernization, cloud migration, and application modernization programs for Fortune 100 and Fortune 500 enterprises as well as major public-sector organizations. His ability to devise innovative solutions for complex business challenges has been instrumental to customer success.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Analyze media content using AWS AI services The article outlines an event-driven AWS architecture that automatically converts raw audio/video into searchable, structured insights. Media uploaded to S3 triggers Step Functions to run Amazon Transcribe for speech-to-text and Amazon Bedrock for AI analysis, identifying ads, interviews, and key segments. Results are stored in a data lake, queried with Athena, visualized in QuickSight, and accessed through natural-language search via Amazon Q. Strong security practices, testing, optimization, and scalability considerations are emphasized. The solution applies widely across broadcasting, enterprise knowledge management, education, legal, healthcare, and customer service.\nBlog 2 - Improve conversational AI response times for enterprise applications with the Amazon Bedrock streaming API and AWS AppSync The article shows how to cut conversational AI latency by streaming Bedrock LLM responses through AWS AppSync. A Lambda–SNS–Lambda pipeline invokes converse_stream, then pushes partial tokens to the frontend via AppSync subscriptions, letting users see answers as they’re generated. Buffering reduces network calls, improving speed while keeping enterprise-grade security (VPC, OAuth, controlled data flow). A global financial firm reduced initial response time from ~10 seconds to ~2–3 seconds. Terraform and sample code enable quick deployment.\nBlog 3 - Advancing healthcare data privacy through AI-driven de-identification system for medical imaging research PixelGuard is an AWS-based medical image de-identification system that uses more than 75 AI models to remove or obscure identifying information in both metadata and pixel data while preserving clinical value. It supports multiple formats (DICOM, JPEG, PNG, NIfTI), detects burned-in text using OCR and ML, and creates encrypted crosswalk files for optional re-identification when authorized. Security is enforced through SSO, API Gateway, Cognito, and AWS KMS. PixelGuard enables organizations to comply with HIPAA/GDPR, share research data safely, and accelerate AI development in medical imaging.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/5.6.3-lambda-functions/","title":"Lambda Functions","tags":[],"description":"","content":"Overview The AI Service layer consists of four Lambda functions that power the IELTS learning platform. These functions process requests asynchronously via SQS queues and integrate with Google Gemini API and Amazon Bedrock for AI-powered evaluations.\nLambda Function 1: Writing Evaluator Evaluates IELTS Writing Task 1 and Task 2 essays using Gemini API with detailed band scoring.\nSetting Value Function name bandup-writing-evaluator Runtime Python 3.11 Memory 1024 MB Timeout 5 minutes Trigger SQS (bandup-writing-queue) AI Model Google Gemini 2.0 Flash Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any logger = logging.getLogger() logger.setLevel(logging.INFO) # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Writing essays using Gemini API.\u0026#34;\u0026#34;\u0026#34; # Parse SQS message or API Gateway request if is_sqs_event(event): request_data, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;, \u0026#39;writing\u0026#39;) else: request_data = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) # Get API key securely from Secrets Manager gemini_api_key = get_gemini_api_key() # Retrieved from AWS Secrets Manager gemini_client = GeminiClient(api_key=gemini_api_key) # Extract request parameters user_id = request_data.get(\u0026#39;user_id\u0026#39;) essay_content = request_data.get(\u0026#39;essay_content\u0026#39;) task_type = request_data.get(\u0026#39;task_type\u0026#39;, \u0026#39;TASK_2\u0026#39;) # Build evaluation prompt prompt = build_writing_prompt(essay_content, task_type) # Call Gemini API for evaluation response = gemini_client.generate_evaluation( prompt=prompt, feature=\u0026#39;writing_task2\u0026#39;, max_retries=3, timeout=60 ) # Parse and validate band scores evaluation = parse_gemini_response(response[\u0026#39;content\u0026#39;]) # Build result with IELTS criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;task_achievement_band\u0026#39;: evaluation[\u0026#39;task_achievement\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;coherence_band\u0026#39;: evaluation[\u0026#39;coherence_cohesion\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;feedback\u0026#39;: evaluation } # Save to DynamoDB dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_EVALUATIONS\u0026#39;)) table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: result[\u0026#39;session_id\u0026#39;], \u0026#39;user_id\u0026#39;: user_id, \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, **result }) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Gemini Prompt Template:\ndef build_writing_prompt(essay_content: str, task_type: str) -\u0026gt; str: return f\u0026#34;\u0026#34;\u0026#34;You are an experienced IELTS examiner. Evaluate this essay: Task Type: {task_type} ESSAY: {essay_content} Evaluate using IELTS band descriptors (1-9, 0.5 increments): 1. Task Achievement - Addresses all parts of task 2. Coherence and Cohesion - Logical organization 3. Lexical Resource - Vocabulary range and accuracy 4. Grammatical Range and Accuracy - Sentence structures RESPOND IN JSON FORMAT: {{ \u0026#34;overall_band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;task_achievement\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;coherence_cohesion\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;lexical_resource\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;grammatical_range_accuracy\u0026#34;: {{\u0026#34;band\u0026#34;: \u0026lt;float\u0026gt;, \u0026#34;feedback\u0026#34;: \u0026#34;...\u0026#34;}}, \u0026#34;quoted_examples\u0026#34;: [{{\u0026#34;quote\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;...\u0026#34;}}] }}\u0026#34;\u0026#34;\u0026#34; Lambda Function 2: Speaking Evaluator Evaluates IELTS Speaking using Gemini native audio processing - 72% cheaper and 2x faster than AWS Transcribe alternatives.\nSetting Value Function name bandup-speaking-evaluator Runtime Python 3.11 Memory 2048 MB Timeout 5 minutes Trigger SQS (bandup-speaking-queue) AI Model Gemini 2.5 Flash (Native Audio) Core Implementation:\nimport json import os import boto3 import logging from typing import Dict, Any, Tuple logger = logging.getLogger() # Import from Lambda layer from lambda_shared.gemini_client import GeminiClient from secrets_helper import get_gemini_api_key def download_audio_from_s3(audio_url: str) -\u0026gt; Tuple[bytes, str]: \u0026#34;\u0026#34;\u0026#34;Download audio file from S3 and determine MIME type.\u0026#34;\u0026#34;\u0026#34; s3_client = boto3.client(\u0026#39;s3\u0026#39;) # Parse S3 URL: s3://bucket-name/path/to/file.mp3 parts = audio_url.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;, 1) bucket, key = parts[0], parts[1] response = s3_client.get_object(Bucket=bucket, Key=key) audio_bytes = response[\u0026#39;Body\u0026#39;].read() # Determine MIME type from extension mime_types = {\u0026#39;.mp3\u0026#39;: \u0026#39;audio/mp3\u0026#39;, \u0026#39;.wav\u0026#39;: \u0026#39;audio/wav\u0026#39;, \u0026#39;.m4a\u0026#39;: \u0026#39;audio/m4a\u0026#39;} ext = \u0026#39;.\u0026#39; + key.split(\u0026#39;.\u0026#39;)[-1].lower() mime_type = mime_types.get(ext, \u0026#39;audio/mp3\u0026#39;) return audio_bytes, mime_type def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Evaluate IELTS Speaking using Gemini native audio.\u0026#34;\u0026#34;\u0026#34; # Parse request request_data = parse_request(event) # Get API key from Secrets Manager gemini_api_key = get_gemini_api_key() gemini_client = GeminiClient(api_key=gemini_api_key) # Extract parameters audio_url = request_data.get(\u0026#39;audio_url\u0026#39;) part = request_data.get(\u0026#39;part\u0026#39;, \u0026#39;PART_1\u0026#39;) questions = request_data.get(\u0026#39;questions\u0026#39;, []) # Step 1: Download audio from S3 audio_bytes, mime_type = download_audio_from_s3(audio_url) logger.info(f\u0026#34;Downloaded {len(audio_bytes)} bytes, MIME: {mime_type}\u0026#34;) # Step 2: Send audio directly to Gemini (ONE API call) # No AWS Transcribe needed - Gemini processes audio natively evaluation = gemini_client.evaluate_audio( audio_bytes=audio_bytes, part=part, questions=questions, mime_type=mime_type, max_retries=3, timeout=120 ) # Step 3: Build response with IELTS Speaking criteria result = { \u0026#39;session_id\u0026#39;: request_data.get(\u0026#39;session_id\u0026#39;), \u0026#39;transcript\u0026#39;: evaluation.get(\u0026#39;transcript\u0026#39;), \u0026#39;duration\u0026#39;: evaluation.get(\u0026#39;duration_seconds\u0026#39;), \u0026#39;overall_band\u0026#39;: evaluation.get(\u0026#39;overall_band\u0026#39;), \u0026#39;fluency_band\u0026#39;: evaluation[\u0026#39;fluency_coherence\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;lexical_band\u0026#39;: evaluation[\u0026#39;lexical_resource\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;grammar_band\u0026#39;: evaluation[\u0026#39;grammatical_range_accuracy\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;pronunciation_band\u0026#39;: evaluation[\u0026#39;pronunciation\u0026#39;][\u0026#39;band\u0026#39;], \u0026#39;model_used\u0026#39;: \u0026#39;gemini-2.5-flash-audio\u0026#39;, \u0026#39;estimated_cost\u0026#39;: evaluation[\u0026#39;usage\u0026#39;][\u0026#39;cost\u0026#39;] } # Save to DynamoDB save_evaluation(result, request_data.get(\u0026#39;user_id\u0026#39;)) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: json.dumps(result)} Cost Comparison:\nApproach Cost per 3-min Audio Latency Gemini Native Audio ~$0.021 30-45s AWS Transcribe + LLM ~$0.076 60-90s Savings 72% 2x faster Lambda Function 3: Flashcard Generator (RAG) Generates flashcards from PDF documents using lightweight RAG pipeline with Titan Embeddings (in-memory vector store, optimized for \u0026lt;50MB Lambda package).\nSetting Value Function name bandup-flashcard-generator Runtime Python 3.11 Memory 1024 MB Timeout 10 minutes Trigger SQS (bandup-flashcard-queue) AI Model Gemini + Amazon Titan Embeddings V2 RAG Pipeline Flow:\n┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ PDF Upload │ ──▶ │ Chunking │ ──▶ │ Titan Embeddings│ │ (S3) │ │ (3000 chars) │ │ (Bedrock) │ └─────────────┘ └──────────────┘ └────────┬────────┘ │ ▼ ┌─────────────┐ ┌──────────────┐ ┌─────────────────┐ │ Flashcards │ ◀── │ Gemini │ ◀── │ In-Memory Store │ │ (JSON) │ │ Generation │ │ (Cosine Sim) │ └─────────────┘ └──────────────┘ └─────────────────┘ Core Implementation:\nimport json import os import boto3 import time import google.generativeai as genai from typing import Dict, Any, List from concurrent.futures import ThreadPoolExecutor, as_completed logger = logging.getLogger() # Global instance for warm starts (Lambda optimization) _rag_instance = None _s3_client = None def get_s3_client(): \u0026#34;\u0026#34;\u0026#34;Get cached S3 client.\u0026#34;\u0026#34;\u0026#34; global _s3_client if _s3_client is None: _s3_client = boto3.client(\u0026#39;s3\u0026#39;) return _s3_client def get_rag_instance(api_key: str): \u0026#34;\u0026#34;\u0026#34;Get cached RAG instance for warm starts.\u0026#34;\u0026#34;\u0026#34; global _rag_instance if _rag_instance is None: _rag_instance = RAG( api_key=api_key, chunk_size=int(os.environ.get(\u0026#39;RAG_CHUNK_SIZE\u0026#39;, \u0026#39;500\u0026#39;)), chunk_overlap=int(os.environ.get(\u0026#39;RAG_CHUNK_OVERLAP\u0026#39;, \u0026#39;100\u0026#39;)) ) logger.info(\u0026#34;Cold start: RAG instance created\u0026#34;) else: logger.info(\u0026#34;Warm start: Reusing RAG instance\u0026#34;) return _rag_instance def download_pdf_from_s3(bucket: str, key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Download PDF from S3 to /tmp.\u0026#34;\u0026#34;\u0026#34; s3 = get_s3_client() local_path = f\u0026#34;/tmp/{key.split(\u0026#39;/\u0026#39;)[-1]}\u0026#34; s3.download_file(bucket, key, local_path) return local_path def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;RAG-based flashcard generation .\u0026#34;\u0026#34;\u0026#34; start_time = time.time() is_async = is_sqs_event(event) # Parse request if is_async: request, job_id = parse_sqs_message(event) update_job_status(job_id, \u0026#39;processing\u0026#39;) else: request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) if isinstance(event.get(\u0026#39;body\u0026#39;), str) else event # Get S3 location pdf_url = request.get(\u0026#39;pdf_url\u0026#39;) s3_bucket, s3_key = parse_s3_url(pdf_url) # Get API key from Secrets Manager secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) secrets_client = boto3.client(\u0026#39;secretsmanager\u0026#39;) api_key = secrets_client.get_secret_value(SecretId=secret_arn)[\u0026#39;SecretString\u0026#39;] # Get parameters num_cards = int(request.get(\u0026#39;num_cards\u0026#39;, 10)) difficulty = request.get(\u0026#39;difficulty\u0026#39;, \u0026#39;MEDIUM\u0026#39;) question_types = request.get(\u0026#39;question_types\u0026#39;, [\u0026#39;DEFINITION\u0026#39;, \u0026#39;VOCABULARY\u0026#39;, \u0026#39;COMPREHENSION\u0026#39;]) # Step 1: Download PDF from S3 local_pdf = download_pdf_from_s3(s3_bucket, s3_key) # Step 2: Index document with RAG (Titan Embeddings + in-memory store) rag = get_rag_instance(api_key) rag._vector_store = None # Reset for new document rag._chunks = [] index_result = rag.index_document(local_pdf, document_id=s3_key) logger.info(f\u0026#34;Indexed {index_result[\u0026#39;chunk_count\u0026#39;]} chunks from {index_result[\u0026#39;page_count\u0026#39;]} pages\u0026#34;) # Step 3: Retrieve relevant chunks (hybrid approach) if index_result[\u0026#39;chunk_count\u0026#39;] \u0026lt;= 15: # Small document: use representative chunks chunks = rag.get_representative_chunks(num_chunks=min(10, index_result[\u0026#39;chunk_count\u0026#39;])) retrieval_method = \u0026#34;representative\u0026#34; else: # Large document: use smart keyword-based queries chunks = rag.retrieve_with_smart_queries(top_k_per_query=3) retrieval_method = \u0026#34;smart_queries\u0026#34; # Step 4: Generate flashcards with Gemini prompt = generate_flashcards_prompt(chunks, num_cards, difficulty, question_types) flashcard_result = call_gemini(prompt, api_key) # Clean up os.remove(local_pdf) # Build response total_time = time.time() - start_time response_body = { \u0026#39;status\u0026#39;: \u0026#39;success\u0026#39;, \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document\u0026#39;: { \u0026#39;s3_bucket\u0026#39;: s3_bucket, \u0026#39;s3_key\u0026#39;: s3_key, \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;] }, \u0026#39;retrieval\u0026#39;: { \u0026#39;method\u0026#39;: retrieval_method, \u0026#39;chunks_used\u0026#39;: len(chunks), \u0026#39;keywords\u0026#39;: index_result.get(\u0026#39;keywords\u0026#39;, [])[:5] }, \u0026#39;flashcards\u0026#39;: flashcard_result.get(\u0026#39;flashcards\u0026#39;, []), \u0026#39;total_cards\u0026#39;: len(flashcard_result.get(\u0026#39;flashcards\u0026#39;, [])), \u0026#39;metrics\u0026#39;: { \u0026#39;total_time_ms\u0026#39;: round(total_time * 1000) } } # Save to DynamoDB (bandup-flashcard-sets table) dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(os.environ.get(\u0026#39;DYNAMODB_FLASHCARD_SETS\u0026#39;)) table.put_item(Item={ \u0026#39;set_id\u0026#39;: request.get(\u0026#39;set_id\u0026#39;), \u0026#39;user_id\u0026#39;: request.get(\u0026#39;user_id\u0026#39;), \u0026#39;document_id\u0026#39;: s3_key, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(response_body[\u0026#39;flashcards\u0026#39;]), \u0026#39;total_cards\u0026#39;: response_body[\u0026#39;total_cards\u0026#39;], \u0026#39;page_count\u0026#39;: index_result[\u0026#39;page_count\u0026#39;], \u0026#39;chunk_count\u0026#39;: index_result[\u0026#39;chunk_count\u0026#39;], \u0026#39;created_at\u0026#39;: int(time.time()) }) if is_async: return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;OK\u0026#39;} return create_response(200, response_body) Titan Embeddings with Parallel Processing:\nclass TitanEmbeddings: \u0026#34;\u0026#34;\u0026#34;Amazon Titan Text Embeddings V2 via Bedrock with parallel processing.\u0026#34;\u0026#34;\u0026#34; MODEL_ID = \u0026#34;amazon.titan-embed-text-v2:0\u0026#34; def __init__(self, region: str = None): self.region = region or os.environ.get(\u0026#39;BEDROCK_REGION\u0026#39;, \u0026#39;us-east-1\u0026#39;) self._client = None @property def client(self): if self._client is None: self._client = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=self.region) return self._client def embed(self, text: str) -\u0026gt; List[float]: \u0026#34;\u0026#34;\u0026#34;Get embedding for single text using Titan V2.\u0026#34;\u0026#34;\u0026#34; response = self.client.invoke_model( modelId=self.MODEL_ID, body=json.dumps({ \u0026#34;inputText\u0026#34;: text[:8000], # Max input length \u0026#34;dimensions\u0026#34;: 512, \u0026#34;normalize\u0026#34;: True }), contentType=\u0026#34;application/json\u0026#34;, accept=\u0026#34;application/json\u0026#34; ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;embedding\u0026#39;] def embed_batch_parallel(self, texts: List[str], max_workers: int = 10) -\u0026gt; List[List[float]]: \u0026#34;\u0026#34;\u0026#34;Embed multiple texts in PARALLEL using ThreadPoolExecutor.\u0026#34;\u0026#34;\u0026#34; embeddings = [None] * len(texts) with ThreadPoolExecutor(max_workers=max_workers) as executor: futures = {executor.submit(self.embed, t): i for i, t in enumerate(texts)} for future in as_completed(futures): idx = futures[future] embeddings[idx] = future.result() return embeddings RAG Pipeline (In-Memory):\nimport math import fitz # PyMuPDF class RAG: \u0026#34;\u0026#34;\u0026#34;Lightweight RAG using Titan Embeddings + in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; def __init__(self, api_key: str, chunk_size: int = 3000, chunk_overlap: int = 300): self.api_key = api_key self.chunk_size = chunk_size self.chunk_overlap = chunk_overlap self._chunks = [] self._embeddings = [] self._titan = TitanEmbeddings() self._keywords = [] def index_document(self, pdf_path: str, document_id: str = None) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Index PDF with Titan V2 embeddings (parallel processing).\u0026#34;\u0026#34;\u0026#34; # Load PDF pages pages = [] with fitz.open(pdf_path) as doc: for page_num, page in enumerate(doc): text = page.get_text() if text.strip(): pages.append({\u0026#39;content\u0026#39;: text, \u0026#39;page\u0026#39;: page_num + 1}) # Chunk text with overlap self._chunks = [] for page in pages: chunks = self._chunk_text(page[\u0026#39;content\u0026#39;]) for chunk in chunks: self._chunks.append({ \u0026#39;text\u0026#39;: chunk, \u0026#39;page\u0026#39;: page[\u0026#39;page\u0026#39;] }) # Extract keywords for smart query generation all_text = \u0026#34; \u0026#34;.join([c[\u0026#39;text\u0026#39;] for c in self._chunks]) self._keywords = self._extract_keywords(all_text, top_n=20) # Generate embeddings in parallel (10 concurrent Bedrock calls) texts = [c[\u0026#39;text\u0026#39;] for c in self._chunks] self._embeddings = self._titan.embed_batch_parallel(texts, max_workers=10) return { \u0026#39;page_count\u0026#39;: len(pages), \u0026#39;chunk_count\u0026#39;: len(self._chunks), \u0026#39;keywords\u0026#39;: self._keywords[:10] } def _cosine_similarity(self, a: List[float], b: List[float]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Calculate cosine similarity between two vectors.\u0026#34;\u0026#34;\u0026#34; dot_product = sum(x * y for x, y in zip(a, b)) norm_a = math.sqrt(sum(x * x for x in a)) norm_b = math.sqrt(sum(x * x for x in b)) if norm_a == 0 or norm_b == 0: return 0.0 return dot_product / (norm_a * norm_b) def similarity_search(self, query: str, top_k: int = 5) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for similar chunks using in-memory cosine similarity.\u0026#34;\u0026#34;\u0026#34; query_embedding = self._titan.embed(query) # Calculate similarities similarities = [] for i, embedding in enumerate(self._embeddings): score = self._cosine_similarity(query_embedding, embedding) similarities.append((i, score)) # Sort by similarity (descending) and return top-k similarities.sort(key=lambda x: x[1], reverse=True) results = [] for rank, (idx, score) in enumerate(similarities[:top_k]): chunk = self._chunks[idx] results.append({ \u0026#39;text\u0026#39;: chunk[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: chunk[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: score, \u0026#39;rank\u0026#39;: rank + 1 }) return results def generate_smart_queries(self, num_queries: int = 5) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Generate document-specific queries using extracted keywords.\u0026#34;\u0026#34;\u0026#34; kw = self._keywords queries = [] if len(kw) \u0026gt;= 2: queries.append(f\u0026#34;definition and explanation of {kw[0]} and {kw[1]}\u0026#34;) if len(kw) \u0026gt;= 4: queries.append(f\u0026#34;key concepts about {kw[2]} {kw[3]}\u0026#34;) if len(kw) \u0026gt;= 6: queries.append(f\u0026#34;important information regarding {kw[4]} {kw[5]}\u0026#34;) return queries[:num_queries] def retrieve_with_smart_queries(self, top_k_per_query: int = 3) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Retrieve chunks using multiple smart queries for better coverage.\u0026#34;\u0026#34;\u0026#34; queries = self.generate_smart_queries() seen_texts = set() all_results = [] for query in queries: results = self.similarity_search(query, top_k=top_k_per_query) for r in results: if r[\u0026#39;text\u0026#39;] not in seen_texts: seen_texts.add(r[\u0026#39;text\u0026#39;]) all_results.append(r) return sorted(all_results, key=lambda x: x[\u0026#39;score\u0026#39;], reverse=True) def get_representative_chunks(self, num_chunks: int = 10) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Get evenly distributed chunks across document.\u0026#34;\u0026#34;\u0026#34; if len(self._chunks) \u0026lt;= num_chunks: return [{\u0026#39;text\u0026#39;: c[\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: c[\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for c in self._chunks] step = len(self._chunks) // num_chunks return [{\u0026#39;text\u0026#39;: self._chunks[i * step][\u0026#39;text\u0026#39;], \u0026#39;page\u0026#39;: self._chunks[i * step][\u0026#39;page\u0026#39;], \u0026#39;score\u0026#39;: 1.0} for i in range(num_chunks)] Flashcard Generation Prompt:\ndef generate_flashcards_prompt(chunks: List[Dict], num_cards: int, difficulty: str, question_types: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Build prompt for Gemini flashcard generation.\u0026#34;\u0026#34;\u0026#34; context = \u0026#34;\\n\\n\u0026#34;.join([ f\u0026#34;[Chunk {i+1}] (Page {c.get(\u0026#39;page\u0026#39;, \u0026#39;?\u0026#39;)}):\\n{c[\u0026#39;text\u0026#39;]}\u0026#34; for i, c in enumerate(chunks) ]) return f\u0026#34;\u0026#34;\u0026#34;Based on the following document excerpts, generate {num_cards} flashcards. CONTEXT: {context} REQUIREMENTS: - Difficulty: {difficulty} - Generate exactly {num_cards} flashcards - Each flashcard should have a clear question and concise answer - Focus on key concepts, definitions, and important facts - Use these question types: {\u0026#34;, \u0026#34;.join(question_types)} OUTPUT FORMAT (JSON): {{ \u0026#34;flashcards\u0026#34;: [ {{ \u0026#34;question\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;DEFINITION\u0026#34;, \u0026#34;difficulty\u0026#34;: \u0026#34;{difficulty}\u0026#34;, \u0026#34;source_chunk\u0026#34;: 1 }} ] }} Return ONLY valid JSON.\u0026#34;\u0026#34;\u0026#34; def call_gemini(prompt: str, api_key: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Call Gemini API for flashcard generation.\u0026#34;\u0026#34;\u0026#34; import google.generativeai as genai genai.configure(api_key=api_key) model = genai.GenerativeModel( model_name=os.environ.get(\u0026#39;GEMINI_MODEL\u0026#39;, \u0026#39;gemini-2.0-flash\u0026#39;), generation_config={ \u0026#39;temperature\u0026#39;: 0.3, \u0026#39;max_output_tokens\u0026#39;: 4096 } ) response = model.generate_content(prompt) text = response.text # Extract JSON if wrapped in markdown if \u0026#39;```json\u0026#39; in text: text = text.split(\u0026#39;```json\u0026#39;)[1].split(\u0026#39;```\u0026#39;)[0] return json.loads(text.strip()) Lambda Function 4: S3 Upload Handler Generates presigned URLs for secure file uploads to S3.\nSetting Value Function name bandup-s3-upload Runtime Python 3.11 Memory 256 MB Timeout 30 seconds Trigger API Gateway (sync) Core Implementation:\nimport json import os import boto3 from datetime import datetime from typing import Dict, Any s3_client = boto3.client(\u0026#39;s3\u0026#39;) def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;Generate presigned URL for S3 upload.\u0026#34;\u0026#34;\u0026#34; request = json.loads(event.get(\u0026#39;body\u0026#39;, \u0026#39;{}\u0026#39;)) user_id = request.get(\u0026#39;user_id\u0026#39;) filename = request.get(\u0026#39;filename\u0026#39;) content_type = request.get(\u0026#39;content_type\u0026#39;, \u0026#39;application/octet-stream\u0026#39;) upload_type = request.get(\u0026#39;upload_type\u0026#39;, \u0026#39;general\u0026#39;) # Determine bucket based on upload type bucket_map = { \u0026#39;speaking_audio\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_AUDIO\u0026#39;), \u0026#39;flashcard_pdf\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), \u0026#39;writing_essay\u0026#39;: os.environ.get(\u0026#39;S3_BUCKET_DOCUMENTS\u0026#39;), } bucket = bucket_map.get(upload_type) # Generate organized S3 key timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) key = f\u0026#34;uploads/{upload_type}/{user_id}/{timestamp}_{filename}\u0026#34; # Generate presigned PUT URL (15 min expiry) upload_url = s3_client.generate_presigned_url( \u0026#39;put_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key, \u0026#39;ContentType\u0026#39;: content_type}, ExpiresIn=900 ) # Generate presigned GET URL (1 hour expiry) get_url = s3_client.generate_presigned_url( \u0026#39;get_object\u0026#39;, Params={\u0026#39;Bucket\u0026#39;: bucket, \u0026#39;Key\u0026#39;: key}, ExpiresIn=3600 ) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: {\u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;}, \u0026#39;body\u0026#39;: json.dumps({ \u0026#39;upload_url\u0026#39;: upload_url, \u0026#39;get_url\u0026#39;: get_url, \u0026#39;file_url\u0026#39;: f\u0026#34;s3://{bucket}/{key}\u0026#34;, \u0026#39;expires_in\u0026#39;: 900 }) } Secure Secrets Management All Lambda functions use AWS Secrets Manager to retrieve API keys:\n# secrets_helper.py (in Lambda Layer) import boto3 import os from functools import lru_cache @lru_cache(maxsize=1) def get_gemini_api_key() -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Retrieve Gemini API key from Secrets Manager (cached).\u0026#34;\u0026#34;\u0026#34; client = boto3.client(\u0026#39;secretsmanager\u0026#39;) secret_arn = os.environ.get(\u0026#39;GEMINI_API_KEY_SECRET_ARN\u0026#39;) response = client.get_secret_value(SecretId=secret_arn) return response[\u0026#39;SecretString\u0026#39;] Security Best Practices:\nNever hardcode API keys in Lambda code Use AWS Secrets Manager for all sensitive credentials Rotate secrets regularly using automatic rotation Use IAM roles with least-privilege permissions IAM Role for Lambda Functions { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;BedrockAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;bedrock:InvokeModel\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:bedrock:*:*:foundation-model/amazon.titan-embed-text-v2*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34;], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:*:*:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:*:*:table/bandup-flashcard-sets\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3Access\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::bandup-*/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SQSAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sqs:ReceiveMessage\u0026#34;, \u0026#34;sqs:DeleteMessage\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:sqs:*:*:bandup-*-queue\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;SecretsAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;secretsmanager:GetSecretValue\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:secretsmanager:*:*:secret:bandup/*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudWatchLogs\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:*:*:log-group:/aws/lambda/bandup-*\u0026#34; } ] } DynamoDB Tables Lambda functions store results in two DynamoDB tables:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Evaluations Table Schema (Writing \u0026amp; Speaking):\n# Used by Writing Evaluator table.put_item(Item={ \u0026#39;evaluation_id\u0026#39;: session_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;evaluation_type\u0026#39;: \u0026#39;writing\u0026#39;, # \u0026#39;writing\u0026#39; or \u0026#39;speaking\u0026#39; \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;overall_band\u0026#39;: \u0026#39;7.0\u0026#39;, \u0026#39;task_achievement_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Writing only \u0026#39;fluency_band\u0026#39;: \u0026#39;6.5\u0026#39;, # Speaking only \u0026#39;pronunciation_band\u0026#39;: \u0026#39;7.0\u0026#39;, # Speaking only \u0026#39;transcript\u0026#39;: \u0026#39;...\u0026#39;, # Speaking only \u0026#39;feedback\u0026#39;: json.dumps(feedback), \u0026#39;created_at\u0026#39;: timestamp }) Flashcard Sets Table Schema:\n# Used by Flashcard Generator table.put_item(Item={ \u0026#39;set_id\u0026#39;: set_id, # Partition Key \u0026#39;user_id\u0026#39;: user_id, # Sort Key \u0026#39;document_id\u0026#39;: document_id, \u0026#39;status\u0026#39;: \u0026#39;completed\u0026#39;, \u0026#39;flashcards\u0026#39;: json.dumps(flashcards), \u0026#39;total_cards\u0026#39;: 10, \u0026#39;page_count\u0026#39;: 5, \u0026#39;chunk_count\u0026#39;: 12, \u0026#39;created_at\u0026#39;: timestamp }) Environment Variables Variable Description Example GEMINI_API_KEY_SECRET_ARN Secrets Manager ARN arn:aws:secretsmanager:...:secret:bandup/gemini-api-key DYNAMODB_EVALUATIONS Evaluations table (Writing + Speaking) bandup-evaluations DYNAMODB_FLASHCARD_SETS Flashcard sets table bandup-flashcard-sets S3_BUCKET_AUDIO Audio bucket bandup-audio-bucket S3_BUCKET_DOCUMENTS Documents bucket bandup-documents-bucket BEDROCK_REGION Bedrock region for Titan us-east-1 RAG_CHUNK_SIZE Chunk size for RAG 3000 RAG_CHUNK_OVERLAP Chunk overlap 300 GEMINI_MODEL Gemini model name gemini-2.0-flash Deploy Lambda Functions cd rag_flashcard pip install -r requirements.txt -t package/ cp lambda_handler.py rag_pipeline.py package/ cd package \u0026amp;\u0026amp; zip -r ../function.zip . \u0026amp;\u0026amp; cd .. # Create Lambda function aws lambda create-function \\ --function-name bandup-flashcard-generator \\ --runtime python3.11 \\ --handler lambda_handler.lambda_handler \\ --role arn:aws:iam::${AWS_ACCOUNT_ID}:role/bandup-lambda-role \\ --timeout 600 \\ --memory-size 1024 \\ --zip-file fileb://function.zip \\ --environment Variables=\u0026#34;{ GEMINI_API_KEY_SECRET_ARN=arn:aws:secretsmanager:${AWS_REGION}:${AWS_ACCOUNT_ID}:secret:bandup/gemini-api-key, BEDROCK_REGION=us-east-1, RAG_CHUNK_SIZE=3000 }\u0026#34; # Add SQS trigger aws lambda create-event-source-mapping \\ --function-name bandup-flashcard-generator \\ --event-source-arn arn:aws:sqs:${AWS_REGION}:${AWS_ACCOUNT_ID}:bandup-flashcard-queue \\ --batch-size 1 Next Steps Proceed to DynamoDB to configure the database tables.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #2 — DevOps on AWS Event Purpose Reinforce the DevOps mindset and cultural principles. Present AWS services that support CI/CD: CodeCommit, CodeBuild, CodeDeploy, CodePipeline. Deep-dive into Infrastructure as Code (IaC) with CloudFormation and AWS CDK. Explore container services (ECS, EKS, App Runner) and observability tools (CloudWatch, X-Ray). Highlights Morning: CI/CD Pipeline \u0026amp; Infrastructure as Code (08:30 – 12:00) DevOps mindset: emphasize core principles and DORA metrics (Deployment Frequency, MTTR, etc.). CI/CD services: Source control: AWS CodeCommit and Git strategies (GitFlow vs. trunk-based). Build \u0026amp; test: AWS CodeBuild. Deployment: AWS CodeDeploy (Blue/Green, Canary, Rolling strategies). Orchestration: automate pipelines with AWS CodePipeline. Demo: walkthrough of a full CI/CD pipeline. IaC: AWS CloudFormation: templates, stacks, and drift detection. AWS CDK: constructs, reusable patterns, and multi-language support. Demo \u0026amp; discussion: deploying with both CloudFormation and CDK; discussion about tool choices. Afternoon: Containers, Observability \u0026amp; Best Practices (13:00 – 17:00) Container services: Docker: fundamentals of containerization. Amazon ECR: image storage, scanning, and lifecycle management. ECS \u0026amp; EKS: deployment strategies, scaling, orchestration. App Runner: an alternative PaaS for containers. Monitoring \u0026amp; observability: AWS CloudWatch: metrics, logs, alarms, dashboards. AWS X-Ray: distributed tracing for microservices. Best practices: Feature flags, A/B testing. Incident management and postmortems. Demo \u0026amp; case study: comparing deployment strategies for microservices. Key Takeaways DevOps culture: understand DORA metrics and the automation mindset. CI/CD: how AWS Code services integrate; advanced deployment strategies (Blue/Green, Canary). IaC: foundational knowledge of CloudFormation and CDK—useful for optimizing templates and multi-stack architectures. Observability: the importance of full-stack monitoring with CloudWatch and X-Ray to debug issues (CORS, Lambda, etc.). Container services (ECS/EKS) provide guidance for more complex future architectures. Practical demos on CI/CD \u0026amp; IaC helped the team see how to optimize deployment workflows. Event Experience The full-day workshop was detailed and practical, providing applicable knowledge. Demos and case studies were helpful for project application. Good networking opportunities with the community and experts. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives Keep pace with the team\u0026rsquo;s learning progress on AWS services. Master AWS Transit Gateway setup and configuration. Deepen understanding of Amazon EC2 and related compute services. Learn Git fundamentals for effective team collaboration. Workshop: Begin VPC \u0026amp; Network Setup for the Bandup IELTS infrastructure. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Explore AWS Transit Gateway: concepts, setup process, and required resources.\n- Compare differences between VPC Peering and Transit Gateway.\n- Complete: Centralized Network Management with AWS Transit Gateway. 29/09/2025 30/09/2025 AWS Transit Gateway 3 - Deep dive into Amazon EC2 through Module 3 lectures.\n- Study EC2 Auto Scaling for automated resource management.\n- Complete: Scaling Applications with EC2 Auto Scaling. 01/10/2025 02/10/2025 FCJ Playlist 4 - Learn and practice Git commands (commit, push, pull) for team collaboration.\n- Explore Amazon Lightsail for simplified compute solutions.\n- Complete: Simplified Computing with Amazon Lightsail. 03/10/2025 04/10/2025 Git Tutorial 5 - Propose ideas and assign tasks to team members for project proposal.\n- Study migration strategies for AWS.\n- Complete: VM Migration with AWS VM Import/Export.\n- Workshop Activity: Create VPC with CIDR 10.0.0.0/16 and configure DNS support. 05/10/2025 06/10/2025 Team Meeting, Workshop 5.3 AWS Skill Builder Courses Completed Course Category Status Centralized Network Management with AWS Transit Gateway Networking ✅ Scaling Applications with EC2 Auto Scaling Compute ✅ Simplified Computing with Amazon Lightsail Compute ✅ Container Deployment with Amazon Lightsail Containers Containers ✅ VM Migration with AWS VM Import/Export Migration ✅ Database Migration with AWS DMS and SCT Migration ✅ Disaster Recovery with AWS Elastic Disaster Recovery Reliability ✅ Monitoring with Amazon CloudWatch Operations ✅ Week 4 Achievements Technical Skills Acquired:\nAWS Transit Gateway:\nMastered Transit Gateway setup and configuration Understood key advantages over VPC Peering: Supports complex multi-VPC topologies (hub-and-spoke model) Enables transitive routing between connected networks Simplifies network management at scale Supports VPN and Direct Connect attachments Learned Transit Gateway route table management Amazon EC2 Deep Dive:\nComprehensive understanding of EC2 key features: Elasticity: Scale resources up/down based on demand Flexible configurations: Multiple instance types for various workloads Cost optimization: On-Demand, Reserved, Spot instance pricing models Mastered EC2 Auto Scaling for automated resource adjustment Understood Instance Store as ephemeral block storage for EC2 Explored Amazon Lightsail as a simplified solution for small-scale applications Learned about Lightsail Containers for easy container deployment Migration Services:\nUnderstood AWS Application Migration Service (MGN) for server migration Learned VM Import/Export for virtual machine migration to AWS Explored Database Migration Service (DMS) and Schema Conversion Tool (SCT) Studied disaster recovery strategies with AWS Elastic Disaster Recovery DevOps and Monitoring:\nProficient in Git commands (commit, push, pull) and team workflows Learned CloudWatch fundamentals for monitoring AWS resources Team Collaboration:\nSuccessfully proposed ideas and assigned tasks for project proposal Team is prepared to begin implementation phase Established clear roles and responsibilities for each team member Workshop Progress - VPC \u0026amp; Network Setup:\nCreated VPC with CIDR 10.0.0.0/16 in ap-southeast-1 region Designed subnet architecture: Public subnets (10.0.1.0/24, 10.0.2.0/24) and Private subnets for App (10.0.11.0/24, 10.0.12.0/24) and DB (10.0.21.0/24, 10.0.22.0/24) across two AZs Configured Internet Gateway for public subnet internet access Set up route tables for proper traffic routing Began security group configuration for multi-tier architecture Key Takeaways:\nTransit Gateway is essential for managing complex multi-VPC architectures EC2 Auto Scaling ensures applications can handle variable load efficiently Lightsail is perfect for simple workloads without AWS complexity Migration services provide multiple paths for moving workloads to AWS VPC design with separate public/private subnets provides security isolation Multi-AZ deployment ensures high availability from the network layer "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.5-setup-be/5.5.4-task/","title":"Create Service &amp; Task","tags":[],"description":"","content":"In the final step of the backend deployment, we define the runtime configuration for the Spring Boot application and launch it as a stable ECS Service.\n1. Create Task Definition Navigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Family: bandup-backend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: 1 vCPU and 2 GB Memory. Note: Java applications (Spring Boot) generally require more memory than Node.js apps to handle the JVM heap effectively. Task Role \u0026amp; Execution Role: Select ecsTaskExecutionRole. Container Details:\nName: bandup-be-container. Image URI: Enter the ECR URI (.../band-up-backend:v1.0.0). Container Port: 8080 (Default Spring Boot port). Environment Configuration (Best Practice): Instead of manually entering sensitive variables (Database URL, User, Password) in plain text, we load them from a secure file stored in S3.\nEnvironment files: Add the S3 ARN of your .env file (e.g., arn:aws:s3:::bandup2025-fcj/.env). Requirement: Ensure your ecsTaskExecutionRole has permission to read this S3 object. 2. Create ECS Service Deploy the task definition to the cluster.\nNavigate to bandup-cluster \u0026gt; Services \u0026gt; Create. Deployment configuration: Compute options: FARGATE. Family: bandup-backend (Revision 7 or latest). Service name: bandup-backend-service. Desired tasks: 1. Networking: VPC: band-up-vpc. Subnets: Select Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-backend-sg (Created in step 5.5.2). Load Balancing: Load balancer: Select bandup-public-alb. Listener: Use existing listener 80:HTTP. Target group: Create a new target group target-bandup-be. Container info: Ensure traffic is routed to port 8080. Click Create. The service will provision the Fargate tasks, pull the image, load the environment variables from S3, and register with the ALB. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.4-setup-fe/5.4.4-task/","title":"Create Task Definition &amp; Service","tags":[],"description":"","content":"In this final step for the frontend, we define how our application container should run (Task Definition) and deploy it as a scalable service (ECS Service) connected to our Load Balancer.\n1. Create Task Definition The Task Definition serves as a blueprint for our application.\nNavigate to Amazon ECS \u0026gt; Task definitions \u0026gt; Create new task definition. Task definition configuration: Task definition family: bandup-frontend. Launch type: AWS Fargate. OS/Architecture: Linux/X86_64. Task size: .5 vCPU and 1 GB Memory (Sufficient for our Next.js frontend). Task Role \u0026amp; Task Execution Role: Select ecsTaskExecutionRole (Created in section 5.3.3). Container details: Name: bandup-fe-container. Image URI: Enter the ECR URI we pushed earlier (e.g., .../band-up-frontend:v1.0.0). Container port: 3000. Click Create. 2. Create ECS Service Now we deploy this blueprint into our Cluster.\nNavigate to Clusters \u0026gt; Select bandup-cluster. In the Services tab, click Create. Step 1: Environment\nCompute options: Launch type -\u0026gt; FARGATE. Task definition: bandup-frontend (Revision 1). Service name: bandup-frontend-service. Desired tasks: 1. Step 2: Networking\nVPC: band-up-vpc. Subnets: Select the Private Subnets (private-subnet-1, private-subnet-2). Security group: Select ecs-private-sg (This allows traffic from ALB). Step 3: Load Balancing\nLoad balancer type: Application Load Balancer. Load balancer: Select bandup-public-alb. Container to load balance: bandup-fe-container 3000:3000. Listener: Create a new listener on Port 80 (HTTP). Target group: Use an existing target group -\u0026gt; target-bandup-fe. Click Create. The service will start deploying your container. Wait until the status changes to Active and the Task status is Running. 3. Verify Deployment Once the service is stable, open your web browser and navigate to the DNS name of your Application Load Balancer.\nYou should see the IELTS BandUp landing page loading successfully, served from your container in the private subnet.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.4-setup-fe/","title":"Frontend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Frontend (Next.js application) to the AWS Cloud.\nWe will utilize Amazon Elastic Container Service (ECS) with the Fargate launch type. This serverless approach allows us to run containers without managing the underlying EC2 instances. The frontend service will be placed in Private Subnets for security but will be accessible to users via the Application Load Balancer (ALB) we configured in the previous section.\nImplementation Steps To successfully deploy the frontend, we will follow this workflow:\nContainer Registry (ECR): Create a repository to store our Docker images and push the local application code to AWS. Security Configuration: Define specific security group rules allowing the Frontend container to receive traffic only from the ALB. ECS Task \u0026amp; Service: Define the blueprint (Task Definition) for our container (CPU, Memory, Environment Variables) and launch it as a stable Service. Content Dockerize Application Setup ECR \u0026amp; Push Image Configure Security Group Create Task Definition \u0026amp; Service "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.3-network/5.3.4-endpoints/","title":"VPC Endpoints Setup","tags":[],"description":"","content":"To ensure security, our backend services running in Private Subnets should not access key AWS services over the public internet. Instead, we use AWS PrivateLink (VPC Endpoints) to keep this traffic within the AWS network.\nWe will create 4 Endpoints:\nInterface Endpoints: For ECR (Docker \u0026amp; API) and CloudWatch Logs. Gateway Endpoint: For Amazon S3. 1. Create Interface Endpoints (ECR \u0026amp; CloudWatch) We will start by creating the endpoint for ECR Docker (ecr.dkr). The process is identical for ECR API (ecr.api) and CloudWatch (logs).\nStep 1: Service Selection\nNavigate to VPC Dashboard \u0026gt; Endpoints \u0026gt; Create endpoint. Name tag: ecr-endpoint (for Docker). Service category: Select AWS services. Services: Search for ecr.dkr and select com.amazonaws.ap-southeast-1.ecr.dkr. Step 2: VPC \u0026amp; Subnets\nVPC: Select band-up-vpc. Subnets: Select the Availability Zones and choose the Private Subnets (private-app-subnet-1 and private-app-subnet-2). Note: This creates Elastic Network Interfaces (ENIs) in your private subnets to serve as entry points. Step 3: Security Group\nSecurity groups: Select the Security Group that allows HTTPS (Port 443) traffic from your VPC. For this workshop, you can use the default security group if it allows inbound traffic from within the VPC. Click Create endpoint. Step 4: Repeat for ECR API and CloudWatch Repeat the steps above to create two more Interface Endpoints:\nECR API: Search for ecr.api -\u0026gt; Name: ecr-api-endpoint. CloudWatch Logs: Search for logs -\u0026gt; Name: cloudwatch-endpoint. 2. Create Gateway Endpoint (S3) For Amazon S3, we use a Gateway Endpoint, which is cost-effective and uses routing tables instead of network interfaces.\nClick Create endpoint. Name tag: s3-endpoint. Services: Search for s3 and select com.amazonaws.ap-southeast-1.s3 (Type: Gateway). VPC: Select band-up-vpc. Route tables: Select the Route Tables associated with your Private Subnets. Click Create endpoint. 3. Verify All Endpoints Once completed, navigate to the Endpoints list. You should see 4 active endpoints ensuring secure connectivity for your infrastructure.\necr-endpoint (Interface) ecr-api-endpoint (Interface) cloudwatch-endpoint (Interface) s3-endpoint (Gateway) "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Event 1 — Kick-off: The First Cloud Journey (FCJ) Date: 06/09/2025 Location: Online Role: Attendee Summary: Official kick-off for the 12-week FCJ program: introductions, program vision, roadmap overview, team formation guidance, and AWS account security setup (2FA, budgets). Outcome / Lessons Learned: Clear understanding of program expectations, the importance of discipline and commitment, and initial AWS security best practices. Event 2 — DX Talk#7: Reinventing DevSecOps with AWS Generative AI Date: 16/10/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Strategic and practical perspectives on integrating Generative AI into DevSecOps; case studies from CMC Global and AWS; an overview of CI/CD and security tooling (Jenkins, SonarQube, OWASP ZAP, Terraform) and Amazon Q Developer. Outcome / Lessons Learned: Insights into AI-enabled DevSecOps workflows, automated security checks, and tools to integrate into pipelines. Event 3 — AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Date: 15/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Overview of AI/ML in Vietnam and AWS services (Amazon SageMaker); introduction to Generative AI via Amazon Bedrock, prompt engineering and Retrieval-Augmented Generation (RAG); live demo building a GenAI chatbot. Outcome / Lessons Learned: Gained practical knowledge of SageMaker and Bedrock, RAG techniques, and GenAI prototyping. Event 4 — AWS Cloud Mastery Series #2: DevOps on AWS Date: 17/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: DevOps culture and practices; AWS CI/CD services; Infrastructure as Code (CloudFormation, CDK); container services (ECS, EKS, App Runner); observability (CloudWatch, X-Ray); and pipeline demos. Outcome / Lessons Learned: Practical experience with CI/CD patterns, IaC best practices, container strategies, and monitoring for cloud applications. Event 5 — AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar Date: 29/11/2025 Location: Bitexco Tower, Ho Chi Minh City / Online Role: Attendee Summary: Security Pillar deep dive: Identity \u0026amp; Access Management, detection (GuardDuty, CloudTrail), infrastructure protection, data protection (KMS, Secrets Manager), and incident response automation. Outcome / Lessons Learned: Reinforced security fundamentals, IAM best practices, data encryption, and incident response playbooks and automation. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/5.6.4-dynamodb/","title":"DynamoDB","tags":[],"description":"","content":"Overview Create two DynamoDB tables to store Lambda function results:\nTable Used By Purpose bandup-evaluations Writing + Speaking Evaluators Stores IELTS band scores, feedback, transcripts bandup-flashcard-sets Flashcard Generator Stores generated flashcards and document metadata Table 1: Evaluations Table Stores results from both Writing Evaluator and Speaking Evaluator Lambda functions.\nSetting Value Table name bandup-evaluations Partition key evaluation_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description evaluation_id String Unique session ID (PK) user_id String User identifier (SK) evaluation_type String writing or speaking status String processing, completed, failed overall_band String Overall IELTS band score (e.g., \u0026ldquo;7.0\u0026rdquo;) task_achievement_band String Writing only coherence_band String Writing only lexical_band String Both Writing and Speaking grammar_band String Both Writing and Speaking fluency_band String Speaking only pronunciation_band String Speaking only transcript String Speaking only - transcribed audio feedback String JSON-encoded detailed feedback model_used String AI model used for evaluation created_at Number Unix timestamp Example Item (Writing):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;eval-abc123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;writing\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;task_achievement_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;coherence_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;strengths\\\u0026#34;:[...],\\\u0026#34;weaknesses\\\u0026#34;:[...]}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-writing_task2\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Example Item (Speaking):\n{ \u0026#34;evaluation_id\u0026#34;: \u0026#34;speak-xyz789\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;evaluation_type\u0026#34;: \u0026#34;speaking\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;overall_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;fluency_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;lexical_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;grammar_band\u0026#34;: \u0026#34;7.0\u0026#34;, \u0026#34;pronunciation_band\u0026#34;: \u0026#34;6.5\u0026#34;, \u0026#34;transcript\u0026#34;: \u0026#34;Well, I\u0026#39;d like to talk about...\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;120.5\u0026#34;, \u0026#34;word_count\u0026#34;: 185, \u0026#34;feedback\u0026#34;: \u0026#34;{\\\u0026#34;fluency\\\u0026#34;:{...},\\\u0026#34;pronunciation\\\u0026#34;:{...}}\u0026#34;, \u0026#34;model_used\u0026#34;: \u0026#34;gemini-2.5-flash-audio\u0026#34;, \u0026#34;created_at\u0026#34;: 1733644800 } Table 2: Flashcard Sets Table Stores results from Flashcard Generator Lambda function.\nSetting Value Table name bandup-flashcard-sets Partition key set_id (String) Sort key user_id (String) Billing mode On-demand (PAY_PER_REQUEST) Table Schema:\nAttribute Type Description set_id String Unique flashcard set ID (PK) user_id String User identifier (SK) document_id String Source document S3 key status String processing, completed, failed flashcards String JSON-encoded array of flashcards total_cards Number Number of flashcards generated page_count Number Number of pages in source PDF chunk_count Number Number of text chunks indexed created_at Number Unix timestamp Example Item:\n{ \u0026#34;set_id\u0026#34;: \u0026#34;flashcard-set-123\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user-456\u0026#34;, \u0026#34;document_id\u0026#34;: \u0026#34;uploads/documents/user-456/vocab-guide.pdf\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;, \u0026#34;flashcards\u0026#34;: \u0026#34;[{\\\u0026#34;question\\\u0026#34;:\\\u0026#34;What is...\\\u0026#34;,\\\u0026#34;answer\\\u0026#34;:\\\u0026#34;...\\\u0026#34;}]\u0026#34;, \u0026#34;total_cards\u0026#34;: 15, \u0026#34;page_count\u0026#34;: 8, \u0026#34;chunk_count\u0026#34;: 24, \u0026#34;created_at\u0026#34;: 1733644800 } Create Tables with AWS CLI # Create evaluations table (Writing + Speaking) aws dynamodb create-table \\ --table-name bandup-evaluations \\ --attribute-definitions \\ AttributeName=evaluation_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=evaluation_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production # Create flashcard sets table aws dynamodb create-table \\ --table-name bandup-flashcard-sets \\ --attribute-definitions \\ AttributeName=set_id,AttributeType=S \\ AttributeName=user_id,AttributeType=S \\ --key-schema \\ AttributeName=set_id,KeyType=HASH \\ AttributeName=user_id,KeyType=RANGE \\ --billing-mode PAY_PER_REQUEST \\ --tags Key=Project,Value=bandup Key=Environment,Value=production Enable Point-in-Time Recovery Enable PITR for data protection:\n# Enable PITR for evaluations table aws dynamodb update-continuous-backups \\ --table-name bandup-evaluations \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true # Enable PITR for flashcard sets table aws dynamodb update-continuous-backups \\ --table-name bandup-flashcard-sets \\ --point-in-time-recovery-specification PointInTimeRecoveryEnabled=true Query Patterns Get user\u0026rsquo;s evaluation history:\nresponse = table.query( IndexName=\u0026#39;user_id-created_at-index\u0026#39;, # If GSI exists KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), ScanIndexForward=False, # Most recent first Limit=10 ) Get specific evaluation by ID:\nresponse = table.get_item( Key={ \u0026#39;evaluation_id\u0026#39;: \u0026#39;eval-abc123\u0026#39;, \u0026#39;user_id\u0026#39;: \u0026#39;user-456\u0026#39; } ) Get user\u0026rsquo;s flashcard sets:\nresponse = table.query( KeyConditionExpression=Key(\u0026#39;user_id\u0026#39;).eq(\u0026#39;user-456\u0026#39;), FilterExpression=Attr(\u0026#39;status\u0026#39;).eq(\u0026#39;completed\u0026#39;) ) Lambda Environment Variables Configure Lambda functions to use these tables:\nLambda Function Environment Variable Value Writing Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Speaking Evaluator DYNAMODB_EVALUATIONS bandup-evaluations Flashcard Generator DYNAMODB_FLASHCARD_SETS bandup-flashcard-sets IAM Policy for Lambda Access { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;DynamoDBAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:PutItem\u0026#34;, \u0026#34;dynamodb:GetItem\u0026#34;, \u0026#34;dynamodb:UpdateItem\u0026#34;, \u0026#34;dynamodb:Query\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-evaluations\u0026#34;, \u0026#34;arn:aws:dynamodb:${AWS_REGION}:${AWS_ACCOUNT_ID}:table/bandup-flashcard-sets\u0026#34; ] } ] } Next Steps Proceed to Bedrock Integration to configure Amazon Titan Embeddings.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Event Report: AWS Cloud Mastery Series #3 — AWS Well-Architected Security Pillar Event Purpose Introduce the role of the Security Pillar within the AWS Well-Architected Framework. Present the five core pillars of cloud security: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. Provide best practices and practical playbooks to protect cloud applications. Highlights Pillar 1 — Identity \u0026amp; Access Management (08:50 – 09:30) Principles: Least Privilege, Zero Trust, Defense in Depth. Modern IAM: avoid long-term credentials; prefer Roles and Policies. IAM Identity Center: SSO and management of Permission Sets. Multi-account security: SCPs (Service Control Policies) and Permission Boundaries. Mini demo: validate IAM policies and simulate access. Pillar 2 — Detection (09:30 – 09:55) Continuous monitoring: CloudTrail (organization-level), GuardDuty, Security Hub. Logging at all layers: VPC Flow Logs, ALB/S3 logs. Automated alerting: using EventBridge. Pillar 3 — Infrastructure Protection (10:10 – 10:40) Network security: VPC segmentation (private vs. public). Defenses: Security Groups vs. NACLs; using WAF, Shield, Network Firewall. Workload security: securing EC2, basics for ECS/EKS. Pillar 4 — Data Protection (10:40 – 11:10) Encryption: encryption at rest \u0026amp; in transit (S3, EBS, RDS, DynamoDB). Key and secret management: KMS, Secrets Manager, Parameter Store. Data classification and access guardrails. Pillar 5 — Incident Response (11:10 – 11:40) IR lifecycle: AWS-recommended incident response processes. IR playbook \u0026amp; automation. Sample scenarios: compromised IAM key, public S3 exposure, EC2 malware detection. Automated response using Lambda / Step Functions. What I Learned Understand the five Security Pillars and the Shared Responsibility Model. Advanced IAM: using IAM Identity Center, SCPs, and avoiding long-term credentials. Data security: the importance of KMS and managing secrets. Incident Response: building playbooks and automating responses with serverless. Event Experience The workshop served as the final summary session in the series, providing essential security knowledge before project completion. The IAM Identity Center and Secrets Manager presentations helped address Sub ID authentication issues and API key management for the team. IR scenarios (e.g., S3 public exposure) were valuable for reinforcing project security policies. The final Q\u0026amp;A helped outline the next learning path (Security Specialty). "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives Identify and resolve abnormal AWS costs on the account. Design and partition infrastructure architecture for the project. Begin initial project configuration and assign team roles. Explore AWS Skill Builder and advance learning on optimization topics. Workshop: Complete VPC Network Setup and begin Database \u0026amp; Storage Setup. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Analyze and identify causes of abnormal costs on AWS account.\n- Complete: Cost and Usage Management and Managing Quotas with Service Quotas. 07/10/2025 08/10/2025 AWS Cost Explorer 3 - Design and partition project infrastructure architecture.\n- Propose basic architecture templates for team reference.\n- Complete: Building Highly Available Web Applications.\n- Workshop Activity: Configure NAT Gateways in public subnets and complete Security Groups setup. 09/10/2025 10/10/2025 FCJ Community, Workshop 5.3 4 - Build code skeleton and configure initial project files.\n- Set up development environment.\n- Complete: Development Environment with AWS Toolkit for VS Code.\n- Workshop Activity: Begin Database \u0026amp; Storage Setup - plan RDS PostgreSQL and ElastiCache Redis configurations. 11/10/2025 13/10/2025 VS Code + AWS Toolkit, Workshop 5.6 5 - Register for AWS Skill Builder and explore courses.\n- Study EC2 optimization techniques.\n- Complete: Right-Sizing with EC2 Resource Optimization. 11/10/2025 12/10/2025 AWS Skill Builder AWS Skill Builder Courses Completed Course Category Status Cost and Usage Management Cost Optimization ✅ Managing Quotas with Service Quotas Operations ✅ Billing Console Delegation Cost Management ✅ Right-Sizing with EC2 Resource Optimization Cost Optimization ✅ Development Environment with AWS Toolkit for VS Code Development ✅ Building Highly Available Web Applications Architecture ✅ Database Essentials with Amazon RDS Database ✅ NoSQL Database Essentials with Amazon DynamoDB Database ✅ In-Memory Caching with Amazon ElastiCache Database ✅ Command Line Operations with AWS CLI Operations ✅ Week 5 Achievements Technical Skills Acquired:\nCost Optimization:\nIdentified causes of abnormal AWS costs: Incomplete deletion of EC2 resources (EBS volumes, Elastic IPs) Lack of control over user accounts and IAM permissions Resources left running in unused regions Learned AWS cost management best practices: AWS Budgets for proactive cost alerts Cost Explorer for analyzing spending patterns Service Quotas for managing account limits Billing Console Delegation for team cost visibility Proposed cost optimization measures for the team Architecture Design:\nSuccessfully designed project infrastructure architecture Created reference architecture templates for team adoption Applied High Availability principles: Multi-AZ deployments Load balancing strategies Database replication patterns Fault-tolerant design patterns Development Environment:\nSet up AWS Toolkit for VS Code for streamlined development Mastered AWS CLI for command-line operations Built robust code skeleton with initial configuration files Established project foundation for team collaboration Database Services:\nUnderstood Amazon RDS for relational database needs Learned DynamoDB for NoSQL workloads Explored ElastiCache for in-memory caching (Redis/Memcached) Applied database selection criteria based on use cases Project Progress:\nRegistered and activated AWS Skill Builder account Began exploring advanced courses and learning paths Infrastructure architecture finalized and documented Development environment configured and ready for coding Workshop Progress - Network \u0026amp; Database Setup:\nCompleted NAT Gateway deployment in both public subnets for private subnet internet access Configured Security Groups for ALB, ECS, RDS, and ElastiCache tiers with least-privilege access Set up route tables: Public routes to Internet Gateway, Private routes to NAT Gateways Designed RDS PostgreSQL Multi-AZ configuration for high availability Planned ElastiCache Redis cluster for session management and caching Configured S3 buckets for static assets and document storage Key Takeaways:\nCost optimization starts with visibility - use Cost Explorer daily Right-sizing EC2 instances can reduce costs by 30-50% High availability requires planning across multiple AZs AWS Toolkit for VS Code significantly improves developer productivity Database selection depends on data model, scale, and access patterns Service Quotas prevent unexpected capacity limitations NAT Gateways enable private subnet resources to access internet securely Security Groups provide defense-in-depth with multiple layers of protection "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.5-setup-be/","title":"Backend Deployment (ECS Fargate)","tags":[],"description":"","content":"Overview In this section, we will deploy the IELTS BandUp Backend, a Spring Boot application that serves as the core logic layer of the platform.\nUnlike the Frontend, the Backend requires persistent storage and caching mechanisms to function effectively. Therefore, before launching the application containers on ECS Fargate, we must provision the data infrastructure (PostgreSQL and Redis). The Backend service will reside in Private Subnets, strictly protected by Security Groups, and will communicate with the AI services via AWS SDK.\nImplementation Steps To deploy the fully functional backend system, we will follow this sequence:\nContainer Registry (ECR): Build the Spring Boot application and push the Docker image to a private ECR repository. Relational Database (RDS): Provision an Amazon RDS for PostgreSQL instance to store user data, test results, and content. In-Memory Cache (ElastiCache): Set up an Amazon ElastiCache (Redis) cluster for session management and high-speed data retrieval. ECS Task \u0026amp; Service: Define the backend task configuration (including environment variables for DB connections) and launch the service. Content Setup ECR \u0026amp; Push Image Create PostgreSQL RDS Create ElastiCache (Redis) Create Service \u0026amp; Task "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/","title":"Workshop","tags":[],"description":"","content":"IELTS Self-Learning Web System - AWS Infrastructure Workshop Overview This comprehensive workshop guides you through building a production-ready AWS infrastructure for the IELTS Self-Learning Web System. You will learn how to deploy a highly available, scalable, and secure web application using modern AWS services and best practices.\nThe architecture implements an active-passive Multi-AZ deployment pattern on Amazon ECS, with a serverless AI service layer for intelligent assessment and content generation.\nWhat You Will Build By completing this workshop, you will have deployed:\nComponent AWS Service Purpose Network Layer VPC, Subnets, NAT Gateway Isolated, secure network infrastructure Container Platform ECS Fargate, ECR Serverless container orchestration Load Balancing ALB, Route 53, ACM Traffic distribution and SSL termination Data Layer RDS PostgreSQL, ElastiCache, S3 Relational database, caching, object storage AI Services API Gateway, SQS, Lambda, DynamoDB Serverless AI processing pipeline CI/CD CodePipeline, CodeBuild Automated deployment pipeline Security IAM, Secrets Manager, WAF Identity management and protection Monitoring CloudWatch Logs, Alarms Observability and alerting Architecture Highlights High Availability Design:\nMulti-AZ deployment across two Availability Zones Active-passive failover for ECS services RDS Multi-AZ with automatic failover Application Load Balancer with health checks Serverless AI Architecture:\nAPI Gateway for RESTful AI endpoints SQS for asynchronous message processing Lambda functions for Writing Assessment, Speaking Assessment, and RAG-based Flashcard Generation DynamoDB for storing AI results Amazon Bedrock integration for AI models (Gemma 3 12B, Titan Embeddings) Google Gemini API for smart query generation Security Best Practices:\nPrivate subnets for application and database tiers Security groups with least-privilege access AWS WAF for application-level protection Secrets Manager for credential management IAM roles with minimal required permissions Prerequisites Before starting this workshop, ensure you have:\nAn AWS account with appropriate permissions AWS CLI installed and configured Basic understanding of AWS services (VPC, EC2, ECS) Docker installed locally for container builds Git for version control Time to Complete Section Estimated Time Prerequisites 15 minutes VPC \u0026amp; Network Setup 30 minutes ECS \u0026amp; Container Setup 45 minutes Load Balancer Configuration 30 minutes Database \u0026amp; Storage Setup 45 minutes AI Service Architecture 60 minutes CI/CD Pipeline 30 minutes Security \u0026amp; IAM 30 minutes Monitoring Setup 20 minutes Total ~5 hours Content Workshop Overview Prerequisites VPC \u0026amp; Network Setup ECS \u0026amp; Container Setup Load Balancer Configuration Database \u0026amp; Storage Setup AI Service Architecture CI/CD Pipeline Security \u0026amp; IAM Monitoring \u0026amp; Logging Clean Up "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/5.6.5-bedrock-integration/","title":"Bedrock Integration","tags":[],"description":"","content":"Overview Configure Amazon Bedrock for AI model access including Gemma 3 12B and Titan Embeddings.\nEnable Model Access Navigate to Amazon Bedrock → Model access Request access to: Amazon Titan Text Express (for assessments) Amazon Titan Embeddings V2 (for RAG) Meta Llama 3 or Anthropic Claude (optional) Test Bedrock API import boto3 import json bedrock = boto3.client(\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) # Test Titan Text response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-text-express-v1\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Hello, how are you?\u0026#39;, \u0026#39;textGenerationConfig\u0026#39;: { \u0026#39;maxTokenCount\u0026#39;: 100, \u0026#39;temperature\u0026#39;: 0.7 } }) ) print(json.loads(response[\u0026#39;body\u0026#39;].read())) Titan Embeddings for RAG # Generate embeddings response = bedrock.invoke_model( modelId=\u0026#39;amazon.titan-embed-text-v2:0\u0026#39;, body=json.dumps({ \u0026#39;inputText\u0026#39;: \u0026#39;Document chunk text here\u0026#39; }) ) embedding = json.loads(response[\u0026#39;body\u0026#39;].read())[\u0026#39;embedding\u0026#39;] # Store in OpenSearch or use for similarity search Google Gemini Integration For smart query generation:\nimport google.generativeai as genai genai.configure(api_key=os.environ[\u0026#39;GEMINI_API_KEY\u0026#39;]) model = genai.GenerativeModel(\u0026#39;gemini-2.5-flash\u0026#39;) response = model.generate_content( f\u0026#34;\u0026#34;\u0026#34;Analyze this document and generate 10 intelligent questions: {document_text} \u0026#34;\u0026#34;\u0026#34; ) Cost Optimization Use Titan Text Express for assessments (lower cost) Batch embeddings generation where possible Implement caching for repeated queries Use Google Gemini free tier for query generation Next Steps Proceed to CI/CD Pipeline.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives Master fundamental AWS storage services and their use cases. Enhance Python programming skills through practical exercises. Design and refine the project\u0026rsquo;s infrastructure architecture. Attend the \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar to explore DevSecOps practices and Amazon Q Developer. Workshop: Complete Database \u0026amp; Storage Setup and configure S3 for static assets. Tasks Completed This Week Day Task Start Date Completion Date Reference Material 2 - Study Amazon S3 fundamentals: Bucket architecture, durability guarantees, and static website hosting capabilities.\n- Explore S3 Storage Classes (Standard, Standard-IA) and Amazon Glacier for cold storage solutions.\n- Complete: Static Website Hosting with Amazon S3. 14/10/2025 15/10/2025 AWS S3 Documentation 3 - Learn AWS Storage Gateway types (File, Volume, Tape Gateway) and their integration patterns.\n- Understand Object Lifecycle Management policies for cost optimization.\n- Practice Python fundamentals: data structures, functions, and error handling.\n- Attend webinar: \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; featuring Hoàng Kha. 16/10/2025 17/10/2025 AWS Storage Gateway\nAWS Events 4 - Study disaster recovery concepts: RTO, RPO, and Backup \u0026amp; Restore strategies.\n- Explore AWS Backup service for centralized backup management.\n- Hands-on: Create S3 buckets, upload files, configure static website hosting, and test lifecycle policies.\n- Research DevSecOps methodologies: CI/CD pipelines, SAST/DAST tools, Infrastructure as Code.\n- Workshop Activity: Deploy RDS PostgreSQL Multi-AZ instance and ElastiCache Redis cluster. 18/10/2025 19/10/2025 AWS Backup, Workshop 5.6 5 - Finalize project infrastructure architecture diagram with detailed component relationships.\n- Restructure code skeleton to align with the updated architecture design.\n- Standardize programming language and framework selection for team consistency.\n- Explore Amazon Q Developer capabilities: AI-powered code generation, testing, and vulnerability scanning. 20/10/2025 21/10/2025 Amazon Q Developer AWS Skill Builder Courses Completed Course Category Status Static Website Hosting with Amazon S3 Storage ✅ Data Protection with AWS Backup Reliability ✅ Content Delivery with Amazon CloudFront Networking ✅ Week 6 Achievements Storage Services Mastery:\nComprehensive understanding of Amazon S3 architecture: Buckets, durability (99.999999999%), and static website hosting Mastered S3 Storage Classes: Standard, Standard-IA, Glacier for different access patterns Learned AWS Storage Gateway integration patterns for hybrid cloud storage Understood Object Lifecycle Management for automated data tiering and cost optimization Disaster Recovery \u0026amp; Backup:\nGrasped disaster recovery fundamentals: RTO (Recovery Time Objective) and RPO (Recovery Point Objective) Learned AWS Backup service for centralized backup management across services Understood Backup \u0026amp; Restore strategies for business continuity Development Skills:\nEnhanced Python programming through practical exercises Successfully created S3 buckets, configured static websites, and tested lifecycle policies Improved understanding of data structures and error handling Project Planning:\nFinalized comprehensive infrastructure architecture diagram Restructured code skeleton with proper directory structure Standardized technology stack for team collaboration DevSecOps Insights:\nAttended \u0026ldquo;Reinventing DevSecOps with AWS Generative AI\u0026rdquo; webinar (October 16, 2025) Learned DevSecOps integration: Security in SDLC using Jenkins (CI/CD), SonarQube (SAST), OWASP ZAP (DAST), Terraform (IaC) Explored Amazon Q Developer: AI assistant for code generation, testing, vulnerability scanning, and AWS optimization Workshop Progress - Database \u0026amp; Storage:\nDeployed RDS PostgreSQL Multi-AZ instance in private DB subnets for high availability Configured ElastiCache Redis cluster for session management and application caching Set up S3 buckets for static website assets, user uploads, and document storage Configured S3 lifecycle policies for cost optimization (transitioning to Glacier) Established database connection strings and caching endpoints for application integration Implemented database backup strategies using AWS Backup service Key Takeaways:\nS3 is the foundation for object storage in AWS - understanding storage classes is crucial for cost optimization Lifecycle policies automate data management and reduce storage costs significantly AWS Backup provides unified backup management across multiple AWS services DevSecOps integrates security throughout the development lifecycle, not as an afterthought RDS Multi-AZ provides automatic failover for database high availability ElastiCache significantly improves application performance through in-memory caching Private subnets for databases provide additional security layer "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.6-ai-service/","title":"AI Service Architecture","tags":[],"description":"","content":"Overview This section covers the serverless AI service architecture using API Gateway, SQS, Lambda, DynamoDB, and Amazon Bedrock for intelligent assessment and content generation.\nAI Service Architecture The AI service implements a fully serverless pattern:\nUser → API Gateway → SQS Queue → Lambda → AI Model → DynamoDB → Response Three Lambda Functions:\nFunction Purpose AI Model Writing Evaluate IELTS writing assessment Gemma 3 12B / Gemini Speaking Evaluate Audio transcription + assessment Transcribe + Gemma 3 12B Flashcard Generate RAG-based flashcard creation Titan Embeddings + Gemini Content API Gateway SQS Queues Lambda Functions DynamoDB Bedrock Integration Request Flow User submits request (writing sample, audio, document) API Gateway validates and enqueues message to SQS SQS triggers appropriate Lambda function Lambda processes with AI model (Bedrock/Gemini) Results stored in DynamoDB User retrieves results via API Estimated Time: ~60 minutes Cost Estimate Monthly Cost Summary:\nUpfront Cost Monthly Cost Total 12 Months Cost Currency $0.00 $23.61 $283.32 USD Note: Includes upfront cost\nDetailed Cost Breakdown:\nService Description Region Monthly Cost (USD) Annual Cost (USD) AWS Lambda Writing Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Speaking Evaluator Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda Evaluation Status Asia Pacific (Singapore) $0.00 $0.00 AWS Lambda S3 Upload Asia Pacific (Singapore) $0.00 $0.00 Amazon API Gateway HTTP API Asia Pacific (Singapore) $0.42 $5.04 S3 Standard Audio bucket Asia Pacific (Singapore) $0.51 $6.12 S3 Standard Documents Bucket Asia Pacific (Singapore) $0.53 $6.36 DynamoDB Evaluations Table Asia Pacific (Singapore) $0.37 $4.44 DynamoDB Flashcard Sets Table Asia Pacific (Singapore) $0.52 $6.24 Amazon SQS Writing/Speaking/Flashcard queues Asia Pacific (Singapore) $0.00 $0.00 AWS Secrets Manager Secrets management Asia Pacific (Singapore) $0.45 $5.40 Amazon CloudWatch RAG Lambda logs Asia Pacific (Singapore) $0.01 $0.08 Amazon Bedrock Bedrock inference US East (N. Virginia) $0.50 $6.00 OpenAI GPT inference US East (N. Virginia) $20.30 $243.65 Total $23.61 $283.32 AWS Pricing Calculator provides only an estimate of your AWS fees and doesn\u0026rsquo;t include any taxes that might apply. Your actual fees depend on a variety of factors, including your actual usage of AWS services.\nView detailed cost breakdown: AWS Pricing Calculator\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Internship Overview From September 8 to December 9, 2024, I interned at Amazon Web Services (AWS) and immersed myself in the daily rhythm of a production-grade cloud team. The experience forced me to connect classroom theories with the realities of deploying resilient, secure, and cost-aware workloads.\nMy primary project was the Bandup IELTS Learning Platform, an AI-assisted learning experience that evaluates Writing and Speaking tasks and generates contextual flashcards. I owned several backend and AI integration streams that turned the concept into a working system students could interact with.\nKey Achievements Working on Bandup allowed me to sharpen both technical depth and product thinking:\nCloud Architecture \u0026amp; AWS Services\nSketched and iterated on a serverless blueprint using AWS Lambda, API Gateway, and SQS to decouple workloads Hardened networking by carving VPC public/private subnets, NAT Gateways, and Security Groups aligned with AWS Well-Architected guidance Provisioned container workloads on Amazon ECS with Fargate to host auxiliary services Chose the right data stores—Amazon S3 for documents, DynamoDB for high-scale metadata, and ElastiCache (Redis) for caching test artifacts Experimented with Amazon Bedrock (Titan Embeddings) alongside Google Gemini API to bring AI insight closer to end users Development \u0026amp; DevOps\nImplemented Python Lambda functions that grade submissions and trigger flashcard creation Built the RAG (Retrieval-Augmented Generation) flow that indexes materials and surfaces contextually relevant snippets Wired CI/CD pipelines through GitLab runners and AWS CodePipeline so every change moved predictably from commit to deployment Practiced Infrastructure as Code, keeping environments reproducible and reviewable AI/ML Integration\nUsed Gemini’s native audio pipeline to process speaking samples at roughly 72% lower cost than a custom stack Embedded lesson chunks with Amazon Titan Text Embeddings V2 to support semantic search and scoring explanations Iterated on prompt engineering tactics to align AI scoring outputs with IELTS descriptors Work Ethics I held myself accountable for every deliverable by:\nClosing tasks with production-ready quality instead of proof-of-concept shortcuts Respecting AWS security guardrails and consistently reviewing cost impact before rolling changes out Scheduling proactive syncs with mentors so blockers surfaced early and were resolved quickly Capturing architecture decisions, runbooks, and test evidence to reduce knowledge gaps for anyone inheriting my work Self-Evaluation Criteria To gauge how much I truly grew, I rated myself on metrics that mattered to the project and team:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applying AWS design patterns, selecting the right services, and delivering production-grade code ✅ ☐ ☐ 2 Ability to learn Picking up new AI services and DevOps tooling quickly enough to unlock project milestones ✅ ☐ ☐ 3 Proactiveness Investigating design options and AWS docs before escalating for help ✅ ☐ ☐ 4 Sense of responsibility Owning Lambda features end-to-end, from architecture discussions to deployment checklists ✅ ☐ ☐ 5 Discipline Staying aligned with sprint ceremonies, coding standards, and daily stand-ups ☐ ✅ ☐ 6 Growth mindset Treating code reviews and architectural feedback as chances to iterate fast ✅ ☐ ☐ 7 Communication Explaining trade-offs in docs and demos, while still improving presentation clarity ☐ ✅ ☐ 8 Teamwork Supporting mentors and peers, sharing test data, and pairing on tricky debugging sessions ✅ ☐ ☐ 9 Professional conduct Following AWS security expectations, respecting data privacy, and keeping commitments ✅ ☐ ☐ 10 Problem-solving skills Unblocking Lambda bugs, tracing performance issues, and driving the 72% audio-processing savings ✅ ☐ ☐ 11 Contribution to project/team Shipping four Lambda services plus documentation, demos, and a repeatable AI pipeline ✅ ☐ ☐ 12 Overall Holistic view of my readiness to contribute as a junior cloud engineer ✅ ☐ ☐ Key Learnings Technical Skills Gained\nStrengthened my mental model for building serverless-first systems on AWS Proved out AI/ML integration patterns that translate research ideas into usable product features Became confident writing structured, testable Python for Lambda runtimes Internalized how RAG pipelines, vector stores, and embeddings interact to serve real queries Soft Skills Developed\nCrafted documentation that balances architectural detail with actionable steps Broke down ambiguous issues into root causes and tracked fixes methodically Evaluated every feature for cost impact, looking for savings before launch Adapted to enterprise collaboration rhythms, tools, and review processes Areas for Improvement Discipline: Build firmer routines so even on hectic days I never miss status updates or deadlines Communication: Practice storytelling for technical demos to make non-technical listeners comfortable Problem-solving: Adopt lighter-weight runbooks for debugging so future incidents resolve faster Networking: Invest time connecting with more AWS teams to better understand adjacent services and opportunities Gratitude Thank you to the mentors who trusted me with real production responsibilities, the operations team that kept the environment stable while I iterated, and AWS for opening its doors to curious students like me. The lessons from this internship now shape how I plan, build, and communicate every new idea.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives Focus on comprehensive review and knowledge consolidation in preparation for the mid-term exam. Practice hands-on labs and multiple-choice questions on AWS Builders and AWSboy platforms to familiarize with exam format. Systematize fundamental AWS services learned: EC2, S3, VPC, IAM, RDS, Lambda, DynamoDB. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Systematize knowledge of Compute services (EC2, Lambda). - Hands-on: Complete exercises/labs on creating, configuring, and managing the lifecycle of EC2 Instances. - Review Lambda function creation, triggers, and execution models. 22/10/2024 22/10/2024 AWS Builders, AWSboy Tuesday - Review knowledge of Storage services (S3, EBS, EFS). - Hands-on: Complete exercises on S3 storage classes (Standard, IA, Glacier), EBS volume types, and EFS use cases. - Practice S3 bucket policies and access control. 23/10/2024 23/10/2024 AWS Builders, AWSboy Wednesday - Consolidate knowledge of Networking (VPC, Subnets, Route Tables, Internet Gateway, Security Groups, NACLs). - Hands-on: Practice questions on VPC configuration, Security Group rules vs NACL rules, and routing principles. - Review VPC Peering and Transit Gateway concepts. 24/10/2024 24/10/2024 AWS Builders, AWSboy Thursday - Review Database services (RDS, DynamoDB) and Security/Identity (IAM). - Hands-on: Focus on fundamental IAM Policies, IAM Roles, and IAM Users concepts. - Practice DynamoDB table design and RDS instance configuration. 25/10/2024 25/10/2024 AWS Builders, AWSboy Friday - Summary and Mock Exams: Take comprehensive practice tests on AWS Builders and AWSboy platforms. - Review weak areas identified during practice tests for further study. - Create summary notes for quick reference before exam. 26/10/2024 26/10/2024 AWS Builders, AWSboy Week 7 Achievements Successfully completed comprehensive review of core AWS service groups: Compute, Storage, Networking, Database, Security (IAM). Successfully practiced numerous labs and multiple-choice questions on AWS Builders and AWSboy platforms. Mastered fundamental parameters of EC2 (Instance Types, AMI, EBS volumes) and S3 operations (Storage Classes, Object/Bucket management). Clearly understood relationships and configuration of components within a VPC (Public/Private Subnets, Routing, Security Groups vs NACLs). Gained confidence in knowledge acquired, ready for the upcoming mid-term exam. Created comprehensive study notes covering all major AWS service categories. Identified and addressed knowledge gaps through targeted practice sessions. Key Takeaways:\nSecurity Groups are stateful (return traffic automatically allowed), NACLs are stateless (bidirectional rules required) EC2 instance types are optimized for different workloads (compute, memory, storage, GPU) S3 storage classes balance cost vs. access frequency requirements IAM policies follow explicit deny principle - most restrictive policy wins VPC routing follows most specific route match principle "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.7-cicd-pipeline/","title":"CI/CD with CodeBuild &amp; CodePipeline","tags":[],"description":"","content":"CI/CD Pipeline with AWS CodeBuild \u0026amp; CodePipeline This guideline describes how to implement a production-ready CI/CD pipeline with AWS CodePipeline and CodeBuild using GitLab as SCM. When a new Release is created in the GitLab repository, CodePipeline is triggered, CodeBuild runs the frontend and backend projects using the existing frontend-buildspec.yml and backend-buildspec.yml, and then CodePipeline deploys to ECS.\nWhat you’ll do 5.3.1 – Configure CodeBuild projects (frontend/backend) and trigger on GitLab Release 5.3.2 – Design CodePipeline for ECS deploy and integrate post-build artifacts Prerequisites An IAM user/role with permissions for CodeBuild, CodePipeline, S3, ECR (if needed for GitLab token), and IAM pass role. An S3 bucket for pipeline artifacts (will be created by CodePipeline wizard or you can pre-create). ECR repository created. Architecture Overview "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" I wrote this section to capture my own takeaways from the First Cloud Journey (FCJ) program and share feedback that might help future interns get even more value from the experience.\nOverall Evaluation 1. Working Environment\nAWS offered the type of environment I imagined only after years in industry: ergonomic offices, quiet focus rooms, and tooling that actually works. What surprised me most was the accessibility of everyone around me. Senior engineers routinely paused to explain context, and managers encouraged me to ping them whenever I hit a wall. Hybrid flexibility also meant I could stay productive during deep-focus build days while still taking advantage of on-site collaboration.\nBecause the FCJ team treated curiosity as a strength, I asked far more questions than I normally would. With Sandbox accounts and AWS credits on hand, I spun up real infrastructure, broke it, and fixed it under guidance—something textbooks can’t replicate.\n2. Support from Mentor / Team Admin\nMy mentor coached rather than dictated. Whenever I brought a blocker, they nudged me toward the right AWS whitepaper or tool so I could reason through the solution myself. Highlights that stood out:\nWeekly 1:1s where we walked through demos, retros, and upcoming risks Thorough code reviews that emphasized readability, testability, and operational readiness Architecture deep dives so I understood trade-offs—not just the final diagram Career roadmapping advice covering certifications, portfolio focus, and next-step roles Meanwhile, the admin crew cleared every logistical hurdle—from IAM requests to laptop support—before I even realized they existed. Their responsiveness let me stay locked in on shipping features.\n3. Relevance of Work to Academic Major\nThe Bandup IELTS initiative connected directly to my computer science curriculum yet stretched me into applied cloud engineering:\nAcademic Knowledge Applied New Skills Developed Python programming AWS Lambda \u0026amp; serverless orchestration Data structures \u0026amp; algorithms RAG pipelines \u0026amp; vector search Database fundamentals DynamoDB, ElastiCache, data modeling Networking basics VPC design, subnetting, security controls Software engineering CI/CD automation, Infrastructure as Code Class projects rarely demand production SLAs or AI integrations. Owning Gemini audio flows and Titan embedding pipelines forced me to connect theory with real-world latency, throughput, and cost considerations.\n4. Learning \u0026amp; Skill Development Opportunities\nFCJ layered structured learning with autonomy. Over twelve weeks I:\nTouched 15+ AWS services spanning compute, integration, data, and AI (Lambda, API Gateway, SQS, DynamoDB, S3, ECS, Fargate, Bedrock, etc.) Owned end-to-end features by scoping, implementing, testing, and deploying four Lambda functions Practiced cost governance and proved out the 72% savings from adopting Gemini’s native audio path Documented extensively, including this workshop so newcomers inherit a clear playbook Experimented with AI/ML integrations that immediately improved learner outcomes Mentor checkpoints prevented me from drifting off course, yet I still had the autonomy to prototype ideas and learn through iteration.\n5. Company Culture \u0026amp; Team Spirit\nEvery Amazon Leadership Principle I’d read about showed up in day-to-day behavior. Examples:\nTeammates invested in my success, pairing with me late evenings to finish demos When I broke something, we ran blameless post-mortems and captured learnings instead of pointing fingers My suggestion to try Gemini’s native audio workflow wasn’t just heard—it became the baseline approach The “Day 1” mantra was more than a poster; people constantly asked how we could push Bandup further for students Feeling like an actual contributor rather than an observer kept me fully engaged from week one.\n6. Internship Policies / Benefits\nFCJ backed up its promises with tangible support:\n✅ Stipend that respected the value of intern contributions ✅ Flex scheduling so I could juggle university commitments and deliverables ✅ Unlimited access to AWS Skill Builder, whitepapers, and internal tech talks ✅ Introductions to engineers across multiple orgs for career conversations ✅ Portfolio-ready work that I can demo to future employers Reflection Questions What did I find most satisfying during my internship?\nShipping features that students can actually use. Watching the Speaking Evaluator ingest an audio file, run it through Gemini, and return band feedback via my Lambda pipeline was the moment everything clicked—my code made someone else’s learning easier.\nWhat could be improved for future interns?\nFaster AWS account provisioning so interns can start building on day one A structured onboarding kit covering essential AWS concepts, CLI setup, and security do’s/don’ts Cross-intern pairings on shared initiatives to boost peer learning Rotations or shadows with other AWS teams to understand how FCJ fits into the broader organization Would I recommend this internship to friends?\nWithout hesitation. If you love cloud engineering, FCJ offers:\nDirect access to AWS services and production patterns Mentors who invest in your growth Ownership over meaningful deliverables Resume-ready proof of your impact All you need is curiosity and the willingness to put in focused effort.\nSuggestions \u0026amp; Future Expectations Ideas to further elevate FCJ:\nCentralized intern playbook with environment setup, troubleshooting tips, and sample architectures Show-and-tell sessions every other week so interns can demo progress, receive feedback, and learn presentation skills Certification incentives such as exam vouchers or study cohorts to encourage formal milestones FCJ alumni circle that connects current interns with past participants for mentoring and career advice Would I like to stay involved?\nAbsolutely. My goals include:\nReturning post-graduation as a cloud engineer or solutions architect Mentoring the next FCJ cohort to pass on lessons learned Continuing to refine the workshop content so future interns onboard faster Final thoughts\nFCJ reshaped how I think about building in the cloud. I entered with classroom knowledge and curiosity; I’m leaving with battle-tested workflows, stronger communication skills, and confidence in my ability to design secure, scalable systems. Thank you to everyone at AWS who invested in my journey. 🙏\n\u0026ldquo;Growth happens where curiosity meets accountability. FCJ created that intersection for me.\u0026rdquo;\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives Complete the mid-term exam (October 31st) with strong performance. Begin implementing foundational CRUD (Create, Read, Update, Delete) functionalities for the Bandup IELTS project. Research and plan integration of AWS Serverless services (Lambda, API Gateway, DynamoDB) for the project architecture. Set up development environment and establish project structure. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - Final comprehensive review of knowledge in preparation for the mid-term exam. - Review challenging questions and commonly confused concepts (IAM Policies vs Roles, Security Groups vs NACLs, VPC routing). - Practice time management for exam completion. 28/10/2024 28/10/2024 Personal notes, AWS Builders Tuesday - Mental preparation and tool setup for the exam. - Hands-on: Begin setting up development environment for the Bandup IELTS project. - Install and configure Python development tools, AWS CLI, and IDE setup. 29/10/2024 30/10/2024 AWS CLI Documentation Wednesday - Mid-term Exam (October 31st) - Completion of the most important objective. - Post-exam reflection on performance and areas for improvement. 31/10/2024 31/10/2024 Exam Venue Thursday - Begin implementing first basic CRUD functionalities (Create operation: generating flashcard sets). - Research and trial deployment of AWS Lambda functions for serverless compute. - Study DynamoDB table design for storing flashcard data. 01/11/2024 01/11/2024 AWS Lambda \u0026amp; DynamoDB documentation Friday - Plan Serverless architecture integration: + Research API Gateway for RESTful API endpoints. + Design data flow: Frontend → API Gateway → Lambda → DynamoDB. + Define Lambda function structure and event handling patterns. - Implement basic Read functionality to retrieve flashcard sets from DynamoDB. 02/11/2024 02/11/2024 API Gateway documentation, Serverless patterns Week 8 Achievements Completed the mid-term exam (October 31st) successfully. Successfully set up basic development environment for the project with Python, AWS CLI, and IDE configuration. Started building initial Create/Read functionalities for the Bandup IELTS project using AWS Lambda and DynamoDB. Researched and designed serverless architecture pattern: API Gateway for HTTP endpoints Lambda functions for business logic DynamoDB for NoSQL data storage Reinforced knowledge of essential Serverless services (Lambda, DynamoDB, API Gateway) critical for project development. Created initial project structure with proper directory organization. Implemented first Lambda function handler for Create operation. Key Takeaways:\nServerless architecture eliminates server management overhead Lambda functions are event-driven and scale automatically DynamoDB provides single-digit millisecond latency for NoSQL workloads API Gateway acts as the entry point for serverless APIs Proper project structure from the start simplifies future development "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/5-workshop/5.8-cleanup/","title":"Clean Up Resources","tags":[],"description":"","content":"Overview This section guides you through cleaning up all AWS resources created during this workshop to avoid ongoing charges. Follow the steps in order as some resources depend on others.\nImportant: Deleting resources is irreversible. Make sure you have backed up any data you need before proceeding.\nEstimated Time: ~30 minutes Step 1: Delete CI/CD Pipeline Resources First, delete the CI/CD pipeline to stop any automated deployments.\nNavigate to AWS CodePipeline console Select your pipeline (e.g., ielts-pipeline) Click Delete pipeline → Confirm deletion Navigate to AWS CodeBuild console Delete all build projects associated with the workshop Delete any S3 buckets used for pipeline artifacts Step 2: Delete ECS Services and Cluster Stop and delete all ECS services before deleting the cluster.\nNavigate to Amazon ECS console Select your cluster (e.g., ielts-cluster) Go to Services tab For each service: Select the service Click Update → Set Desired tasks to 0 → Update Wait for running tasks to stop Click Delete service → Confirm After all services are deleted, go back to Clusters Select your cluster → Delete cluster → Confirm Step 3: Delete ECR Repositories Delete container images and repositories.\nNavigate to Amazon ECR console Select each repository: ielts-frontend ielts-backend Click Delete → Type the repository name to confirm Step 4: Delete AI Service Resources Delete serverless AI components in this order:\n4.1 Delete Lambda Functions Navigate to AWS Lambda console Delete each function: writing-evaluator speaking-evaluator flashcard-generator evaluation-status s3-upload Select function → Actions → Delete → Confirm 4.2 Delete API Gateway Navigate to Amazon API Gateway console Select your API (e.g., ielts-ai-api) Click Actions → Delete API → Confirm 4.3 Delete SQS Queues Navigate to Amazon SQS console Delete each queue: writing-evaluation-queue writing-evaluation-dlq speaking-evaluation-queue speaking-evaluation-dlq flashcard-generation-queue flashcard-generation-dlq Select queue → Delete → Confirm 4.4 Delete DynamoDB Tables Navigate to Amazon DynamoDB console Delete each table: evaluations flashcard-sets Select table → Delete → Confirm deletion Step 5: Delete Load Balancer Resources Navigate to EC2 console → Load Balancers Select your ALB (e.g., ielts-alb) Click Actions → Delete → Confirm Go to Target Groups Delete all target groups associated with the workshop Go to Listeners and delete any remaining listeners Step 6: Delete Database Resources 6.1 Delete RDS Instance Navigate to Amazon RDS console Select your database instance Click Actions → Delete Uncheck Create final snapshot (if not needed) Check I acknowledge\u0026hellip; → Delete RDS deletion may take 5-10 minutes to complete.\n6.2 Delete ElastiCache (Redis) Navigate to Amazon ElastiCache console Select your Redis cluster Click Delete → Confirm 6.3 Delete RDS Subnet Group In RDS console, go to Subnet groups Select your subnet group → Delete Step 7: Delete S3 Buckets Navigate to Amazon S3 console For each bucket created in the workshop: ielts-audio-bucket ielts-documents-bucket Any pipeline artifact buckets Select bucket → Empty → Confirm After emptying, select bucket → Delete → Confirm S3 buckets must be emptied before they can be deleted.\nStep 8: Delete Secrets Manager Secrets Navigate to AWS Secrets Manager console Select each secret created for the workshop Click Actions → Delete secret Set recovery window to 7 days (minimum) or choose immediate deletion Confirm deletion Step 9: Delete CloudWatch Resources Navigate to Amazon CloudWatch console Go to Log groups Delete log groups: /aws/lambda/writing-evaluator /aws/lambda/speaking-evaluator /aws/lambda/flashcard-generator /ecs/ielts-frontend /ecs/ielts-backend Go to Alarms and delete any created alarms Step 10: Delete VPC and Network Resources Delete network resources in this specific order:\n10.1 Delete NAT Gateway Navigate to VPC console → NAT Gateways Select your NAT Gateway Click Actions → Delete NAT gateway → Confirm Wait for status to change to Deleted 10.2 Release Elastic IPs Go to Elastic IPs Select any Elastic IPs associated with NAT Gateway Click Actions → Release Elastic IP addresses → Confirm 10.3 Delete VPC Endpoints Go to Endpoints Select all VPC endpoints created for the workshop Click Actions → Delete VPC endpoints → Confirm 10.4 Delete Security Groups Go to Security Groups Delete security groups in this order (due to dependencies): Application security groups first Database security groups Load balancer security groups Do not delete the default security group 10.5 Delete Subnets Go to Subnets Select all subnets in your workshop VPC Click Actions → Delete subnet → Confirm 10.6 Delete Route Tables Go to Route Tables Delete custom route tables (not the main route table) Select route table → Actions → Delete route table 10.7 Delete Internet Gateway Go to Internet Gateways Select your IGW → Actions → Detach from VPC → Confirm Select IGW again → Actions → Delete internet gateway → Confirm 10.8 Delete VPC Go to Your VPCs Select your workshop VPC Click Actions → Delete VPC → Confirm Step 11: Delete IAM Resources Navigate to IAM console Go to Roles and delete: ecsTaskExecutionRole (if created for this workshop) ielts-lambda-execution-role Any other workshop-specific roles Go to Policies and delete custom policies created for the workshop Be careful not to delete IAM resources used by other applications.\nVerification Checklist After completing the cleanup, verify all resources are deleted:\nResource Service Console Status CodePipeline CodePipeline ☐ Deleted ECS Cluster ECS ☐ Deleted ECR Repositories ECR ☐ Deleted Lambda Functions Lambda ☐ Deleted API Gateway API Gateway ☐ Deleted SQS Queues SQS ☐ Deleted DynamoDB Tables DynamoDB ☐ Deleted Load Balancer EC2 ☐ Deleted RDS Instance RDS ☐ Deleted ElastiCache ElastiCache ☐ Deleted S3 Buckets S3 ☐ Deleted Secrets Secrets Manager ☐ Deleted CloudWatch Logs CloudWatch ☐ Deleted NAT Gateway VPC ☐ Deleted VPC VPC ☐ Deleted IAM Roles IAM ☐ Deleted Cost Verification To ensure no unexpected charges:\nGo to AWS Billing console Check Bills for the current month Review Cost Explorer to verify no active resources Set up a Budget alert if you plan to continue using AWS Wait 24-48 hours and check your billing dashboard again to confirm all resources have been cleaned up and no charges are accruing.\n"},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.9-week9/","title":"Worklog Week 9","tags":[],"description":"","content":"Week 9 Objectives Complete transition to AWS SAM (Serverless Application Model) development framework. Refactor and re-implement CRUD functionalities following SAM architecture patterns. Resolve environment-related issues to achieve successful deployment status on AWS. Integrate Docker for standardized build environment and dependency management. Workshop: ECS \u0026amp; Container Setup - Deploy containerized Frontend and Backend services. Tasks Completed This Week Day Task Start Date Completion Date Resources Monday - In-depth research on AWS SAM: Understand template.yaml structure, SAM CLI commands, and how Serverless resources (Lambda, API Gateway) operate within SAM model. - Plan detailed migration strategy: Convert existing Lambda functions to SAM-compatible structure. - Study SAM local testing capabilities (sam local invoke, sam local start-api). 04/11/2024 04/11/2024 AWS SAM Documentation, AWS Study Group Tuesday - Source Code Refactoring: Rewrite CRUD functionalities (Create/Read operations) using SAM patterns (Lambda handlers and API Gateway events). - Docker Integration: Install and configure Docker to ensure consistent Python runtime environment for sam build process. - Create Dockerfile for Lambda layer dependencies. - Workshop Activity: Create ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images. 05/11/2024 06/11/2024 Docker Documentation, SAM CLI, Workshop 5.4 Wednesday - Local Debugging and Testing: Execute sam local invoke to test individual Lambda functions. - Encountered critical issues in Local environment: Dependency conflicts, Python version mismatches, DynamoDB local connection problems. - Attempt to resolve local testing barriers through configuration adjustments. 06/11/2024 07/11/2024 SAM CLI Error Reports, Stack Overflow Thursday - Strategic Decision: Backend Team decided to adopt deploy-then-test strategy on actual AWS environment to overcome local debugging limitations, accepting calculated risk. - Focus on fixing configuration errors in template.yaml (resource definitions, IAM permissions, environment variables). - Validate SAM template syntax and resource dependencies. 07/11/2024 08/11/2024 CloudFormation Template Validator Friday - Successful Deployment: Executed sam deploy --guided and successfully deployed project to AWS environment. - Basic Verification: Tested created API endpoints using Postman/curl, confirming CRUD functionality is operational. - Document deployment process and configuration for team reference. - Workshop Activity: Build and push Docker images to ECR, create ECS Task Definitions and deploy ECS Services with Fargate. 08/11/2024 08/11/2024 AWS CloudFormation Deployment Logs, Workshop 5.4 Week 9 Achievements Completed technology transition to AWS SAM development model for entire project. Successfully refactored CRUD functionalities into SAM Serverless structure with proper handler organization. Resolved environment issues by using Docker to ensure sam build process uses correct Python version and dependencies. Achieved critical milestone: Successfully deployed project to AWS environment, overcoming local debugging hurdles. The Bandup IELTS project now has working API version on real Cloud environment (though deeper testing still required). Established deployment workflow and best practices for team collaboration. Created comprehensive template.yaml with proper resource definitions and IAM permissions. Workshop Progress - ECS \u0026amp; Container Setup:\nCreated ECR repositories for Frontend (Next.js) and Backend (Spring Boot) container images Built and pushed Docker images to ECR with proper tagging strategy Created ECS Task Definitions with CPU/memory specifications (Frontend: 512 CPU/1024MB, Backend: 1024 CPU/2048MB) Set up ECS Cluster with Fargate capacity providers Deployed ECS Services in active-passive Multi-AZ pattern (2 replicas active, 1 standby) Configured Service Connect for internal service discovery between Frontend and Backend Implemented health checks for automatic task recovery Key Takeaways:\nSAM simplifies serverless application development with infrastructure as code Docker ensures consistent build environments across different development machines Deploy-then-test strategy can be viable when local testing is problematic SAM templates provide single source of truth for serverless infrastructure Proper IAM permissions in SAM templates are critical for Lambda function execution ECS Fargate eliminates server management overhead for containerized applications Multi-AZ deployment ensures high availability for container services Service Connect simplifies internal service communication without load balancers "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.10-week10/","title":"Worklog Week 10","tags":[],"description":"","content":"Week 10 Objectives Stabilize AWS SAM/Serverless deployment environment and resolve critical issues. Focus on debugging core problems: CORS configuration, template validation errors, and API response formatting. Integrate Frontend/Backend to enable end-to-end testing on user interface. Complete basic Read and Delete functionalities with proper error handling. Participate in AWS Cloud Mastery Series event to receive expert guidance and address project challenges. Workshop: Load Balancer Configuration - Set up Application Load Balancer for traffic distribution. Tasks Completed This Week Day Task Start Date Completion Date Resources Mon - Debug CORS: Analyze CORS configuration in API Gateway (CORS headers, preflight OPTIONS requests) and Lambda response headers to allow Frontend access. - Fix template validation errors: Review and optimize template.yaml file to prevent deployment loop errors and resource dependency issues during sam deploy. 11/11/2024 11/11/2024 API Gateway/CORS Documentation Tue - Strengthen Read function (Retrieving flashcard sets): Ensure data is queried from DynamoDB correctly and returned in proper JSON format for Frontend consumption. - Implement error handling for missing records and invalid queries. - Add logging for debugging purposes. 12/11/2024 12/11/2024 DynamoDB Query Documentation Wed - Frontend Integration: Begin combining Frontend codebase with project and test deployed API endpoints. - Successfully display flashcard sets list on user interface. - Test API connectivity and data rendering in React/Vue components. - Workshop Activity: Create Application Load Balancer (ALB) in public subnets and configure target groups for ECS services. 13/11/2024 13/11/2024 Frontend Framework Documentation, Workshop 5.5 Thu - Deploy and test Delete function (Removing flashcard sets). - Encountered Error: Identified authorization issue with Cognito User Sub ID when executing Delete function - Lambda unable to extract/process Sub ID from Cognito token correctly. - Begin troubleshooting authentication flow. 14/11/2024 14/11/2024 AWS Cognito Documentation Fri - Participation in AWS Cloud Mastery Series: + Received expert guidance and clarified questions regarding Serverless architecture, Lambda best practices, and authentication patterns. - Analyze Update/Delete errors: Apply mentor guidance to resolve authorization issues and Cognito token parsing problems. - Document solutions for future reference. 15/11/2024 15/11/2024 Mentor, AWS Cloud Mastery Series Week 10 Achievements Successfully fixed CORS error and stabilized SAM deployment process (mitigated template validation errors). Participated in AWS Cloud Mastery Series event and gathered essential information to solve major project blockers. Completed Frontend and Backend integration, achieving first functional user interface for end-to-end testing. Successfully deployed Read (Retrieving flashcard sets) and Delete (Removing flashcard sets) functionalities, operational on web interface. Identified and gained direction to solve critical bottlenecks: Authorization error: Lambda fails to retrieve/incorrectly process Cognito Sub ID from JWT token, affecting privileged operations Update function dependency: Requires proper authentication flow and token validation The project has transitioned to basic user testing phase with working CRUD operations. Established debugging workflow and error handling patterns for team. Workshop Progress - Load Balancer Configuration:\nCreated Application Load Balancer (ALB) in public subnets across two AZs Configured target groups for Frontend (port 3000) and Backend (port 8080) ECS services Set up health checks for automatic unhealthy target removal Configured SSL/TLS termination using ACM certificates Integrated ALB with Route 53 for DNS routing Implemented listener rules for path-based routing (Frontend: /, Backend: /api/*) Configured security groups to allow ALB traffic to ECS tasks Key Takeaways:\nCORS requires proper configuration in both API Gateway and Lambda response headers Cognito JWT tokens must be properly decoded to extract user identity (Sub ID) Frontend-Backend integration requires careful attention to API contracts and data formats Error handling and logging are essential for debugging production issues AWS Cloud Mastery Series provides valuable real-world insights from experienced practitioners ALB provides intelligent traffic distribution and automatic failover Health checks ensure only healthy targets receive traffic SSL/TLS termination at ALB reduces computational load on backend services "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.11-week11/","title":"Worklog Week 11","tags":[],"description":"","content":"Week 11 Objectives: Participate in AWS Cloud Mastery Series #2 to continue resolving specialized technical issues. Refactor and standardize the Frontend structure for improved stability and maintainability. Implement a Multi-Stack architecture to optimize deployment speed and Serverless project management. Integrate basic CRUD functionalities with AI Image Processing (using Rekognition) into the website. Completely resolve deployment errors (especially CORS issues) to stabilize the system. Workshop: AI Service Architecture, Security \u0026amp; IAM, Monitoring - Complete serverless AI pipeline setup. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Sun - Participate in AWS Cloud Mastery Series #2 (Nov 17th): Continue receiving guidance and addressing deeper technical questions about authorization errors and the AI workflow. 17/11/2024 17/11/2024 Mentor, AWS Cloud Mastery Series Mon - Frontend Structure Unification and Refactor: Hold team meeting to standardize the Frontend code structure for maintainability. - Research Multi-Stack Solution: Begin analyzing how to split the template.yaml file into smaller Stacks (Multi-Stack) to optimize the sam deploy process. 18/11/2024 18/11/2024 Serverless Architecture Docs Tue - Implement Multi-Stack Architecture: Start splitting and configuring separate Stacks (e.g., API Backend Stack, Frontend Hosting Stack). - Proceed with AI Image Processing Integration: Combine basic CRUD functions with image processing logic (e.g., calling Rekognition API/S3 trigger) in preparation for the Update function. - Workshop Activity: Set up API Gateway REST endpoints and SQS queues for asynchronous AI processing. 19/11/2024 19/11/2024 Backend Codebase, AWS Rekognition, Workshop 5.7 Wed - Error Encountered after AI Integration: The system faced errors after combining AI functionality, necessitating a full Stack deletion and redeployment. - Leader Develops Backup Stack: The team leader created a separate, optimized Multi-Stack as a contingency and reference for future optimal deployments. 20/11/2024 20/11/2024 Leader\u0026rsquo;s Backup Stack Thu - Persistent CORS Error: After redeploying, the CORS issue re-emerged. - In-depth CORS Debugging: Spent time thoroughly analyzing the root cause and permanently fixing the CORS error, ensuring correct header configuration on both API Gateway and Lambda. - Workshop Activity: Configure IAM roles and policies for Lambda functions, set up Secrets Manager for API keys, and implement WAF rules. 21/11/2024 21/11/2024 API Gateway/Lambda Configuration, Workshop 5.9 Fri - Team Meeting and Project Stabilization: Held a team meeting to review the new Frontend structure, stabilize the main project Stack, and synchronize the fixes for CORS and basic template errors. - Optimization for Maintenance: Finalized the solution to use a separate stack (developed by the leader) for flexibility and easier optimization in future development. 22/11/2024 22/11/2024 New Structure Report Week 11 Achievements: Participated in AWS Cloud Mastery Series #2, gaining deeper knowledge of Serverless, Rekognition, and solutions for authorization errors. Successfully refactored the Frontend and standardized the overall project structure, improving maintainability. Implemented the Multi-Stack architecture (or at least established a reliable solution/backup stack), which speeds up deployment and simplifies resource management. Completely resolved the persistent CORS error after identifying the root cause, ensuring stable communication between Frontend and Backend. Acquired knowledge on fixing basic Template errors and gained a clearer understanding of AWS SAM deployment issues. Developed a separate stack for backup/optimization, enhancing project flexibility and safety during future major updates. The project has moved into the AI functionality testing phase, and although errors were encountered, a clear path for troubleshooting has been established. Workshop Progress - AI Services, Security \u0026amp; Monitoring:\nConfigured API Gateway REST API with three endpoints: /writing/evaluate, /speaking/evaluate, /flashcard/generate Set up SQS queues for asynchronous message processing (writing-queue, speaking-queue, flashcard-queue) Deployed Lambda functions: writing_evaluator, speaking_evaluator, rag_flashcard with proper IAM roles Configured DynamoDB tables: bandup-evaluations and bandup-flashcard-sets for storing AI results Integrated Amazon Bedrock (Titan Embeddings V2) and Google Gemini API for AI processing Set up AWS Secrets Manager for secure API key storage Configured IAM roles with least-privilege permissions for Lambda functions Implemented AWS WAF rules for application-level protection Set up CloudWatch Logs and Alarms for monitoring Lambda execution and errors Configured CloudWatch Insights for log analysis and troubleshooting "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/1-worklog/1.12-week12/","title":"Worklog Week 12","tags":[],"description":"","content":"Week 12 Objectives: Complete 100% of the basic CRUD functionalities and AI image processing (including the Update function). Upgrade the image processing architecture by integrating SQS for asynchronous processing and clear flow segmentation. Finalize essential missing project features: Security, Map Pinning, and SNS. Complete the main interfaces of the Frontend, prepare for domain name purchase, and attend the final AWS Cloud Mastery Series event. Tasks to be Deployed This Week: Day Task Start Date Completion Date Resources Mon - Finalize Update Function and AI: Resolve remaining bugs (Sub ID, Rekognition) to ensure CRUD and image processing are fully functional. 25/11/2024 25/11/2024 Mentor Guidance, Backend Codebase Tue - Upgrade AI Flow with SQS: Implement AWS SQS to create an asynchronous image processing queue, enhancing flow segmentation and performance under high load. - Define Clear Processing Flows: Redefine the data flow (Upload -\u0026gt; S3 -\u0026gt; SQS -\u0026gt; Lambda (AI) -\u0026gt; DynamoDB). 26/11/2024 26/11/2024 AWS SQS Documentation, Lambda Architecture Wed - Frontend Interface Finalization: Complete interfaces for the main pages (Homepage, Article Detail, Personal Management Page). - Implement Map Pinning: Integrate Map Pinning functionality for posts, utilizing geo data in DynamoDB or an appropriate map service. 27/11/2024 27/11/2024 Frontend Codebase, DynamoDB Geo Thu - Finalize Security (Authorization): Optimize authentication and permissions (IAM Policy/Cognito), especially accurate Sub retrieval for user operations. - Implement SNS: Integrate AWS SNS for basic notification features (e.g., notification when a post is successfully processed/uploaded). 28/11/2024 28/11/2024 AWS SNS, Cognito/IAM Documentation Fri - Attend the Final AWS Cloud Mastery Series Event: Receive overall project guidance, review and finalize missing parts (domain, security, SNS) before the demo. - Domain Name: Conduct research and prepare for purchasing the website domain, configuring basic DNS (Route 53) as necessary (based on mentor guidance). 29/11/2024 29/11/2024 Mentor, AWS Cloud Mastery Series, Route 53 Week 12 Achievements: Completed 100% of basic CRUD functionalities and AI image processing, ensuring system stability. Upgraded the image processing architecture by integrating SQS and defining clear asynchronous processing flows, improving performance and reliability. Finalized essential features: Implemented Security, Map Pinning, and SNS notifications. The basic Frontend interface is complete, ready for presentation. Successfully participated in the final AWS Cloud Mastery Series event, receiving comprehensive guidance for project completion. Research for domain name purchase has been conducted and planned. The project has reached Demo Readiness status. "},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nhatanhaxtanh.github.io/nhatanh/tags/","title":"Tags","tags":[],"description":"","content":""}]